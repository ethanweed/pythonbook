{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "insured-cedar",
   "metadata": {},
   "source": [
    "(anova2)=\n",
    "# Factorial ANOVA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9a282ab",
   "metadata": {},
   "source": [
    "Over the course of the last few chapters you can probably detect a general trend. We started out looking at tools that you can use to compare two groups to one another, most notably the [$t$-test](ttest). Then, we introduced [analysis of variance](anova) (ANOVA) as a method for comparing more than two groups. The chapter on [regression](regression) covered a somewhat different topic, but in doing so it introduced a powerful new idea: building statistical models that have *multiple* predictor variables used to explain a single outcome variable. For instance, a regression model could be used to predict the number of errors a student makes in a reading comprehension test based on the number of hours they studied for the test, and their score on a standardised IQ test. The goal in this chapter is to import this idea into the ANOVA framework. For instance, suppose we were interested in using the reading comprehension test to measure student achievements in three different schools, and we suspect that girls and boys are developing at different rates (and so would be expected to have different performance on average). Each student is classified in two different ways: on the basis of their gender, and on the basis of their school. What we'd like to do is analyse the reading comprehension scores in terms of *both* of these grouping variables. The tool for doing so is generically referred to as **_factorial ANOVA_**. However, since we have two grouping variables, we sometimes refer to the analysis as a two-way ANOVA, in contrast to the one-way ANOVAs that we [ran earlier](anova)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19203557",
   "metadata": {},
   "source": [
    "(factorialanovasimple)=\n",
    "## Balanced designs, main effects\n",
    "\n",
    "When we discussed [analysis of variance](anova), we assumed a fairly simple experimental design: each person falls into one of several groups, and we want to know whether these groups have different means on some outcome variable. In this section, I'll discuss a broader class of experimental designs, known as **_factorial designs_**, in we have more than one grouping variable. I gave one example of how this kind of design might arise above. Another example appears in the chapter on [ANOVA](anova), in which we were looking at the effect of different drugs on the `mood_gain` experienced by each person. In that chapter we did find a significant effect of drug, but at the end of the chapter we also ran an analysis to see if there was an effect of therapy. We didn't find one, but there's something a bit worrying about trying to run two *separate* analyses trying to predict the same outcome. Maybe there actually *is* an effect of therapy on mood gain, but we couldn't find it because it was being \"hidden\" by the effect of drug? In other words, we're going to want to run a *single* analysis that includes *both* `drug` and `therapy` as predictors. For this analysis each person is cross-classified by the drug they were given (a factor with 3 levels) and what therapy they received (a factor with 2 levels). We refer to this as a $3 \\times 2$ factorial design. If we cross-tabulate `drug` by `therapy` using the `crosstab()` in `pandas`, we get the following table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b38e94ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>therapy</th>\n",
       "      <th>CBT</th>\n",
       "      <th>no.therapy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drug</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anxifree</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joyzepam</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>placebo</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "therapy   CBT  no.therapy\n",
       "drug                     \n",
       "anxifree    3           3\n",
       "joyzepam    3           3\n",
       "placebo     3           3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/ethanweed/pythonbook/main/Data/clintrial.csv\")\n",
    "\n",
    "pd.crosstab(index=df[\"drug\"], columns=df[\"therapy\"],margins=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2aee2f64",
   "metadata": {},
   "source": [
    "As you can see, not only do we have participants corresponding to all possible combinations of the two factors, indicating that our design is **_completely crossed_**, it turns out that there are an equal number of people in each group. In other words, we have a **_balanced_** design. In this section I'll talk about how to analyse data from balanced designs, since this is the simplest case. The story for unbalanced designs is quite tedious, so we'll put it to one side for the moment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbf44bf3",
   "metadata": {},
   "source": [
    "(factanovahyp)=\n",
    "### What hypotheses are we testing?\n",
    "\n",
    "Like one-way ANOVA, factorial ANOVA is a tool for testing certain types of hypotheses about population means. So a sensible place to start would be to be explicit about what our hypotheses actually are. However, before we can even get to that point, it's really useful to have some clean and simple notation to describe the population means. Because of the fact that observations are cross-classified in terms of two different factors, there are quite a lot of different means that one might be interested. To see this, let's start by thinking about all the different sample means that we can calculate for this kind of design. Firstly, there's the obvious idea that we might be interested in this table of group means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b4a176a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>therapy</th>\n",
       "      <th>drug</th>\n",
       "      <th>mood_gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CBT</td>\n",
       "      <td>anxifree</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CBT</td>\n",
       "      <td>joyzepam</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CBT</td>\n",
       "      <td>placebo</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no.therapy</td>\n",
       "      <td>anxifree</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no.therapy</td>\n",
       "      <td>joyzepam</td>\n",
       "      <td>1.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>no.therapy</td>\n",
       "      <td>placebo</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      therapy      drug  mood_gain\n",
       "0         CBT  anxifree       1.03\n",
       "1         CBT  joyzepam       1.50\n",
       "2         CBT   placebo       0.60\n",
       "3  no.therapy  anxifree       0.40\n",
       "4  no.therapy  joyzepam       1.47\n",
       "5  no.therapy   placebo       0.30"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(round(df.groupby(by=['therapy', 'drug'])['mood_gain'].mean(),2)).reset_index()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "baf64f4f",
   "metadata": {},
   "source": [
    "Now, this output shows a cross-tabulation of the group means for all possible combinations of the two factors (e.g., people who received the placebo and no therapy, people who received the placebo while getting CBT, etc). However, we can also construct tables that ignore one of the two factors. That gives us output that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8653859e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>therapy</th>\n",
       "      <th>mood_gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CBT</td>\n",
       "      <td>1.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no.therapy</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      therapy  mood_gain\n",
       "0         CBT       1.04\n",
       "1  no.therapy       0.72"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(round(df.groupby(['therapy'])['mood_gain'].mean(),2)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "821fd280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drug</th>\n",
       "      <th>mood_gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anxifree</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>joyzepam</td>\n",
       "      <td>1.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>placebo</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       drug  mood_gain\n",
       "0  anxifree       0.72\n",
       "1  joyzepam       1.48\n",
       "2   placebo       0.45"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(round(df.groupby(['drug'])['mood_gain'].mean(),2)).reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8db1851",
   "metadata": {},
   "source": [
    "But of course, if we can ignore one factor we can certainly ignore both. That is, we might also be interested in calculating the average  mood gain across all 18 participants, regardless of what drug or psychological therapy they were given:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3a84461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(df['mood_gain'].mean(),2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee5175b3",
   "metadata": {},
   "source": [
    "At this point we have 12 different sample means to keep track of! It is helpful to organise all these numbers into a single table, which would look like this:\n",
    "\n",
    "|         |no therapy |CBT  |total |\n",
    "|:--------|:----------|:----|:-----|\n",
    "|placebo  |0.30       |0.60 |0.45  |\n",
    "|anxifree |0.40       |1.03 |0.72  |\n",
    "|joyzepam |1.47       |1.50 |1.48  |\n",
    "|total    |0.72       |1.04 |0.88  |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fec1291",
   "metadata": {},
   "source": [
    "Now, each of these different means is of course a sample statistic: it's a quantity that pertains to the specific observations that we've made during our study. What we want to make inferences about are the corresponding population parameters: that is, the true means as they exist within some broader population. Those population means can also be organised into a similar table, but we'll need a little mathematical notation to do so. As usual, I'll use the symbol $\\mu$ to denote a population mean. However, because there are lots of different means, I'll need to use subscripts to distinguish between them. \n",
    "\n",
    "Here's how the notation works. Our table is defined in terms of two factors: each row corresponds to a different level of Factor A (in this case `drug`), and each column corresponds to a different level of Factor B (in this case `therapy`). If we let $R$ denote the number of rows in the table, and $C$ denote the number of columns, we can refer to this as an $R \\times C$ factorial ANOVA. In this case $R=3$ and $C=2$. We'll use lowercase letters to refer to specific rows and columns, so $\\mu_{rc}$ refers to the population mean associated with the $r$th level of Factor A (i.e. row number $r$) and the $c$th level of Factor B (column number $c$).[^notesubscript] So the population means are now written like this:\n",
    "\n",
    "[^notesubscript]: The nice thing about the subscript notation is that generalises nicely: if our experiment had involved a third factor, then we could just add a third subscript. In principle, the notation extends to as many factors as you might care to include, but in this book we'll rarely consider analyses involving more than two factors, and never more than three. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f3fbcf3",
   "metadata": {},
   "source": [
    "|         |no therapy |CBT        |total |\n",
    "|:--------|:----------|:----------|:-----|\n",
    "|placebo  |$\\mu_{11}$ |$\\mu_{12}$ |      |\n",
    "|anxifree |$\\mu_{21}$ |$\\mu_{22}$ |      |\n",
    "|joyzepam |$\\mu_{31}$ |$\\mu_{32}$ |      |\n",
    "|total    |           |           |      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7eaf7b58",
   "metadata": {},
   "source": [
    "Okay, what about the remaining entries? For instance, how should we describe the average mood gain across the entire (hypothetical) population of people who might be given Joyzepam in an experiment like this, regardless of whether they were in CBT? We use the \"dot\" notation to express this. In the case of Joyzepam, notice that we're talking about the mean associated with the third row in the table. That is, we're averaging across two cell means (i.e., $\\mu_{31}$ and $\\mu_{32}$). The result of this averaging is referred to as a **_marginal mean_**, and would be denoted $\\mu_{3.}$ in this case. The marginal mean for CBT corresponds to the population mean associated with the second column in the table, so we use the notation $\\mu_{.2}$ to describe it. The grand mean is denoted $\\mu_{..}$ because it is the mean obtained by averaging (marginalising[^notemarginalising]) over both. So our full table of population means can be written down like this:\n",
    "\n",
    "[^notemarginalising]: Technically, marginalising isn't quite identical to a regular mean: it's a weighted average, where you take into account the frequency of the different events that you're averaging over. However, in a balanced design, all of our cell frequencies are equal by definition, so the two are equivalent. We'll discuss unbalanced designs later, and when we do so you'll see that all of our calculations become a real headache. But let's ignore this for now."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b8e1412",
   "metadata": {},
   "source": [
    "|         |no therapy |CBT        |total      |\n",
    "|:--------|:----------|:----------|:----------|\n",
    "|placebo  |$\\mu_{11}$ |$\\mu_{12}$ |$\\mu_{1.}$ |\n",
    "|anxifree |$\\mu_{21}$ |$\\mu_{22}$ |$\\mu_{2.}$ |\n",
    "|joyzepam |$\\mu_{31}$ |$\\mu_{32}$ |$\\mu_{3.}$ |\n",
    "|total    |$\\mu_{.1}$ |$\\mu_{.2}$ |$\\mu_{..}$ |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fad9faf3",
   "metadata": {},
   "source": [
    "Now that we have this notation, it is straightforward to formulate and express some hypotheses. Let's suppose that the goal is to find out two things: firstly, does the choice of drug have any effect on mood, and secondly, does CBT have any effect on mood? These aren't the only hypotheses that we could formulate of course, and we'll see a really important example of a different kind of hypothesis in the section on [interactions](interactions), but these are the two simplest hypotheses to test, and so we'll start there. Consider the first test. If drug has no effect, then we would expect all of the row means to be identical, right? So that's our null hypothesis. On the other hand, if the drug does matter then we should expect these row means to be different. Formally, we write down our null and alternative hypotheses in terms of the *equality of marginal means*:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d946da2",
   "metadata": {},
   "source": [
    "|                              |                                                             |\n",
    "|:-----------------------------|:------------------------------------------------------------|\n",
    "|Null hypothesis $H_0$:        |row means are the same i.e. $\\mu_{1.} = \\mu_{2.} = \\mu_{3.}$ |\n",
    "|Alternative hypothesis $H_1$: |at least one row mean is different.                          |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fb68110",
   "metadata": {},
   "source": [
    "It's worth noting that these are *exactly* the same statistical hypotheses that we formed when we ran a one-way ANOVA on these data [way back when](anova). Back then I used the notation $\\mu_P$ to refer to the mean mood gain for the placebo group, with $\\mu_A$ and $\\mu_J$ corresponding to the group means for the two drugs, and the null hypothesis was $\\mu_P = \\mu_A = \\mu_J$. So we're actually talking about the same hypothesis: it's just that the more complicated ANOVA requires more careful notation due to the presence of multiple grouping variables, so we're now referring to this hypothesis as $\\mu_{1.} = \\mu_{2.} = \\mu_{3.}$. However, as we'll see shortly, although the hypothesis is identical, the test of that hypothesis is subtly different due to the fact that we're now acknowledging the existence of the second grouping variable.\n",
    "\n",
    "Speaking of the other grouping variable, you won't be surprised to discover that our second hypothesis test is formulated the same way. However, since we're talking about the psychological therapy rather than drugs, our null hypothesis now corresponds to the equality of the column means:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a4d1435",
   "metadata": {},
   "source": [
    "|                              |                                                          |\n",
    "|:-----------------------------|:---------------------------------------------------------|\n",
    "|Null hypothesis $H_0$:        |column means are the same, i.e., $\\mu_{.1} = \\mu_{.2}$    |\n",
    "|Alternative hypothesis $H_1$: |column means are different, i.e., $\\mu_{.1} \\neq \\mu_{.2}$|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "472ed7cd",
   "metadata": {},
   "source": [
    "### Running the analysis in Python\n",
    "\n",
    "If the data you're trying to analyse correspond to a balanced factorial design, then running your analysis of variance is easy. To see how easy it is, let's start by reproducing the original analysis from [earlier](anova). In case you've forgotten, for that analysis we were using only a single factor (i.e., `drug`) as our between-subjects variable to predict our outcome (dependent) variable (i.e., `mood_gain`), and so this was what we did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d667428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>SS</th>\n",
       "      <th>DF</th>\n",
       "      <th>MS</th>\n",
       "      <th>F</th>\n",
       "      <th>p-unc</th>\n",
       "      <th>np2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drug</td>\n",
       "      <td>3.45</td>\n",
       "      <td>2</td>\n",
       "      <td>1.73</td>\n",
       "      <td>18.61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Within</td>\n",
       "      <td>1.39</td>\n",
       "      <td>15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Source    SS  DF    MS      F  p-unc   np2\n",
       "0    drug  3.45   2  1.73  18.61    0.0  0.71\n",
       "1  Within  1.39  15  0.09    NaN    NaN   NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pingouin as pg\n",
    "\n",
    "model1 = pg.anova(dv='mood_gain', between='drug', data=df, detailed=True)\n",
    "round(model1, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "709da32e",
   "metadata": {},
   "source": [
    "Note that this time around I've used the name `model1` as the label for my output variable, since I'm planning on creating quite a few other models too. To start with, suppose I'm also curious to find out if `therapy` has a relationship to `mood_gain`. In light of what we've seen from our discussion of [multiple regression](regression), you probably won't be surprised that all we have to do is extend the formula: in other words, if we specify `dv=mood_gain, between=['drug', 'therapy']` as our model, we'll probably get what we're after:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c10f48ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>SS</th>\n",
       "      <th>DF</th>\n",
       "      <th>MS</th>\n",
       "      <th>F</th>\n",
       "      <th>p-unc</th>\n",
       "      <th>np2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drug</td>\n",
       "      <td>3.4533</td>\n",
       "      <td>2</td>\n",
       "      <td>1.7267</td>\n",
       "      <td>31.7143</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>therapy</td>\n",
       "      <td>0.4672</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4672</td>\n",
       "      <td>8.5816</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.4170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>drug * therapy</td>\n",
       "      <td>0.2711</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1356</td>\n",
       "      <td>2.4898</td>\n",
       "      <td>0.1246</td>\n",
       "      <td>0.2933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Residual</td>\n",
       "      <td>0.6533</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0544</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Source      SS  DF      MS        F   p-unc     np2\n",
       "0            drug  3.4533   2  1.7267  31.7143  0.0000  0.8409\n",
       "1         therapy  0.4672   1  0.4672   8.5816  0.0126  0.4170\n",
       "2  drug * therapy  0.2711   2  0.1356   2.4898  0.1246  0.2933\n",
       "3        Residual  0.6533  12  0.0544      NaN     NaN     NaN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = pg.anova(dv='mood_gain', between=['drug', 'therapy'], data=df, detailed=True)\n",
    "round(model2, 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "085e641f",
   "metadata": {},
   "source": [
    "Most of this output is pretty simple to read too: the first row of the table reports a between-group sum of squares (SS) value associated with the `drug` factor, along with a corresponding between-group $df$ value. It also calculates a mean square value (MS), and $F$-statistic, an (uncorrected) $p$-value, and an estimate of the effect size (`np2`, that is, partial eta-squared). There is also a row corresponding to the `therapy` factor, and a row corresponding to the residuals (i.e., the within groups variation). \n",
    "\n",
    "Now, the third row is a little trickier, so let's just save that one for [later](interactions), shall we? (Spoiler: this is the interaction of `drug` and `therapy`, but we'll get there soon).\n",
    "\n",
    "Not only are all of the individual quantities pretty familiar, the relationships between these different quantities has remained unchanged: just like we saw with the original one-way ANOVA, note that the mean square value is calculated by dividing SS by the corresponding $df$. That is, it's still true that\n",
    "\n",
    "$$\n",
    "\\mbox{MS} = \\frac{\\mbox{SS}}{df}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4badab0d",
   "metadata": {},
   "source": [
    "regardless of whether we're talking about `drug`, `therapy` or the residuals. To see this, let's not worry about how the sums of squares values are calculated: instead, let's take it on faith that Python has calculated the SS values correctly, and try to verify that all the rest of the numbers make sense. First, note that for the `drug` factor, we divide $3.45$ by $2$, and end up with a mean square value of $1.73$. For the `therapy` factor, there's only 1 degree of freedom, so our calculations are even simpler: dividing $0.47$ (the SS value) by 1 gives us an answer of $0.47$ (the MS value). \n",
    "\n",
    "Turning to the $F$ statistics and the $p$ values, notice that we have two of each: one corresponding to the `drug` factor and the other corresponding to the `therapy` factor. Regardless of which one we're talking about, the $F$ statistic is calculated by dividing the mean square value associated with the factor by the mean square value associated with the residuals. If we use \"A\" as shorthand notation to refer to the first factor (factor A; in this case `drug`) and \"R\" as shorthand notation to refer to the residuals, then the $F$ statistic associated with factor A is denoted $F_A$, and is calculated as follows:\n",
    "\n",
    "$$\n",
    "F_{A} = \\frac{\\mbox{MS}_{A}}{\\mbox{MS}_{R}}\n",
    "$$\n",
    "\n",
    "Note that this use of \"R\" to refer to residuals is a bit awkward, since we also used the letter R to refer to the number of rows in the table, but I'm only going to use \"R\" to mean residuals in the context of SS$_R$ and MS$_R$, so hopefully this shouldn't be confusing. Anyway, to apply this formula to the  `drugs` factor, we take the mean square of $1.73$ and divide it by the residual mean square value of $0.07$, which gives us an $F$-statistic of $26.15$. The corresponding calculation for the `therapy` variable would be to divide $0.47$ by $0.07$ which gives $7.08$ as the $F$-statistic. Not surprisingly, of course, these are the same values that R has reported in the ANOVA table above.\n",
    "\n",
    "The last part of the ANOVA table is the calculation of the $p$ values. Once again, there is nothing new here: for each of our two factors, what we're trying to do is test the null hypothesis that there is no relationship between the factor and the outcome variable (I'll be a bit more precise about this later on). To that end, we've (apparently) followed a similar strategy that we did in the one way ANOVA, and have calculated an $F$-statistic for each of these hypotheses. To convert these to $p$ values, all we need to do is note that the  that the sampling distribution for the $F$ *statistic* under the null hypothesis (that the factor in question is irrelevant) is an $F$ *distribution*: and that two degrees of freedom values are those corresponding to the factor, and those corresponding to the residuals. For the `drug` factor we're talking about an $F$ distribution with 2 and 14 degrees of freedom (I'll discuss degrees of freedom in more detail later). In contrast, for the `therapy` factor sampling distribution is $F$ with 1 and 14 degrees of freedom."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c324cf91",
   "metadata": {},
   "source": [
    "At this point, I hope you can see that the ANOVA table for this more complicated analysis corresponding to `model2` should be read in much the same way as the ANOVA table for the simpler analysis for `model1`. In short, it's telling us that the factorial ANOVA for our $3 \\times 2$ design found a significant effect of drug ($F_{2,12} = 31.71, p < .001$) as well as a significant effect of therapy ($F_{1,12} = 8.58, p = .01$). Or, to use the more technically correct terminology, we would say that there are two **_main effects_** of drug and therapy. Why are these \"main\" effects? Well, because there could also be an *interaction* between the effect of `drug` and the effect of `therapy`. We'll explore the concept of [interactions](interactions) below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89c02ba5",
   "metadata": {},
   "source": [
    "In the previous section I had two goals: firstly, to show you that the Python commands needed to do factorial ANOVA are pretty much the same ones that we used for a one way ANOVA. The only difference is that we add to the number of predictors in the `between` argument of `pingouin`'s `anova()` function. Secondly, I wanted to show you what the ANOVA table looks like in this case, so that you can see from the outset that the basic logic and structure behind factorial ANOVA is the same as that which underpins one way ANOVA. Try to hold onto that feeling. It's genuinely true, insofar as factorial ANOVA is built in more or less the same way as the simpler one-way ANOVA model. It's just that this feeling of familiarity starts to evaporate once you start digging into the details. Traditionally, this comforting sensation is replaced by an urge to murder the the authors of statistics textbooks.\n",
    "\n",
    "Okay, let's start looking at some of those details. The explanation that I gave in the last section illustrates the fact that the hypothesis tests for the main effects (of drug and therapy in this case) are $F$-tests, but what it doesn't do is show you how the sum of squares (SS) values are calculated. Nor does it tell you explicitly how to calculate degrees of freedom ($df$ values) though that's a simple thing by comparison. Let's assume for now that we have only two predictor variables, Factor A and Factor B. If we use $Y$ to refer to the outcome variable, then we would use $Y_{rci}$ to refer to the outcome associated with the $i$-th member of group $rc$ (i.e., level/row $r$ for Factor A and level/column $c$ for Factor B). Thus, if we use $\\bar{Y}$ to refer to a sample mean, we can use the same notation as before to refer to group means, marginal means and grand means: that is, $\\bar{Y}_{rc}$ is the sample mean associated with the $r$th level of Factor A and the $c$th level of Factor B, $\\bar{Y}_{r.}$ would be the marginal mean for the $r$th level of Factor A, $\\bar{Y}_{.c}$ would be the marginal mean for the $c$th level of Factor B, and $\\bar{Y}_{..}$ is the grand mean. In other words, our sample means can be organised into the same table as the population means. For our clinical trial data, that table looks like this:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d76b792a",
   "metadata": {},
   "source": [
    "|         |no therapy     |CBT            |total          |\n",
    "|:--------|:--------------|:--------------|:--------------|\n",
    "|placebo  |$\\bar{Y}_{11}$ |$\\bar{Y}_{12}$ |$\\bar{Y}_{1.}$ |\n",
    "|anxifree |$\\bar{Y}_{21}$ |$\\bar{Y}_{22}$ |$\\bar{Y}_{2.}$ |\n",
    "|joyzepam |$\\bar{Y}_{31}$ |$\\bar{Y}_{32}$ |$\\bar{Y}_{3.}$ |\n",
    "|total    |$\\bar{Y}_{.1}$ |$\\bar{Y}_{.2}$ |$\\bar{Y}_{..}$ |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "417af7c0",
   "metadata": {},
   "source": [
    "And if we look at the sample means that I showed earlier, we have $\\bar{Y}_{11} = 0.30$, $\\bar{Y}_{12} = 0.60$ etc. In our clinical trial example, the `drugs` factor has 3 levels and the `therapy` factor has 2 levels, and so what we're trying to run is a $3 \\times 2$ factorial ANOVA. However, we'll be a little more general and say that Factor A (the row factor) has $R$ levels and Factor B (the column factor) has $C$ levels, and so what we're runnning here is an $R \\times C$ factorial ANOVA.\n",
    "\n",
    "Now that we've got our notation straight, we can compute the sum of squares values for each of the two factors in a relatively familiar way. For Factor A, our between group sum of squares is calculated by assessing the extent to which the (row) marginal means $\\bar{Y}_{1.}$, $\\bar{Y}_{2.}$ etc, are different from the grand mean $\\bar{Y}_{..}$. We do this in the same way that we did for one-way ANOVA: calculate the sum of squared difference between the $\\bar{Y}_{i.}$ values and the $\\bar{Y}_{..}$ values. Specifically, if there are $N$ people in each group, then we calculate this:\n",
    "\n",
    "$$\n",
    "\\mbox{SS}_{A} = (N \\times C)  \\sum_{r=1}^R  \\left( \\bar{Y}_{r.} - \\bar{Y}_{..} \\right)^2\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ce726e7",
   "metadata": {},
   "source": [
    "As with one-way ANOVA, the most interesting [^translation] part of this formula is the $\\left( \\bar{Y}_{r.} - \\bar{Y}_{..} \\right)^2$ bit, which corresponds to the squared deviation associated with level $r$. All that this formula does is calculate this squared deviation for all $R$ levels of the factor, add them up, and then multiply the result by $N \\times C$. The reason for this last part is that there are multiple cells in our design that have level $r$ on Factor A: in fact, there are $C$ of them, one corresponding to each possible level of Factor B! For instance, in our toy example, there are *two* different cells in the design corresponding to the `anxifree` drug: one for people with `no.therapy`, and one for the `CBT` group. Not only that, within each of these cells there are $N$ observations. So, if we want to convert our SS value into a quantity that calculates the between-groups sum of squares on a \"per observation\" basis, we have to multiply by by $N \\times C$. The formula for factor B is of course the same thing, just with some subscripts shuffled around:\n",
    "\n",
    "$$\n",
    "\\mbox{SS}_{B} = (N \\times R) \\sum_{c=1}^C \\left( \\bar{Y}_{.c} - \\bar{Y}_{..} \\right)^2\n",
    "$$\n",
    "\n",
    "[^translation]: English translation: \"least tedious\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ddfb08f",
   "metadata": {},
   "source": [
    "Now that we have these formulas, we can check them against the `pingouin` output from the earlier section. First, notice that we calculated all the marginal means (i.e., row marginal means $\\bar{Y}_{r.}$ and column marginal means $\\bar{Y}_{.c}$) earlier using `.mean()`, and we also calculated the grand mean. Let's repeat those calculations, but this time we'll save the results to variables so that we can use them in subsequent calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7694d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_means = df.groupby(['drug'])['mood_gain'].mean()\n",
    "therapy_means = df.groupby(['therapy'])['mood_gain'].mean()\n",
    "grand_mean = df['mood_gain'].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d15dc88c",
   "metadata": {},
   "source": [
    "Okay, now letâ€™s calculate the sum of squares associated with the main effect of drug. There are a total of $N=3$ people in each group, and $C=2$ different types of therapy. Or, to put it another way, there are $3 \\times 2 = 6$ people who received any particular drug. So our calculations are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a76c801d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4533"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SS_drug = (3*2) * sum((drug_means - grand_mean)**2)\n",
    "round(SS_drug,4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbec528b",
   "metadata": {},
   "source": [
    "Not surprisingly, this is the same number that you get when you look up the SS value for the drugs factor in the ANOVA table that I presented earlier. We can repeat the same kind of calculation for the effect of therapy. Again there are $N=3$ people in each group, but since there are $R=3$ different drugs, this time around we note that there are $3 \\times 3 = 9$ people who received CBT, and an additional 9 people who received the placebo. So our calculation is now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03d9109b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4672"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SS_therapy = (3*3) * sum((therapy_means - grand_mean)**2)\n",
    "round(SS_therapy,4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd389570",
   "metadata": {},
   "source": [
    "and we are, once again, unsurprised to see that our calculations are identical to the ANOVA output.\n",
    "\n",
    "So that's how you calculate the SS values for the two main effects. These SS values are analogous to the between-group sum of squares values that we calculated when doing [one-way ANOVA](anova). However, it's not a good idea to think of them as between-groups SS values anymore, just because we have two different grouping variables and it's easy to get confused. In order to construct an $F$ test, however, we also need to calculate the within-groups sum of squares. In keeping with the terminology that we used in the [regression chapter](regression)) and the terminology that R uses when printing out the ANOVA table, I'll start referring to the within-groups SS value as the *residual* sum of squares SS$_R$. \n",
    "\n",
    "The easiest way to think about the residual SS values in this context, I think, is to think of it as the leftover variation in the outcome variable after you take into account the differences in the marginal means (i.e., after you remove SS$_A$ and SS$_B$). What I mean by that is we can start by calculating the total sum of squares, which I'll label SS$_T$. The formula for this is pretty much the same as it was for one-way ANOVA: we take the difference between each observation $Y_{rci}$ and the grand mean $\\bar{Y}_{..}$, square the differences, and add them all up\n",
    "\n",
    "$$\n",
    "\\mbox{SS}_T = \\sum_{r=1}^R \\sum_{c=1}^C \\sum_{i=1}^N \\left( Y_{rci} - \\bar{Y}_{..}\\right)^2\n",
    "$$\n",
    "\n",
    "The \"triple summation\" here looks more complicated than it is. In the first two summations, we're summing across all levels of Factor A (i.e., over all possible rows $r$ in our table), across all levels of Factor B (i.e., all possible columns $c$). Each $rc$ combination corresponds to a single group, and each group contains $N$ people: so we have to sum across all those people (i.e., all $i$ values) too. In other words, all we're doing here is summing across all observations in the data set (i.e., all possible $rci$ combinations). \n",
    "\n",
    "At this point, we know the total variability of the outcome variable SS$_T$, and we know how much of that variability can be attributed to Factor A (SS$_A$) and how much of it can be attributed to Factor B (SS$_B$). The residual sum of squares is thus defined to be the variability in $Y$ that *can't* be attributed to either of our two factors. In other words:\n",
    "\n",
    "$$\n",
    "\\mbox{SS}_R = \\mbox{SS}_T - (\\mbox{SS}_A + \\mbox{SS}_B)\n",
    "$$\n",
    "\n",
    "Of course, there is a formula that you can use to calculate the residual SS directly, but I think that it makes more conceptual sense to think of it like this. The whole point of calling it a residual is that it's the leftover variation, and the formula above makes that clear. I should also note that, in keeping with the terminology used in the regression chapter, it is commonplace to refer to $\\mbox{SS}_A + \\mbox{SS}_B$ as the variance attributable to the \"ANOVA model\", denoted SS$_M$, and so we often say that the total sum of squares is equal to the model sum of squares plus the residual sum of squares. Later on in this chapter we'll see that this isn't just a surface similarity: ANOVA and regression are actually the same thing under the hood. \n",
    "\n",
    "In any case, it's probably worth taking a moment to check that we can calculate SS$_R$ using this formula, and verify that we do obtain the same answer that `pingouin` produces in its ANOVA table. The calculations are pretty straightforward. First we calculate the total sum of squares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47f120a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.845"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SS_tot = sum((df['mood_gain'] - grand_mean)**2)\n",
    "round(SS_tot,4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e84ff2b3",
   "metadata": {},
   "source": [
    "and then we use it to calculate the residual sum of squares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74129b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9244"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SS_res = SS_tot - (SS_drug + SS_therapy)\n",
    "round(SS_res,4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a385c1b7",
   "metadata": {},
   "source": [
    "Yet again, we get the same answer. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b16684a6",
   "metadata": {},
   "source": [
    "### What are our degrees of freedom?\n",
    "\n",
    "The degrees of freedom are calculated in much the same way as for one-way ANOVA. For any given factor, the degrees of freedom is equal to the number of levels minus 1 (i.e., $R-1$ for the row variable, Factor A, and $C-1$ for the column variable, Factor B). So, for the `drugs` factor we obtain $df = 2$, and for the `therapy` factor we obtain $df=1$. Later on on, when we discuss the interpretation of [ANOVA as a regression model](anovalm)), I'll give a clearer statement of how we arrive at this number, but for the moment we can use the simple definition of degrees of freedom, namely that the degrees of freedom equals the number of quantities that are observed, minus the number of constraints. So, for the `drugs` factor, we observe 3 separate group means, but these are constrained by 1 grand mean; and therefore the degrees of freedom is 2. For the residuals, the logic is similar, but not quite the same. The total number of observations in our experiment is 18. The constraints correspond to the 1 grand mean, the 2 additional group means that the `drug` factor introduces, and the 1 additional group mean that the the `therapy` factor introduces, and so our degrees of freedom is 14. As a formula, this is $N-1 -(R-1)-(C-1)$, which simplifies to $N-R-C+1$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2345149",
   "metadata": {},
   "source": [
    "### Factorial ANOVA versus one-way ANOVAs\n",
    "\n",
    "Now that we've seen *how* a factorial ANOVA works, it's worth taking a moment to compare it to the results of the one way analyses, because this will give us a really good sense of *why* it's a good idea to run the factorial ANOVA. In the [chapter on 1-way ANOVA](anova), I ran a one-way ANOVA that looked to see if there are any differences between drugs, and a second one-way ANOVA to see if there were any differences between therapies. As we saw in [previously](factanovahyp), the null and alternative hypotheses tested by the one-way ANOVAs are in fact identical to the hypotheses tested by the factorial ANOVA. Looking even more carefully at the ANOVA tables, we can see that the sum of squares associated with the factors are identical in the two different analyses (3.45 for `drug` and 0.92 for `therapy`), as are the degrees of freedom (2 for `drug`, 1 for `therapy`). But they don't give the same answers! Most notably, when we ran the [one-way ANOVA for `therapy`](anovaandt) we didn't find a significant effect (the $p$-value was 0.21). However, when we look at the main effect of `therapy` within the context of the two-way ANOVA, we do get a significant effect ($p=.019$). The two analyses are clearly not the same.\n",
    "\n",
    "Why does that happen? The answer lies in understanding how the *residuals* are calculated. Recall that the whole idea behind an $F$-test is to compare the variability that can be attributed to a particular factor with the variability that cannot be accounted for (the residuals). If you run a one-way ANOVA for `therapy`, and therefore ignore the effect of `drug`, the ANOVA will end up dumping all of the drug-induced variability into the residuals! This has the effect of making the data look more noisy than they really are, and the effect of `therapy` which is correctly found to be significant in the two-way ANOVA now becomes non-significant. If we ignore something that actually matters (e.g., `drug`) when trying to assess the contribution of something else (e.g., `therapy`) then our analysis will be distorted. Of course, it's perfectly okay to ignore variables that are genuinely irrelevant to the phenomenon of interest: if we had recorded the colour of the walls, and that turned out to be non-significant in a three-way ANOVA (i.e. `pg.anova(dv='mood_gain', between=['drug', 'therapy', 'wall_color']`), it would be perfectly okay to disregard it and just report the simpler two-way ANOVA that doesn't include this irrelevant factor. What you shouldn't do is drop variables that actually make a difference! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a676948",
   "metadata": {},
   "source": [
    "### What kinds of outcomes does this analysis capture?\n",
    "\n",
    "The ANOVA model that we've been talking about so far covers a range of different patterns that we might observe in our data. For instance, in a two-way ANOVA design, there are four possibilities: (a) only Factor A matters, (b) only Factor B matters, (c) both A and B matter, and (d) neither A nor B matters. An example of each of these four possibilities is plotted in {numref}`fig-anovas-sans-interaction`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "867640fb",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "levels = ['Level 1', 'Level 1', 'Level 2', 'Level 2']\n",
    "factors = ['Factor B, Level 1', 'Factor B, Level 2', 'Factor B, Level 1', 'Factor B, Level 2']\n",
    "panel1 = [1,1.2,2,2.2]\n",
    "panel2 = [1.2, 1.9, 1.2, 1.9]\n",
    "panel3 = [0.6, 1.4, 1.5, 2.4]\n",
    "panel4 = [1.5, 1.6, 1.5, 1.6]\n",
    "\n",
    "d = pd.DataFrame({'Factor': factors,\n",
    "                   'Factor A': levels,\n",
    "                   'Panel 1': panel1,\n",
    "                   'Panel 2': panel2,\n",
    "                   'Panel 3': panel3,\n",
    "                   'Panel 4': panel4})\n",
    "\n",
    "fig, axes = plt.subplots(2,2)\n",
    "\n",
    "\n",
    "p1 = sns.pointplot(data = d, x = 'Factor A', y = 'Panel 1', hue= 'Factor', errorbar=('ci', False), ax=axes[0,0])\n",
    "p2 = sns.pointplot(data = d, x = 'Factor A', y = 'Panel 2', hue= 'Factor', errorbar=('ci', False), ax=axes[0,1])\n",
    "p3 = sns.pointplot(data = d, x = 'Factor A', y = 'Panel 3', hue= 'Factor', errorbar=('ci', False), ax=axes[1,0])\n",
    "p4 = sns.pointplot(data = d, x = 'Factor A', y = 'Panel 4', hue= 'Factor', errorbar=('ci', False), ax=axes[1,1])\n",
    "\n",
    "ps = [p1, p2, p3, p4]\n",
    "panelID = ['A', 'B', 'C', 'D']\n",
    "titles = ['Only Factor A has an effect', 'Only Factor B has an effect', \n",
    "          'Both A and B have an effect', 'Neither A nor B has an effect']\n",
    "\n",
    "for n, p in enumerate(ps):\n",
    "    p.set_ylim(0,3)\n",
    "    p.set_ylabel('')\n",
    "    p.set_xlabel('')\n",
    "    p.get_legend().remove()\n",
    "    p.text(0.1, 1, titles[n], horizontalalignment='left', verticalalignment='top', transform=p.transAxes)\n",
    "    p.text(0, 1.1, panelID[n], horizontalalignment='left', verticalalignment='top', transform=p.transAxes)\n",
    "    sns.despine()\n",
    "\n",
    "p1.set_xticklabels('')\n",
    "p2.set_xticklabels('')\n",
    "\n",
    "p3.set_xlabel('Factor A')\n",
    "p4.set_xlabel('Factor A')\n",
    "\n",
    "handles, labels = p1.get_legend_handles_labels()\n",
    "fig.legend(handles, ['Factor B, Level 1', 'Factor B, Level 2'], loc='upper center', ncol=2)\n",
    "\n",
    "\n",
    "# Plot figure in book, with caption\n",
    "from myst_nb import glue\n",
    "plt.close(fig)\n",
    "glue(\"anovas-sans-interaction-fig\", fig, display=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1486c14a",
   "metadata": {},
   "source": [
    " ```{glue:figure} anovas-sans-interaction-fig\n",
    ":figwidth: 600px\n",
    ":name: fig-anovas-sans-interaction\n",
    "\n",
    "The four different outcomes for a 2 x 2 ANOVA when no interactions are present. In Panel A we see a main effect of Factor A, and no effect of Factor B. Panel B shows a main effect of Factor B but no effect of Factor A. Panel C shows main effects of both Factor A and Factor B. Finally, Panel D shows no effect of either factor.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc0c6c1b",
   "metadata": {},
   "source": [
    "(interactions)=\n",
    "## Balanced designs, main effects and interactions\n",
    "\n",
    "The four patterns of data shown in {numref}`fig-anovas-sans-interaction` are all quite realistic: there are a great many data sets that produce exactly those patterns. However, they are not the whole story, and the ANOVA model that we have been talking about up to this point is not sufficient to fully account for a table of group means. Why not? Well, so far we have the ability to talk about the idea that drugs can influence mood, and therapy can influence mood, but no way of talking about the possibility of an **_interaction_** between the two. An interaction between A and B is said to occur whenever the effect of Factor A is *different*, depending on which level of Factor B we're talking about. Several examples of an interaction effect with the context of a 2 x 2 ANOVA are shown in {numref}`fig-anovas-avec-interaction`. To give a more concrete example, suppose that the operation of Anxifree and Joyzepam is governed quite different physiological mechanisms, and one consequence of this is that while Joyzepam has more or less the same effect on mood regardless of whether one is in therapy, Anxifree is actually much more effective when administered in conjunction with CBT. The ANOVA that we developed in the previous section does not capture this idea. To get some idea of whether an interaction is actually happening here, it helps to plot the various group means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f30fd859",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "levels = ['Level 1', 'Level 1', 'Level 2', 'Level 2']\n",
    "factors = ['Factor B, Level 1', 'Factor B, Level 2', 'Factor B, Level 1', 'Factor B, Level 2']\n",
    "panel1 = [1,2,2,1]\n",
    "panel2 = [1, 1.1, 1.4, 2]\n",
    "panel3 = [1, 1.1, 1, 2.4]\n",
    "panel4 = [1, 1.4, 1, 2.3]\n",
    "\n",
    "d = pd.DataFrame({'Factor': factors,\n",
    "                   'Factor A': levels,\n",
    "                   'Panel 1': panel1,\n",
    "                   'Panel 2': panel2,\n",
    "                   'Panel 3': panel3,\n",
    "                   'Panel 4': panel4})\n",
    "\n",
    "fig, axes = plt.subplots(2,2)\n",
    "\n",
    "\n",
    "p1 = sns.pointplot(data = d, x = 'Factor A', y = 'Panel 1', hue= 'Factor', errorbar=('ci', False), ax=axes[0,0])\n",
    "p2 = sns.pointplot(data = d, x = 'Factor A', y = 'Panel 2', hue= 'Factor', errorbar=('ci', False), ax=axes[0,1])\n",
    "p3 = sns.pointplot(data = d, x = 'Factor A', y = 'Panel 3', hue= 'Factor', errorbar=('ci', False), ax=axes[1,0])\n",
    "p4 = sns.pointplot(data = d, x = 'Factor A', y = 'Panel 4', hue= 'Factor', errorbar=('ci', False), ax=axes[1,1])\n",
    "\n",
    "ps = [p1, p2, p3, p4]\n",
    "panelID = ['A', 'B', 'C', 'D']\n",
    "titles = ['Crossover interaction', 'Effect for one level of Factor A', \n",
    "          'One cell is different', 'Effect for one level of Factor B']\n",
    "\n",
    "for n, p in enumerate(ps):\n",
    "    p.set_ylim(0,3)\n",
    "    p.set_ylabel('')\n",
    "    p.set_xlabel('')\n",
    "    p.get_legend().remove()\n",
    "    p.text(0.1, 1, titles[n], horizontalalignment='left', verticalalignment='top', transform=p.transAxes)\n",
    "    p.text(0, 1.1, panelID[n], horizontalalignment='left', verticalalignment='top', transform=p.transAxes)\n",
    "    sns.despine()\n",
    "\n",
    "p1.set_xticklabels('')\n",
    "p2.set_xticklabels('')\n",
    "\n",
    "p3.set_xlabel('Factor A')\n",
    "p4.set_xlabel('Factor A')\n",
    "\n",
    "handles, labels = p1.get_legend_handles_labels()\n",
    "fig.legend(handles, ['Factor B, Level 1', 'Factor B, Level 2'], loc='upper center', ncol=2)\n",
    "\n",
    "\n",
    "# Plot figure in book, with caption\n",
    "from myst_nb import glue\n",
    "plt.close(fig)\n",
    "glue(\"anovas-avec-interaction-fig\", fig, display=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8455d86f",
   "metadata": {},
   "source": [
    " ```{glue:figure} anovas-avec-interaction-fig\n",
    ":figwidth: 600px\n",
    ":name: fig-anovas-avec-interaction\n",
    "\n",
    "Qualitatively different interactions for a 2 x 2 ANOVA\n",
    "\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26aaeed1",
   "metadata": {},
   "source": [
    "To give a more concrete example, suppose that the operation of Anxifree and Joyzepam is governed quite different physiological mechanisms, and one consequence of this is that while Joyzepam has more or less the same effect on mood regardless of whether one is in therapy, Anxifree is actually much more effective when administered in conjunction with CBT. The ANOVA that we developed in the previous section does not capture this idea. To get some idea of whether an interaction is actually happening here, it helps to plot the various group means. We can do this easily with `seaborn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "945c56fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.pointplot(data=df, x='drug', y='mood_gain', hue='therapy')\n",
    "sns.despine()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6101b34f",
   "metadata": {},
   "source": [
    "Our main concern relates to the fact that the two lines aren't parallel. The effect of CBT (difference between solid line and dotted line) when the drug is Joyzepam (right side) appears to be near zero, even smaller than the effect of CBT when a placebo is used (left side). However, when Anxifree is administered, the effect of CBT is larger than the placebo (middle). Is this effect real, or is this just random variation due to chance? Our original ANOVA cannot answer this question, because we make no allowances for the idea that interactions even exist! In this section, we'll fix this problem.\n",
    "\n",
    "\n",
    "### What exactly *is* an interaction effect?\n",
    "\n",
    "The key idea that we're going to introduce in this section is that of an interaction effect. `pingouin` has already been calculating the for us, but to keep things simple, I have just ignored them. Now the time has come to look at that _other_ line in the ANOVA table, the one that about \"drug*therapy\".\n",
    "\n",
    "Intuitively, the idea behind an interaction effect is fairly simple: it just means that the effect of Factor A is different, depending on which level of Factor B we're talking about. But what does that actually mean in terms of our data? {numref}`fig-anovas-avec-interaction` depicts several different patterns that, although quite different to each other, would all count as an interaction effect. So it's not entirely straightforward to translate this qualitative idea into something mathematical that a statistician can work with. As a consequence, the way that the idea of an interaction effect is formalised in terms of null and alternative hypotheses is slightly difficult, and I'm guessing that a lot of readers of this book probably won't be all that interested. Even so, I'll try to give the basic idea here.\n",
    "\n",
    "To start with, we need to be a little more explicit about our main effects. Consider the main effect of Factor A (`drug` in our running example). We originally formulated this in terms of the null hypothesis that the two marginal means $\\mu_{r.}$ are all equal to each other. Obviously, if all of these are equal to each other, then they must also be equal to the grand mean $\\mu_{..}$ as well, right? So what we can do is define the *effect* of Factor A at level $r$ to be equal to the difference between the marginal mean $\\mu_{r.}$ and the grand mean $\\mu_{..}$.  \n",
    "\n",
    "Let's denote this effect by $\\alpha_r$, and note that\n",
    "\n",
    "$$\n",
    "\\alpha_r  = \\mu_{r.} - \\mu_{..} \n",
    "$$\n",
    "\n",
    "Now, by definition all of the $\\alpha_r$ values must sum to zero, for the same reason that the average of the marginal means $\\mu_{r.}$ must be the grand mean $\\mu_{..}$. We can similarly define the effect of Factor B at level $i$ to be the difference between the column marginal mean $\\mu_{.c}$ and the grand mean $\\mu_{..}$\n",
    "\n",
    "$$\n",
    "\\beta_c = \\mu_{.c} - \\mu_{..}\n",
    "$$\n",
    "\n",
    "and once again, these $\\beta_c$ values must sum to zero. The reason that statisticians sometimes like to talk about the main effects in terms of these $\\alpha_r$ and $\\beta_c$ values is that it allows them to be precise about what it means to say that there is no interaction effect. If there is no interaction at all, then these $\\alpha_r$ and $\\beta_c$ values will perfectly describe the group means $\\mu_{rc}$. Specifically, it means that\n",
    "\n",
    "$$\n",
    "\\mu_{rc} = \\mu_{..} + \\alpha_r + \\beta_c \n",
    "$$\n",
    "\n",
    "That is, there's nothing *special* about the group means that you couldn't predict perfectly by knowing all the marginal means. And that's our null hypothesis, right there. The alternative hypothesis is that\n",
    "\n",
    "$$\n",
    "\\mu_{rc} \\neq \\mu_{..} + \\alpha_r + \\beta_c \n",
    "$$\n",
    "\n",
    "for at least one group $rc$ in our table. However, statisticians often like to write this slightly differently. They'll usually define the specific interaction associated with group $rc$ to be some number, awkwardly referred to as $(\\alpha\\beta)_{rc}$, and then they will say that the alternative hypothesis is that \n",
    "\n",
    "$$\n",
    "\\mu_{rc} = \\mu_{..} + \\alpha_r + \\beta_c + (\\alpha\\beta)_{rc}\n",
    "$$\n",
    "\n",
    "where $(\\alpha\\beta)_{rc}$ is non-zero for at least one group. This notation is kind of ugly to look at, but it is handy as we'll see in the next section when discussing how to calculate the sum of squares.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b137159",
   "metadata": {},
   "source": [
    "### Calculating sums of squares for the interaction\n",
    "\n",
    "How should we calculate the sum of squares for the interaction terms, SS$_{A:B}$? \n",
    "\n",
    "Well, first off, it helps to notice how the previous section defined the interaction effect in terms of the extent to which the actual group means differ from what you'd expect by just looking at the marginal means. Of course, all of those formulas refer to population parameters rather than sample statistics, so we don't actually know what they are. However, we can estimate them by using sample means in place of population means. So for Factor A, a good way to estimate the main effect at level $r$ as the difference between the *sample* marginal mean $\\bar{Y}_{rc}$ and the sample grand mean $\\bar{Y}_{..}$. That is, we would use this as our estimate of the effect:\n",
    "\n",
    "$$\n",
    "\\hat{\\alpha}_r = \\bar{Y}_{r.} - \\bar{Y}_{..}\n",
    "$$\n",
    "\n",
    "Similarly, our estimate of the main effect of Factor B at level $c$ can be defined as follows:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_c = \\bar{Y}_{.c} - \\bar{Y}_{..}\n",
    "$$\n",
    "\n",
    "Now, if you go back to the formulas that I used to describe the SS values for the two main effects, you'll notice that these effect terms are exactly the quantities that we were squaring and summing! So what's the analog of this for interaction terms? The answer to this can be found by first rearranging the formula for the group means $\\mu_{rc}$ under the alternative hypothesis, so that we get this:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "(\\alpha \\beta)_{rc} &= \\mu_{rc} - \\mu_{..} - \\alpha_r - \\beta_c \\\\\n",
    "                &= \\mu_{rc} - \\mu_{..} - (\\mu_{r.} - \\mu_{..}) - (\\mu_{.c} - \\mu_{..}) \\\\\n",
    "                & =  \\mu_{rc} - \\mu_{r.} - \\mu_{.c} + \\mu_{..}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "So, once again, if we substitute our sample statistics in place of the population means, we get the following as our estimate of the interaction effect for group $rc$, which is\n",
    "\n",
    "$$\n",
    "\\hat{(\\alpha\\beta)}_{rc} = \\bar{Y}_{rc} - \\bar{Y}_{r.} - \\bar{Y}_{.c} + \\bar{Y}_{..}\n",
    "$$\n",
    "\n",
    "Now all we have to do is sum all of these estimates across all $R$ levels of Factor A and all $C$ levels of Factor B, and we obtain the following formula for the sum of squares associated with the interaction as a whole:\n",
    "\n",
    "$$\n",
    "\\mbox{SS}_{A:B} = N \\sum_{r=1}^R \\sum_{c=1}^C \\left( \\bar{Y}_{rc} - \\bar{Y}_{r.} - \\bar{Y}_{.c} + \\bar{Y}_{..} \\right)^2\n",
    "$$\n",
    "\n",
    "where, we multiply by $N$ because there are $N$ observations in each of the groups, and we want our SS values to reflect the variation among *observations* accounted for by the interaction, not the variation among groups. \n",
    "\n",
    "\n",
    "Now that we have a formula for calculating SS$_{A:B}$, it's important to recognise that the interaction term is part of the model (of course), so the total sum of squares associated with the model, SS$_M$ is now equal to the sum of the three relevant SS values, $\\mbox{SS}_A + \\mbox{SS}_B + \\mbox{SS}_{A:B}$. The residual sum of squares $\\mbox{SS}_R$ is still defined as the leftover variation, namely $\\mbox{SS}_T - \\mbox{SS}_M$, but now that we have the interaction term this becomes\n",
    "\n",
    "$$\n",
    "\\mbox{SS}_R = \\mbox{SS}_T - (\\mbox{SS}_A + \\mbox{SS}_B + \\mbox{SS}_{A:B})\n",
    "$$ \n",
    "\n",
    "\n",
    "As a consequence, the residual sum of squares SS$_R$ will be smaller than in our original ANOVA that didn't include interactions.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f674b2f",
   "metadata": {},
   "source": [
    "### Degrees of freedom for the interaction\n",
    "\n",
    "Calculating the degrees of freedom for the interaction is, once again, slightly trickier than the corresponding calculation for the main effects. To start with, let's think about the ANOVA model as a whole. Once we include interaction effects in the model, we're allowing every single group to have a unique mean, $\\mu_{rc}$. For an $R \\times C$ factorial ANOVA, this means that there are $R \\times C$ quantities of interest in the model, and only the one constraint: all of the group means need to average out to the grand mean. So the model as a whole needs to have $(R\\times C) - 1$ degrees of freedom. But the main effect of Factor A has $R-1$ degrees of freedom, and the main effect of Factor B has $C-1$ degrees of freedom. Which means that the degrees of freedom associated with the interaction is \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "df_{A:B} &= (R\\times C - 1) - (R - 1) - (C -1 ) \\\\\n",
    "        &= RC - R - C + 1 \\\\\n",
    "        &= (R-1)(C-1)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which is just the product of the degrees of freedom associated with the row factor and the column factor.\n",
    "\n",
    "What about the residual degrees of freedom? Because we've added interaction terms, which absorb some degrees of freedom, there are fewer residual degrees of freedom left over. Specifically, note that if the model with interaction has a total of $(R\\times C) - 1$, and there are $N$ observations in your data set that are constrained to satisfy 1 grand mean, your residual degrees of freedom now become $N-(R \\times C)-1+1$, or just $N-(R \\times C)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6eee58bc",
   "metadata": {},
   "source": [
    "### Interpreting the results\n",
    "\n",
    "Now we are in a position to understand all the rows in the ANOVA table that we saw earlier. Even if you skimmed lightly over the math in the previous section (and you would be forgiven if you did), hopefully you will now have some intuition for the differences between main effects and interaction effects. But what does it all mean, in the end?\n",
    "\n",
    "Let's quickly remind ourselves of the results of our factorial ANOVA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c22ad370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>SS</th>\n",
       "      <th>DF</th>\n",
       "      <th>MS</th>\n",
       "      <th>F</th>\n",
       "      <th>p-unc</th>\n",
       "      <th>np2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drug</td>\n",
       "      <td>3.453</td>\n",
       "      <td>2</td>\n",
       "      <td>1.727</td>\n",
       "      <td>31.714</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>therapy</td>\n",
       "      <td>0.467</td>\n",
       "      <td>1</td>\n",
       "      <td>0.467</td>\n",
       "      <td>8.582</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>drug * therapy</td>\n",
       "      <td>0.271</td>\n",
       "      <td>2</td>\n",
       "      <td>0.136</td>\n",
       "      <td>2.490</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Residual</td>\n",
       "      <td>0.653</td>\n",
       "      <td>12</td>\n",
       "      <td>0.054</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Source     SS  DF     MS       F  p-unc    np2\n",
       "0            drug  3.453   2  1.727  31.714  0.000  0.841\n",
       "1         therapy  0.467   1  0.467   8.582  0.013  0.417\n",
       "2  drug * therapy  0.271   2  0.136   2.490  0.125  0.293\n",
       "3        Residual  0.653  12  0.054     NaN    NaN    NaN"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = pg.anova(dv='mood_gain', between=['drug', 'therapy'], data=df, detailed=True)\n",
    "round(model2, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73ca4349",
   "metadata": {},
   "source": [
    "We can now see that while we do have a significant main effect of drug ($F_{2,12} = 31.7, p <.001$) and therapy type ($F_{1,12} = 8.6, p=.013$), there is no significant interaction between the two ($F_{2,12} = 2.5, p = 0.125$)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "836263e3",
   "metadata": {},
   "source": [
    "There's a couple of very important things to consider when interpreting the results of factorial ANOVA. Firstly, there's the same issue that we had with one-way ANOVA, which is that if you obtain a significant main effect of (say) `drug`, it doesn't tell you anything about which drugs are different to one another. To find that out, you need to run additional analyses. We'll talk about some analyses that you can run in the sections on [contrasts](contrasts) and [post-hoc tests](posthoc2). The same is true for interaction effects: knowing that there's a significant interaction doesn't tell you anything about what kind of interaction exists. Again, you'll need to run additional analyses. \n",
    "\n",
    "Secondly, there's a very peculiar interpretation issue that arises when you obtain a significant interaction effect but no corresponding main effect. This happens sometimes. For instance, in the crossover interaction shown in {numref}`fig-anovas-avec-interaction`), this is exactly what you'd find: in this case, neither of the main effects would be significant, but the interaction effect would be. This is a difficult situation to interpret, and people often get a bit confused about it. The general advice that statisticians like to give in this situation is that you shouldn't pay much attention to the main effects when an interaction is present. The reason they say this is that, although the tests of the main effects are perfectly valid from a mathematical point of view, when there is a significant interaction effect, the main effects rarely test interesting hypotheses. [Recall](factanovahyp) that the null hypothesis for a main effect is that the *marginal means* are equal to each other, and that a marginal mean is formed by averaging across several different groups. But if you have a significant interaction effect, then you *know* that the groups that comprise the marginal mean aren't homogeneous, so it's not really obvious why you would even care about those marginal means. \n",
    "\n",
    "Here's what I mean. Again, let's stick with a clinical example. Suppose that we had a $2 \\times 2$ design comparing two different treatments for phobias (e.g., systematic desensitisation vs flooding), and two different anxiety reducing drugs (e.g., Anxifree vs Joyzepam). Now suppose what we found was that Anxifree had no effect when desensitisation was the treatment, and Joyzepam had no effect when flooding was the treatment. But both were pretty effective for the other treatment. This is a classic crossover interaction, and what we'd find when running the ANOVA is that there is no main effect of drug, but a significant interaction. Now, what does it actually *mean* to say that there's no main effect? Well, it means that, if we average over the two different psychological treatments, then the *average* effect of Anxifree and Joyzepam is the same. But why would anyone care about that? When treating someone for phobias, it is never the case that a person can be treated using an \"average\" of flooding and desensitisation: that doesn't make a lot of sense. You either get one or the other. For one treatment, one drug is effective; and for the other treatment, the other drug is effective. The interaction is the important thing; the main effect is kind of irrelevant. \n",
    "\n",
    "This sort of thing happens a lot: the main effect are tests of marginal means, and when an interaction is present we often find ourselves not being terribly interested in marginal means, because they imply averaging over things that the interaction tells us shouldn't be averaged! Of course, it's not always the case that a main effect is meaningless when an interaction is present. Often you can get a big main effect and a very small interaction, in which case you can still say things like \"drug A is generally more effective than drug B\" (because there was a big effect of drug), but you'd need to modify it a bit by adding that \"the difference in effectiveness was different for different psychological treatments\". In any case, the main point here is that whenever you get a significant interaction you should stop and *think* about what the main effect actually means in this context. Don't automatically assume that the main effect is interesting. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07bd5740",
   "metadata": {},
   "source": [
    "### Effect sizes\n",
    "\n",
    "The effect size calculations for a factorial ANOVA is pretty similar to [those used in one way ANOVA](etasquared). Specifically, we can use $\\eta^2$ (eta-squared) as simple way to measure how big the overall effect is for any particular term. As before, $\\eta^2$ is defined by dividing the sum of squares associated with that term by the total sum of squares. For instance, to determine the size of the main effect of Factor A, we would use the following formula\n",
    "\n",
    "$$\n",
    "\\eta_A^2 = \\frac{\\mbox{SS}_{A}}{\\mbox{SS}_{T}}\n",
    "$$\n",
    "\n",
    "As before, this can be interpreted in much the same way as $R^2$ in regression.[^R]  It tells you the proportion of variance in the outcome variable that can be accounted for by the main effect of Factor A. It is therefore a number that ranges from 0 (no effect at all) to 1 (accounts for *all* of the variability in the outcome). Moreover, the sum of all the $\\eta^2$ values, taken across all the terms in the model, will sum to the the total $R^2$ for the ANOVA model. If, for instance, the ANOVA model fits perfectly (i.e., there is no within-groups variability at all!), the $\\eta^2$ values will sum to 1. Of course, that rarely if ever happens in real life.\n",
    "\n",
    "However, when doing a factorial ANOVA, there is a second measure of effect size that people like to report, known as partial $\\eta^2$. This is the effect size measure that `pingouin` gives you by default. The idea behind partial $\\eta^2$ (which is sometimes denoted $_p\\eta^2$ or $\\eta^2_p$) is that, when measuring the effect size for a particular term (say, the main effect of Factor A), you want to deliberately ignore the other effects in the model (e.g., the main effect of Factor B). That is, you would pretend that the effect of all these other terms is zero, and then calculate what the $\\eta^2$ value would have been. This is actually pretty easy to calculate. All you have to do is remove the sum of squares associated with the other terms from the denominator. In other words, if you want the partial $\\eta^2$ for the main effect of Factor A, the denominator is just the sum of the SS values for Factor A and the residuals:\n",
    "\n",
    "$$\n",
    "\\mbox{partial } \\eta^2_A = \\frac{\\mbox{SS}_{A}}{\\mbox{SS}_{A} + \\mbox{SS}_{R}}\n",
    "$$\n",
    "\n",
    "This will always give you a larger number than $\\eta^2$, which the cynic in me suspects accounts for the popularity of partial $\\eta^2$. And once again you get a number between 0 and 1, where 0 represents no effect. However, it's slightly trickier to interpret what a large partial $\\eta^2$ value means. In particular, you can't actually compare the partial $\\eta^2$ values across terms! Suppose, for instance, there is no within-groups variability at all: if so, SS$_R = 0$. What that means is that *every* term has a partial $\\eta^2$ value of 1. But that doesn't mean that all terms in your model are equally important, or indeed that they are equally large. All it mean is that all terms in your model have effect sizes that are large *relative to the residual variation*. It is not comparable across terms.\n",
    "\n",
    "[^R]: This chapter seems to be setting a new record for the number of different things that the letter R can stand for: so far we have R referring to the number of rows in our table of means, the residuals in the model, and now the correlation coefficient in a regression. Sorry: we clearly don't have enough letters in the alphabet. However, I've tried pretty hard to be clear on which thing R is referring to in each case.\n",
    "\n",
    "To see what I mean by this, it's useful to see a concrete example. We can get `pingouin` to report $\\eta^2$ instead of partial $\\eta^2$ by specifying this in the `effsize` argument when we run our ANOVA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e24f51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>SS</th>\n",
       "      <th>DF</th>\n",
       "      <th>MS</th>\n",
       "      <th>F</th>\n",
       "      <th>p-unc</th>\n",
       "      <th>n2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drug</td>\n",
       "      <td>3.453</td>\n",
       "      <td>2</td>\n",
       "      <td>1.727</td>\n",
       "      <td>31.714</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>therapy</td>\n",
       "      <td>0.467</td>\n",
       "      <td>1</td>\n",
       "      <td>0.467</td>\n",
       "      <td>8.582</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>drug * therapy</td>\n",
       "      <td>0.271</td>\n",
       "      <td>2</td>\n",
       "      <td>0.136</td>\n",
       "      <td>2.490</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Residual</td>\n",
       "      <td>0.653</td>\n",
       "      <td>12</td>\n",
       "      <td>0.054</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Source     SS  DF     MS       F  p-unc     n2\n",
       "0            drug  3.453   2  1.727  31.714  0.000  0.713\n",
       "1         therapy  0.467   1  0.467   8.582  0.013  0.096\n",
       "2  drug * therapy  0.271   2  0.136   2.490  0.125  0.056\n",
       "3        Residual  0.653  12  0.054     NaN    NaN    NaN"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = pg.anova(dv='mood_gain', \n",
    "                  between=['drug', 'therapy'], \n",
    "                  data=df, \n",
    "                  detailed=True, \n",
    "                  effsize='n2')\n",
    "round(model2, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2641e95",
   "metadata": {},
   "source": [
    "Looking at the $\\eta^2$ values first, we see that `drug` accounts for 71.3\\% of the variance (i.e. $\\eta^2 = 0.713$) in `mood_gain`, whereas `therapy` only accounts for 9.6\\%. This leaves a total of 19.1\\% of the variation unaccounted for (i.e., the residuals constitute 19.1\\% of the variation in the outcome). Overall, this implies that we have a very large effect[^bigeffect] of `drug` and a modest effect of `therapy`. \n",
    "\n",
    "[^bigeffect]: Implausibly large, I would think: the artificiality of this data set is really starting to show!\n",
    "\n",
    "Now let's look at the partial $\\eta^2$ values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0de77fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>SS</th>\n",
       "      <th>DF</th>\n",
       "      <th>MS</th>\n",
       "      <th>F</th>\n",
       "      <th>p-unc</th>\n",
       "      <th>np2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drug</td>\n",
       "      <td>3.453</td>\n",
       "      <td>2</td>\n",
       "      <td>1.727</td>\n",
       "      <td>31.714</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>therapy</td>\n",
       "      <td>0.467</td>\n",
       "      <td>1</td>\n",
       "      <td>0.467</td>\n",
       "      <td>8.582</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>drug * therapy</td>\n",
       "      <td>0.271</td>\n",
       "      <td>2</td>\n",
       "      <td>0.136</td>\n",
       "      <td>2.490</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Residual</td>\n",
       "      <td>0.653</td>\n",
       "      <td>12</td>\n",
       "      <td>0.054</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Source     SS  DF     MS       F  p-unc    np2\n",
       "0            drug  3.453   2  1.727  31.714  0.000  0.841\n",
       "1         therapy  0.467   1  0.467   8.582  0.013  0.417\n",
       "2  drug * therapy  0.271   2  0.136   2.490  0.125  0.293\n",
       "3        Residual  0.653  12  0.054     NaN    NaN    NaN"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = pg.anova(dv='mood_gain', \n",
    "                  between=['drug', 'therapy'], \n",
    "                  data=df, \n",
    "                  detailed=True, \n",
    "                  effsize='np2')\n",
    "round(model2, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca624119",
   "metadata": {},
   "source": [
    "Because the effect of `therapy` isn't all that large, controlling for it doesn't make much of a difference, so the partial $\\eta^2$ for `drug` doesn't increase very much, and we obtain a value of $_p\\eta^2 = 0.789$). In contrast, because the effect of `drug` was very large, controlling for it makes a big difference, and so when we calculate the partial $\\eta^2$ for `therapy` you can see that it rises to $_p\\eta^2 = 0.336$. The question that we have to ask ourselves is, what do these partial $\\eta^2$ values actually *mean*? The way I generally interpret the partial $\\eta^2$ for the main effect of Factor A is to interpret it as a statement about a hypothetical experiment in which *only* Factor A was being varied. So, even though in *this* experiment we varied both A and B, we can easily imagine an experiment in which only Factor A was varied: the partial $\\eta^2$ statistic tells you how much of the variance in the outcome variable you would expect to see accounted for in that experiment. However, it should be noted that this interpretation -- like many things associated with main effects -- doesn't make a lot of sense when there is a large and significant interaction effect. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45fbeceb",
   "metadata": {},
   "source": [
    "(factorialanovaassumptions)=\n",
    "## Assumption checking\n",
    "\n",
    "As with one-way ANOVA, the key assumptions of factorial ANOVA are homogeneity of variance (all groups have the same standard deviation), normality of the residuals, and independence of the observations. The first two are things we can test for. The third is something that you need to assess yourself by asking if there are any special relationships between different observations. What about homogeneity of variance and normality of the residuals? As it turns out, these are pretty easy to check: it's no different to the checks we did for a one-way ANOVA.\n",
    "\n",
    "### Levene test for homogeneity of variance\n",
    "\n",
    "To test whether the groups have the same variance, we can use the Levene test. The theory behind the Levene test has [already been discussed](levene), so I won't discuss it again. The funny thing is, though, that while pacakges exist to perform a Levene test on factorial ANOVA models in e.g. R, I have not been able to find a package that does the same in Python. As of the date of writing this, I can only find examples of Levene's tests being performed on one-way models in Python. However, as far as I can tell, all that one needs to do is to create a new column in the dataframe that represents the interaction of the two factors, and run the Levene's test on that. This seems to give the same result as e.g. the R function `levene()` run on a two-way model with interactions, so I offer it here as my solution to this problem:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "352bfe32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>W</th>\n",
       "      <th>pval</th>\n",
       "      <th>equal_var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>levene</th>\n",
       "      <td>0.09545</td>\n",
       "      <td>0.99123</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              W     pval  equal_var\n",
       "levene  0.09545  0.99123       True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df['interaction'] = df['drug']+df['therapy']\n",
    "\n",
    "pg.homoscedasticity(data=df, \n",
    "                    dv='mood_gain', \n",
    "                    group='interaction').round(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1249612f",
   "metadata": {},
   "source": [
    "The fact that the Levene test is non-significant means that we can safely assume that the homogeneity of variance assumption is not violated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ac77ad3",
   "metadata": {},
   "source": [
    "### Normality of residuals\n",
    "\n",
    "As with one-way ANOVA, we can test for the normality of residuals in a [straightforward fashion](anovanormality). At the time of writing, `pingouin` doesn't have a way to extract the residuals, so we will need to use `statsmodels` to run our ANOVA first, and then use the `.resid` method to extract the residuals from the model. Once we have done that, we can examine those residuals in a few different ways. It's generally a good idea to examine them graphically, by drawing histograms (i.e., `sns.histplot()` function) and QQ plots (i.e., `pg.qqplot()` function. If you want a formal test for the normality of the residuals, then we can run the Shapiro-Wilk test (i.e., `pg.normality()`). If we wanted to check the residuals with respect to our two-way ANOVA (including interactions), we could first get the residuals using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9badcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "formula = 'mood_gain ~ drug + therapy + drug:therapy'\n",
    "\n",
    "model = ols(formula, data=df).fit()\n",
    "res = model.resid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b84077fb",
   "metadata": {},
   "source": [
    "As you can see from the `formula` variable above, in `statsmodels` we need to explicitly ask it to model the interactions for us, while `pingouin` does this automatically. We specify the interaction using `:`, so the interaction of `drug` and `therapy`is written as `drug:therapy`.\n",
    "\n",
    "Now we can do our normality test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ee2f46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>W</th>\n",
       "      <th>pval</th>\n",
       "      <th>normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.928816</td>\n",
       "      <td>0.185093</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          W      pval  normal\n",
       "0  0.928816  0.185093    True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg.normality(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b86b61a9",
   "metadata": {},
   "source": [
    "I havenâ€™t included the plots (you can draw them yourself if you want to see them), but you can see from the non-significance of the Shapiro-Wilk test that normality isnâ€™t violated here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33423cb2",
   "metadata": {},
   "source": [
    "(omnibusF)=\n",
    "## The $F$ test as a model comparison\n",
    "\n",
    "At this point, I want to talk in a little more detail about what the $F$-tests in an ANOVA are actually doing. In the context of ANOVA, I've been referring to the $F$-test as a way of testing whether a particular term in the model (e.g., main effect of Factor A) is significant. This interpretation is perfectly valid, but it's not necessarily the most useful way to think about the test. In fact, it's actually a fairly limiting way of thinking about what the $F$-test does. Consider the clinical trial data we've been working with in this chapter. Suppose I want to see if there are *any* effects of any kind that involve `therapy`. I'm not fussy: I don't care if it's a main effect or an interaction effect.[^imagine] One thing I could do is look at the output for `model2` earlier: in this model we did see a main effect of therapy ($p=.013$) but we did not see an interaction effect ($p=.125$). That's kind of telling us what we want to know, but it's not quite the same thing. What we really want is a single test that *jointly* checks the main effect of therapy and the interaction effect. \n",
    "\n",
    "Given the way that I've been describing the ANOVA $F$-test up to this point, you'd be tempted to think that this isn't possible. On the other hand, if you recall the [chapter on regression](modelselreg), we were able to use $F$-tests to make comparisons between a wide variety of regression models. Perhaps something of that sort is possible with ANOVA? And of course, the answer here is yes. The thing that you really need to understand is that the $F$-test, as it is used in both ANOVA and regression, is really a comparison of *two* statistical models. One of these models is the full model (alternative hypothesis), and the other model is a simpler model that is missing one or more of the terms that the full model includes (null hypothesis). The null model cannot contain any terms that are not in the full model. In the example I gave above, the full model is `model2`, and it contains a main effect for therapy, a main effect for drug, and the drug by therapy interaction term. The null model would be `model1` since it contains only the main effect of drug. \n",
    "\n",
    "### The $F$ test comparing two models\n",
    "\n",
    "Let's frame this in a slightly more abstract way. We'll say that our full model can be written as a `statsmodels` formula that contains several different terms, say `Y ~ A + B + C + D` (remember that `~` here can be read as \"predicted by\"). Our null model only contains some subset of these terms, say `Y ~ A + B`. Some of these terms might be main effect terms, others might be interaction terms. It really doesn't matter. The only thing that matters here is that we want to treat some of these terms as the \"starting point\" (i.e. the terms in the null model, `A` and `B`), and we want to see if including the other terms (i.e., `C` and `D`) leads to a significant improvement in model performance, over and above what could be achieved by a model that includes only `A` and `B`.\n",
    "\n",
    "\n",
    "[^imagine]: There could be all sorts of reasons for doing this, I would imagine."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7964f149",
   "metadata": {},
   "source": [
    "Is there a way of making this comparison directly?\n",
    "\n",
    "To answer this, let's go back to fundamentals. As we saw back when we learned about [one-way ANOVA](ANOVA), the $F$-test is constructed from two kinds of quantity: sums of squares (SS) and degrees of freedom (df). These two things define a mean square value (MS = SS/df), and we obtain our $F$ statistic by contrasting the MS value associated with \"the thing we're interested in\" (the model) with the MS value associated with \"everything else\" (the residuals). What we want to do is figure out how to talk about the SS value that is associated with the *difference* between two models. It's actually not all that hard to do. \n",
    "\n",
    "Let's start with the fundamental rule that we used throughout the chapter on regression:\n",
    "\n",
    "$$\n",
    "\\mbox{SS}_{T} = \\mbox{SS}_{M} + \\mbox{SS}_{R} \n",
    "$$\n",
    "\n",
    "That is, the total sums of squares (i.e., the overall variability of the outcome variable) can be decomposed into two parts: the variability associated with the model $\\mbox{SS}_{M}$, and the residual or leftover variability, $\\mbox{SS}_{R}$. However, it's kind of useful to rearrange this equation slightly, and say that the SS value associated with a model is defined like this...\n",
    "\n",
    "$$\n",
    "\\mbox{SS}_{M} = \\mbox{SS}_{T} - \\mbox{SS}_{R} \n",
    "$$\n",
    "\n",
    "Now, in our scenario, we have two models: the null model (M0) and the full model (M1):\n",
    "\n",
    "$$\n",
    "\\mbox{SS}_{M0} = \\mbox{SS}_{T} - \\mbox{SS}_{R0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mbox{SS}_{M1} = \\mbox{SS}_{T} - \\mbox{SS}_{R1} \n",
    "$$\n",
    "\n",
    "Next, let's think about what it is we *actually* care about here. What we're interested in is the *difference* between the full model and the null model. So, if we want to preserve the idea that what we're doing is an \"analysis of the variance\" (ANOVA) in the outcome variable, what we should do is define the SS associated with the difference to be equal to the difference in the SS:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mbox{SS}_{\\Delta} &= \\mbox{SS}_{M1} - \\mbox{SS}_{M0}\\\\\n",
    "&= (\\mbox{SS}_{T} - \\mbox{SS}_{R1}) - (\\mbox{SS}_{T} - \\mbox{SS}_{R0} ) \\\\\n",
    "&= \\mbox{SS}_{R0} - \\mbox{SS}_{R1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now that we have our degrees of freedom, we can calculate mean squares and $F$ values in the usual way. Specifically, we're interested in the mean square for the difference between models, and the mean square for the residuals associated with the *full* model (M1), which are given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mbox{MS}_{\\Delta} &= \\frac{\\mbox{SS}_{\\Delta} }{ \\mbox{df}_{\\Delta} } \\\\\n",
    "\\mbox{MS}_{R1} &= \\frac{ \\mbox{SS}_{R1} }{  \\mbox{df}_{R1} }\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Finally, taking the ratio of these two gives us our $F$ statistic:\n",
    "\n",
    "$$\n",
    "F = \\frac{\\mbox{MS}_\\Delta} {\\mbox{MS}_{R1}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b43a3623",
   "metadata": {},
   "source": [
    "### Running the test in Python\n",
    "\n",
    "At this point, it may help to go back to our concrete example. The null model here is `model1`, which stipulates that there is a main effect of drug, but no other effects exist. We expressed this via the model formula `mood_gain ~ drug`. The alternative model here is `model3`, which stipulates that there is a main effect of drug, a main effect of therapy, and an interaction. This model corresponds to the formula `mood_gain ~ drug + therapy + drug:therapy`. The key thing here is that if we compare `model1` to `model3`, weâ€™re lumping the main effect of therapy and the interaction term together. Running this test in Python is straightforward: we just input both models to the `anova_lm` function from `statsmodels`, and it will run the exact F -test that I outlined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f770232c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df_resid</th>\n",
       "      <th>ssr</th>\n",
       "      <th>df_diff</th>\n",
       "      <th>ss_diff</th>\n",
       "      <th>F</th>\n",
       "      <th>Pr(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.391667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.738333</td>\n",
       "      <td>4.520408</td>\n",
       "      <td>0.024242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   df_resid       ssr  df_diff   ss_diff         F    Pr(>F)\n",
       "0      15.0  1.391667      0.0       NaN       NaN       NaN\n",
       "1      12.0  0.653333      3.0  0.738333  4.520408  0.024242"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/ethanweed/pythonbook/main/Data/clintrial.csv\")\n",
    "\n",
    "formula1 = 'mood_gain ~ drug'\n",
    "formula2 = 'mood_gain ~ drug + therapy + drug:therapy'\n",
    "\n",
    "model1 = ols(formula1, data=df).fit()\n",
    "model2 = ols(formula2, data=df).fit()\n",
    "\n",
    "anovaResults = anova_lm(model1, model2)\n",
    "anovaResults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654a0c87",
   "metadata": {},
   "source": [
    "Let's see if we can reproduce this $F$-test ourselves. Firstly, if you go back and look at the ANOVA tables that we printed out for `model1` and `model3` you can reassure yourself that the RSS values printed in this table really do correspond to the residual sum of squares associated with these two models. So let's type them in as variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8ec6850",
   "metadata": {},
   "outputs": [],
   "source": [
    " ss_res_null = 1.391667\n",
    " ss_res_full = 0.653333"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91df920f",
   "metadata": {},
   "source": [
    "Now, following the procedure that I described above, we will say that the \"between model\" sum of squares, is the difference between these two residual sum of squares values. So, if we do the subtraction, we discover that the sum of squares associated with those terms that appear in the full model but not the null model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9dcc4fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7383339999999999"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ss_diff = ss_res_null - ss_res_full \n",
    " ss_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89b4380",
   "metadata": {},
   "source": [
    "Right. Next, as always we need to convert these SS values into MS (mean square) values, which we do by dividing by the degrees of freedom. The degrees of freedom associated with the full-model residuals hasn't changed from our original ANOVA for `model2`: it's the total sample size $N$, minus the total number of groups $G$ that are relevant to the model. We have 18 people in the trial and 6 possible groups (i.e., 2 therapies $\\times$ 3 drugs), so the degrees of freedom here is 12. The degrees of freedom for the null model are calculated similarly. The only difference here is that there are only 3 relevant groups (i.e., 3 drugs), so the degrees of freedom here is 15. And, because the degrees of freedom associated with the difference is equal to the difference in the two degrees of freedom, we arrive at the conclusion that we have $15-12 = 3$ degrees of freedom. Now that we know the degrees of freedom, we can calculate our MS values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a107207",
   "metadata": {},
   "outputs": [],
   "source": [
    " ms_res = ss_res_full / 12\n",
    " ms_diff = ss_diff / 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e951e0f8",
   "metadata": {},
   "source": [
    "Okay, now that we have our two MS values, we can divide one by the other, and obtain an $F$-statistic ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "644ccf98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.520414551231913"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " F_stat = ms_diff / ms_res \n",
    " F_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5226a3e",
   "metadata": {},
   "source": [
    "... and, just as we had hoped, this turns out to be nearly identical to the $F$-statistic that the `anova_lm()` function produced earlier. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1c074c0",
   "metadata": {},
   "source": [
    "(anovalm)=\n",
    "## ANOVA as a linear model\n",
    "\n",
    "One of the most important things to understand about ANOVA and regression is that they're basically the same thing. On the surface of it, you wouldn't think that this is true: after all, the way that I've described them so far suggests that ANOVA is primarily concerned with testing for group differences, and regression is primarily concerned with understanding the correlations between variables. And as far as it goes, that's perfectly true. But when you look under the hood, so to speak, the underlying mechanics of ANOVA and regression are awfully similar. In fact, if you think about it, you've already seen evidence of this. ANOVA and regression both rely heavily on sums of squares (SS), both make use of $F$ tests, and so on. Looking back, it's hard to escape the feeling that the chapters on ANOVA and regression were a bit repetitive. \n",
    "\n",
    "The reason for this is that ANOVA and regression are both kinds of **_linear models_**. In the case of regression, this is kind of obvious. The regression equation that we use to define the relationship between predictors and outcomes *is* the equation for a straight line, so it's quite obviously a linear model. When we use e.g. a `pingouin` command like `pg.linear_regression([predictor1, predictor2], outcome)` what we're really working with is the somewhat uglier linear model:\n",
    "\n",
    "$$\n",
    "Y_{p} = b_1 X_{1p} + b_2 X_{2p} + b_0 + \\epsilon_{p}\n",
    "$$\n",
    "\n",
    "where $Y_p$ is the outcome value for the $p$-th observation (e.g., $p$-th person), $X_{1p}$ is the value of the first predictor for the $p$-th observation, $X_{2p}$ is the value of the second predictor for the $p$-th observation, the $b_1$, $b_2$ and $b_0$ terms are our regression coefficients, and $\\epsilon_{p}$ is the $p$-th residual. If we ignore the residuals $\\epsilon_{p}$ and just focus on the regression line itself, we get the following formula:\n",
    "\n",
    "$$\n",
    "\\hat{Y}_{p} = b_1 X_{1p} + b_2 X_{2p} + b_0\n",
    "$$\n",
    "\n",
    "where $\\hat{Y}_p$ is the value of $Y$ that the regression line predicts for person $p$, as opposed to the actually-observed value $Y_p$. The thing that isn't immediately obvious is that we can write ANOVA as a linear model as well. However, it's actually pretty straightforward to do this. Let's start with a really simple example: rewriting a $2 \\times 2$ factorial ANOVA as a linear model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5129dee9",
   "metadata": {},
   "source": [
    "(changingbaseline)=\n",
    "### Changing the baseline category"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8cb1b8a",
   "metadata": {},
   "source": [
    "### The equivalence between ANOVA and regression for non-binary factors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "023eb81c",
   "metadata": {},
   "source": [
    "### Degrees of freedom as parameter counting!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9822b55a",
   "metadata": {},
   "source": [
    "### A postscript"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7312a906",
   "metadata": {},
   "source": [
    "(contrasts)=\n",
    "## Different ways to specify contrasts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73844ec3",
   "metadata": {},
   "source": [
    "### Treatment contrasts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a371e42",
   "metadata": {},
   "source": [
    "### Helmert contrasts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3143af1",
   "metadata": {},
   "source": [
    "### Sum to zero contrasts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "099029dc",
   "metadata": {},
   "source": [
    "### Setting contrasts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "601681f7",
   "metadata": {},
   "source": [
    "(posthoc2)=\n",
    "## Post hoc tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "750fbb0a",
   "metadata": {},
   "source": [
    "(unbalancedanova)=\n",
    "## Unbalanced designs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b40d4f6",
   "metadata": {},
   "source": [
    "### The coffee data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f764979f",
   "metadata": {},
   "source": [
    "### \"Standard ANOVA\" does not exist for unbalanced designs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79bdb60a",
   "metadata": {},
   "source": [
    "### Type I sum of squares"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5c57864",
   "metadata": {},
   "source": [
    "### Type III sum of squares"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab95a1b2",
   "metadata": {},
   "source": [
    "### Type II sum of squares"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e7be2d7",
   "metadata": {},
   "source": [
    "### Effect sizes (and non-additive sums of squares)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a14fdc6d",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02399b77",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}