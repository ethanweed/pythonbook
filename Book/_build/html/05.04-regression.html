

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>16. Linear regression &#8212; Learning Statistics with Python</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '05.04-regression';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="17. Factorial ANOVA" href="05.05-anova2.html" />
    <link rel="prev" title="15. Comparing several means (one-way ANOVA)" href="05.03-anova.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="landingpage.html">
  
  
  
  
  
    <p class="title logo__title">Learning Statistics with Python</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="landingpage.html">
                    Learning Statistics with Python
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Part I. Background</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01.01-intro.html">1. Why do we learn statistics?</a></li>
<li class="toctree-l1"><a class="reference internal" href="01.02-studydesign.html">2. A brief introduction to research design</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part II. An Introduction to Python</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="02.01-getting_started_with_python.html">3. Getting Started with Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="02.02-more_python_concepts.html">4. More Python Concepts</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part III. Working With Data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="03.01-descriptives.html">5. Descriptive statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="03.02-drawing_graphs.html">6. Drawing Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="03.03-pragmatic_matters.html">7. Data Wrangling</a></li>
<li class="toctree-l1"><a class="reference internal" href="03.04-basic_programming.html">8. Basic Programming</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part IV. Statistical Theory</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04.01-intro-to-probability.html">9. Statistical theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="04.02-probability.html">10. Introduction to Probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="04.03-estimation.html">11. Estimating unknown quantities from a sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="04.04-hypothesis-testing.html">12. Hypothesis Testing</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part V. Statistical Tools</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="05.01-chisquare.html">13. Categorical data analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="05.02-ttest.html">14. Comparing Two Means</a></li>
<li class="toctree-l1"><a class="reference internal" href="05.03-anova.html">15. Comparing several means (one-way ANOVA)</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">16. Linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="05.05-anova2.html">17. Factorial ANOVA</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part VI. Endings, Alternatives and Prospects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="06.01-bayes.html">18. Bayesian Statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="06.02-epilogue.html">19. Epilogue</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">20. References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ethanweed/pythonbook" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ethanweed/pythonbook/issues/new?title=Issue%20on%20page%20%2F05.04-regression.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/05.04-regression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Linear regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-a-linear-regression-model">16.1. Estimating a linear regression model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-with-python">16.2. Linear Regression with Python</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warning">16.2.1. Warning!!!</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-estimated-model">16.2.2. Interpreting the estimated model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-linear-regression">16.3. Multiple linear regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-linear-regression-in-python">16.4. Multiple Linear Regression in Python</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#formula-for-the-general-case">16.4.1. Formula for the general case</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantifying-the-fit-of-the-regression-model">16.5. Quantifying the fit of the regression model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-r-2-value">16.5.1. The <span class="math notranslate nohighlight">\(R^2\)</span> value</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-relationship-between-regression-and-correlation">16.5.2. The relationship between regression and correlation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-adjusted-r-2-value">16.5.3. The adjusted <span class="math notranslate nohighlight">\(R^2\)</span> value</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hypothesis-tests-for-regression-models">16.6. Hypothesis tests for regression models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-the-model-as-a-whole">16.6.1. Testing the model as a whole</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#an-f-test-function">16.6.2. An F-test function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tests-for-individual-coefficients">16.6.3. Tests for individual coefficients</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-the-significance-of-a-correlation">16.7. Testing the significance of a correlation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hypothesis-tests-for-a-single-correlation">16.7.1. Hypothesis tests for a single correlation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hypothesis-tests-for-all-pairwise-correlations">16.7.2. Hypothesis tests for all pairwise correlations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-standardised-regression-coefficients">16.7.3. Calculating standardised regression coefficients</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumptions-of-regression">16.8. Assumptions of regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-checking">16.9. Model checking</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#three-kinds-of-residuals">16.9.1. Three kinds of residuals</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#three-kinds-of-anomalous-data">16.9.2. Three kinds of anomalous data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checking-the-normality-of-the-residuals">16.9.3. Checking the normality of the residuals</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checking-the-linearity-of-the-relationship">16.9.4. Checking the linearity of the relationship</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checking-the-homogeneity-of-variance">16.9.5. Checking the homogeneity of variance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checking-for-collinearity">16.9.6. Checking for collinearity</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-selection">16.10. Model Selection</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-elimination">16.10.1. Backward elimination</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-caveat">16.10.2. A caveat</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-two-regression-models">16.10.3. Comparing two regression models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">16.11. Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="linear-regression">
<span id="regression"></span><h1><span class="section-number">16. </span>Linear regression<a class="headerlink" href="#linear-regression" title="Permalink to this heading">#</a></h1>
<p>The goal in this chapter is to introduce <strong><em>linear regression</em></strong>. Stripped to its bare essentials, linear regression models are basically a slightly fancier version of the <span class="xref myst">Pearson correlation</span>, though as we’ll see, regression models are much more powerful tools.</p>
<p>Since the basic ideas in regression are closely tied to correlation, we’ll return to the <code class="docutils literal notranslate"><span class="pre">parenthood.csv</span></code> file that we were using to illustrate how correlations work. Recall that, in this data set, we were trying to find out why Dan is so very grumpy all the time, and our working hypothesis was that I’m not getting enough sleep.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">file</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/ethanweed/pythonbook/main/Data/parenthood.csv&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>

<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>dan_sleep</th>
      <th>baby_sleep</th>
      <th>dan_grump</th>
      <th>day</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>7.59</td>
      <td>10.18</td>
      <td>56</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>7.91</td>
      <td>11.66</td>
      <td>60</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5.14</td>
      <td>7.92</td>
      <td>82</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>7.71</td>
      <td>9.61</td>
      <td>55</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>6.68</td>
      <td>9.75</td>
      <td>67</td>
      <td>5</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We drew some scatterplots to help us examine the relationship between the amount of sleep I get, and my grumpiness the following day.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">myst_nb</span> <span class="kn">import</span> <span class="n">glue</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="p">,</span>
                <span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> 
                <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;dan_grump&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Grumpiness and sleep&#39;</span><span class="p">,</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="s1">&#39;My grumpiness (0-100)&#39;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;My sleep (hours)&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;sleepycorrelation-fig&quot;</span><span class="p">,</span> <span class="n">fig</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<figure class="align-default" id="fig-sleepycorrelation" style="width: 600px">
<img alt="_images/41efc9111636ea80459dcf52f59575ef2f42e1107dfb8932b2cc2d95a92d6077.png" src="_images/41efc9111636ea80459dcf52f59575ef2f42e1107dfb8932b2cc2d95a92d6077.png" />
<figcaption>
<p><span class="caption-number">Fig. 16.1 </span><span class="caption-text">Scatterplot showing grumpiness as a function of hours slept.</span><a class="headerlink" href="#fig-sleepycorrelation" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The actual scatterplot that we draw is the one shown in <a class="reference internal" href="#fig-sleepycorrelation"><span class="std std-numref">Fig. 16.1</span></a>, and as we saw previously this corresponds to a correlation of <span class="math notranslate nohighlight">\(r=-.90\)</span>, but what we find ourselves secretly imagining is something that looks closer to the left panel in <a class="reference internal" href="#fig-sleep-regressions-1"><span class="std std-numref">Fig. 16.2</span></a>. That is, we mentally draw a straight line through the middle of the data. In statistics, this line that we’re drawing is called a <strong><em>regression line</em></strong>. Notice that – since we’re not idiots – the regression line goes through the middle of the data. We don’t find ourselves imagining anything like the rather silly plot shown in the right panel in <a class="reference internal" href="#fig-sleep-regressions-1"><span class="std std-numref">Fig. 16.2</span></a>.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> 
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>

<span class="c1"># find the regression coefficients to allow manually plotting the line</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s2">&quot;dan_grump ~ dan_sleep&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">intercept</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">Intercept</span>
<span class="n">slope</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dan_sleep</span>


<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>


<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;dan_grump&#39;</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;The best-fitting regression line&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;My sleep (hours)&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;My grumpiness (0-10)&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">slope</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="n">intercept</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;dan_grump&#39;</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Not the best-fitting regression line!&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;My sleep (hours)&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;My grumpiness (0-10)&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="mi">80</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;sleep_regressions_1-fig&quot;</span><span class="p">,</span> <span class="n">fig</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<figure class="align-default" id="fig-sleep-regressions-1" style="width: 600px">
<img alt="_images/14a008cf05a8dae3e8de56966057531506648fd04954a01a767d4f0f2a330dc6.png" src="_images/14a008cf05a8dae3e8de56966057531506648fd04954a01a767d4f0f2a330dc6.png" />
<figcaption>
<p><span class="caption-number">Fig. 16.2 </span><span class="caption-text">The panel to the left shows the sleep-grumpiness scatterplot from <a class="reference internal" href="#fig-sleepycorrelation"><span class="std std-numref">Fig. 16.1</span></a> with the best fitting regression line drawn over the top. Not surprisingly, the line goes through the middle of the data. In contrast, the panel to the right shows the same data, but with a very poor choice of regression line drawn over the top.</span><a class="headerlink" href="#fig-sleep-regressions-1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>This is not highly surprising: the line that I’ve drawn in panel to the right doesn’t “fit” the data very well, so it doesn’t make a lot of sense to propose it as a way of summarising the data, right? This is a very simple observation to make, but it turns out to be very powerful when we start trying to wrap just a little bit of maths around it. To do so, let’s start with a refresher of some high school maths. The formula for a straight line is usually written like this:</p>
<div class="math notranslate nohighlight">
\[
y = mx + c
\]</div>
<p>Or, at least, that’s what it was when I went to high school all those years ago.<a class="footnote-reference brackets" href="#americanhighschool" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> The two <em>variables</em> are <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, and we have two <em>coefficients</em>, <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(c\)</span>. The coefficient <span class="math notranslate nohighlight">\(m\)</span> represents the <em>slope</em> of the line, and the coefficient <span class="math notranslate nohighlight">\(c\)</span> represents the <em><span class="math notranslate nohighlight">\(y\)</span>-intercept</em> of the line. Digging further back into our decaying memories of high school (sorry, for some of us high school was a long time ago), we remember that the intercept is interpreted as “the value of <span class="math notranslate nohighlight">\(y\)</span> that you get when <span class="math notranslate nohighlight">\(x=0\)</span>”. Similarly, a slope of <span class="math notranslate nohighlight">\(m\)</span> means that if you increase the <span class="math notranslate nohighlight">\(x\)</span>-value by 1 unit, then the <span class="math notranslate nohighlight">\(y\)</span>-value goes up by <span class="math notranslate nohighlight">\(m\)</span> units; a negative slope means that the <span class="math notranslate nohighlight">\(y\)</span>-value would go down rather than up. Ah yes, it’s all coming back to me now.</p>
<p>Now that we’ve remembered that, it should come as no surprise to discover that we use the exact same formula to describe a regression line. If <span class="math notranslate nohighlight">\(Y\)</span> is the outcome variable (the DV) and <span class="math notranslate nohighlight">\(X\)</span> is the predictor variable (the IV), then the formula that describes our regression is written like this:</p>
<div class="math notranslate nohighlight">
\[
\hat{Y_i} = b_1 X_i + b_0
\]</div>
<p>Hm. Looks like the same formula, but there’s some extra frilly bits in this version. Let’s make sure we understand them. Firstly, notice that I’ve written <span class="math notranslate nohighlight">\(X_i\)</span> and <span class="math notranslate nohighlight">\(Y_i\)</span> rather than just plain old <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>. This is because we want to remember that we’re dealing with actual data. In this equation, <span class="math notranslate nohighlight">\(X_i\)</span> is the value of predictor variable for the <span class="math notranslate nohighlight">\(i\)</span>th observation (i.e., the number of hours of sleep that I got on day <span class="math notranslate nohighlight">\(i\)</span> of my little study), and <span class="math notranslate nohighlight">\(Y_i\)</span> is the corresponding value of the outcome variable (i.e., my grumpiness on that day). And although I haven’t said so explicitly in the equation, what we’re assuming is that this formula works for all observations in the data set (i.e., for all <span class="math notranslate nohighlight">\(i\)</span>). Secondly, notice that I wrote <span class="math notranslate nohighlight">\(\hat{Y}_i\)</span> and not <span class="math notranslate nohighlight">\(Y_i\)</span>. This is because we want to make the distinction between the <em>actual data</em> <span class="math notranslate nohighlight">\(Y_i\)</span>, and the <em>estimate</em> <span class="math notranslate nohighlight">\(\hat{Y}_i\)</span> (i.e., the prediction that our regression line is making). Thirdly, I changed the letters used to describe the coefficients from <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(c\)</span> to <span class="math notranslate nohighlight">\(b_1\)</span> and <span class="math notranslate nohighlight">\(b_0\)</span>. That’s just the way that statisticians like to refer to the coefficients in a regression model. I’ve no idea why they chose <span class="math notranslate nohighlight">\(b\)</span>, but that’s what they did. In any case <span class="math notranslate nohighlight">\(b_0\)</span> always refers to the intercept term, and <span class="math notranslate nohighlight">\(b_1\)</span> refers to the slope.</p>
<p>Excellent, excellent. Next, I can’t help but notice that – regardless of whether we’re talking about the good regression line or the bad one – the data don’t fall perfectly on the line. Or, to say it another way, the data <span class="math notranslate nohighlight">\(Y_i\)</span> are not identical to the predictions of the regression model <span class="math notranslate nohighlight">\(\hat{Y_i}\)</span>. Since statisticians love to attach letters, names and numbers to everything, let’s refer to the difference between the model prediction and that actual data point as a <em>residual</em>, and we’ll refer to it as <span class="math notranslate nohighlight">\(\epsilon_i\)</span>.<a class="footnote-reference brackets" href="#noteepsilon" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a> Written using mathematics, the residuals are defined as:</p>
<div class="math notranslate nohighlight">
\[
\epsilon_i = Y_i - \hat{Y}_i
\]</div>
<p>which in turn means that we can write down the complete linear regression model as:</p>
<div class="math notranslate nohighlight">
\[
Y_i = b_1 X_i + b_0 + \epsilon_i
\]</div>
<section id="estimating-a-linear-regression-model">
<span id="regressionestimation"></span><h2><span class="section-number">16.1. </span>Estimating a linear regression model<a class="headerlink" href="#estimating-a-linear-regression-model" title="Permalink to this heading">#</a></h2>
<p>Okay, now let’s redraw our pictures, but this time I’ll add some lines to show the size of the residual for all observations. When the regression line is good, our residuals (the lengths of the solid black lines) all look pretty small, as shown in the left panel of <a class="reference internal" href="#fig-sleep-regressions-2"><span class="std std-numref">Fig. 16.3</span></a>, but when the regression line is a bad one, the residuals are a lot larger, as you can see from looking at the right panel of <a class="reference internal" href="#fig-sleep-regressions-2"><span class="std std-numref">Fig. 16.3</span></a>. Hm. Maybe what we “want” in a regression model is <em>small</em> residuals. Yes, that does seem to make sense. In fact, I think I’ll go so far as to say that the “best fitting” regression line is the one that has the smallest residuals. Or, better yet, since statisticians seem to like to take squares of everything why not say that …</p>
<blockquote>
<div><p>The estimated regression coefficients, <span class="math notranslate nohighlight">\(\hat{b}_0\)</span> and <span class="math notranslate nohighlight">\(\hat{b}_1\)</span> are those that minimise the sum of the squared residuals, which we could either write as <span class="math notranslate nohighlight">\(\sum_i (Y_i - \hat{Y}_i)^2\)</span> or as <span class="math notranslate nohighlight">\(\sum_i {\epsilon_i}^2\)</span>.</p>
</div></blockquote>
<p>Yes, yes that sounds even better. And since I’ve indented it like that, it probably means that this is the right answer. And since this is the right answer, it’s probably worth making a note of the fact that our regression coefficients are <em>estimates</em> (we’re trying to guess the parameters that describe a population!), which is why I’ve added the little hats, so that we get <span class="math notranslate nohighlight">\(\hat{b}_0\)</span> and <span class="math notranslate nohighlight">\(\hat{b}_1\)</span> rather than <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span>. Finally, I should also note that – since there’s actually more than one way to estimate a regression model – the more technical name for this estimation process is <strong><em>ordinary least squares (OLS) regression</em></strong>.</p>
<p>At this point, we now have a concrete definition for what counts as our “best” choice of regression coefficients, <span class="math notranslate nohighlight">\(\hat{b}_0\)</span> and <span class="math notranslate nohighlight">\(\hat{b}_1\)</span>. The natural question to ask next is,  if our optimal regression coefficients are those that minimise the sum squared residuals, how do we <em>find</em> these wonderful numbers? The actual answer to this question is complicated, and it doesn’t help you understand the logic of regression.<a class="footnote-reference brackets" href="#notekungfu" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a> As a result, this time I’m going to let you off the hook. Instead of showing you how to do it the long and tedious way first, and then “revealing” the wonderful shortcut that Python provides you with, let’s cut straight to the chase… and use Python to do all the heavy lifting.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span><span class="o">,</span> <span class="nn">scipy</span><span class="o">,</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">curve_fit</span>

<span class="n">xData</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">]</span>
<span class="n">yData</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">])</span>

<span class="c1"># (the solution to this figure stolen shamelessly from this stack-overflow answer by James Phillips:</span>
<span class="c1"># https://stackoverflow.com/questions/53779773/python-linear-regression-best-fit-line-with-residuals)</span>

<span class="c1"># fit linear regression model and save parameters</span>
<span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span>

<span class="n">initialParameters</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>

<span class="n">fittedParameters</span><span class="p">,</span> <span class="n">pcov</span> <span class="o">=</span> <span class="n">curve_fit</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">xData</span><span class="p">,</span> <span class="n">yData</span><span class="p">,</span> <span class="n">initialParameters</span><span class="p">)</span>

<span class="n">modelPredictions</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">xData</span><span class="p">,</span> <span class="o">*</span><span class="n">fittedParameters</span><span class="p">)</span> 

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">xData</span><span class="p">,</span>
                     <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">yData</span><span class="p">})</span>

<span class="c1"># plot data points</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;The best-fitting regression line!&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;My sleep (hours)&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;My grumpiness (0-10)&quot;</span><span class="p">)</span>

<span class="c1"># add regression line</span>
<span class="n">xModel</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">xData</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">xData</span><span class="p">))</span>
<span class="n">yModel</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">xModel</span><span class="p">,</span> <span class="o">*</span><span class="n">fittedParameters</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xModel</span><span class="p">,</span> <span class="n">yModel</span><span class="p">)</span>

<span class="c1"># add drop lines</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">xData</span><span class="p">)):</span>
    <span class="n">lineXdata</span> <span class="o">=</span> <span class="p">(</span><span class="n">xData</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">xData</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="c1"># same X</span>
    <span class="n">lineYdata</span> <span class="o">=</span> <span class="p">(</span><span class="n">yData</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">modelPredictions</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="c1"># different Y</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lineXdata</span><span class="p">,</span> <span class="n">lineYdata</span><span class="p">)</span>

    
<span class="c1">#####</span>

<span class="c1"># create poor-fitting model</span>
<span class="n">badParameters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">80</span><span class="p">])</span>
<span class="n">badPredictions</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">xData</span><span class="p">,</span> <span class="o">*</span><span class="n">badParameters</span><span class="p">)</span> 

<span class="n">bad_xModel</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">xData</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">xData</span><span class="p">))</span>
<span class="n">bad_yModel</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">bad_xModel</span><span class="p">,</span> <span class="o">*</span><span class="n">badParameters</span><span class="p">)</span>

<span class="c1"># plot data with poor-fitting model</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Not the best-fitting regression line!&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;My sleep (hours)&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;My grumpiness (0-10)&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">bad_xModel</span><span class="p">,</span> <span class="n">bad_yModel</span><span class="p">)</span>  

<span class="c1"># add drop lines</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">xData</span><span class="p">)):</span>
    <span class="n">lineXdata</span> <span class="o">=</span> <span class="p">(</span><span class="n">xData</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">xData</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> 
    <span class="n">lineYdata</span> <span class="o">=</span> <span class="p">(</span><span class="n">yData</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">badPredictions</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> 
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lineXdata</span><span class="p">,</span> <span class="n">lineYdata</span><span class="p">)</span>
  
    
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;sleep_regressions_2-fig&quot;</span><span class="p">,</span> <span class="n">fig</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<figure class="align-default" id="fig-sleep-regressions-2" style="width: 600px">
<img alt="_images/effb56837a30c0f9c20b16b552dd4b458ed1b097d4eaabad3cea10191a9236b4.png" src="_images/effb56837a30c0f9c20b16b552dd4b458ed1b097d4eaabad3cea10191a9236b4.png" />
<figcaption>
<p><span class="caption-number">Fig. 16.3 </span><span class="caption-text">A depiction of the residuals associated with the best fitting regression line (left panel), and the residuals associated with a poor regression line (right panel). The residuals are much smaller for the good regression line. Again, this is no surprise given that the good line is the one that goes right through the middle of the data.</span><a class="headerlink" href="#fig-sleep-regressions-2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="linear-regression-with-python">
<span id="pingouinregression"></span><h2><span class="section-number">16.2. </span>Linear Regression with Python<a class="headerlink" href="#linear-regression-with-python" title="Permalink to this heading">#</a></h2>
<p>As always, there are several different ways we could go about calculating a linear regression in Python, but we’ll stick with <code class="docutils literal notranslate"><span class="pre">pingouin</span></code><a class="footnote-reference brackets" href="#drink" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>, which for my money is one of the simplest and easiest packages to use. The <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> command for linear regression is, well, <code class="docutils literal notranslate"><span class="pre">linear_regression</span></code>, so that couldn’t be much more straightforward. After that, we just need to tell <code class="docutils literal notranslate"><span class="pre">pinguoin</span></code> which variable we want to use as a predictor variable (independent variable), and which one we want to use as the outcome variable (dependent variable). <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> wants the predictor variable first, so, since we want to model my grumpiness as a function of my sleep, we write:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pingouin</span> <span class="k">as</span> <span class="nn">pg</span>

<span class="n">mod1</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Display results, rounded to two decimal places.</span>
<span class="n">mod1</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>coef</th>
      <th>se</th>
      <th>T</th>
      <th>pval</th>
      <th>r2</th>
      <th>adj_r2</th>
      <th>CI[2.5%]</th>
      <th>CI[97.5%]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Intercept</td>
      <td>125.96</td>
      <td>3.02</td>
      <td>41.76</td>
      <td>0.0</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>119.97</td>
      <td>131.94</td>
    </tr>
    <tr>
      <th>1</th>
      <td>dan_sleep</td>
      <td>-8.94</td>
      <td>0.43</td>
      <td>-20.85</td>
      <td>0.0</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>-9.79</td>
      <td>-8.09</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>As is its way, <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> gives us a nice simple table, with a lot of information. Most importantly for now, we can see that <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> has caclulated the intercept <span class="math notranslate nohighlight">\(\hat{b}_0 = 125.96\)</span> and the slope <span class="math notranslate nohighlight">\(\hat{b}_1 = -8.94\)</span>. In other words, the best-fitting regression line that I plotted in <a class="reference internal" href="#fig-sleep-regressions-1"><span class="std std-numref">Fig. 16.2</span></a> has this formula:</p>
<div class="math notranslate nohighlight">
\[
\hat{Y}_i = -8.94 \ X_i + 125.96
\]</div>
<section id="warning">
<h3><span class="section-number">16.2.1. </span>Warning!!!<a class="headerlink" href="#warning" title="Permalink to this heading">#</a></h3>
<p>Remember, it’s critical that you put the variables in the right order. If you reverse the predictor and outcome variables, <code class="docutils literal notranslate"><span class="pre">pinguoin</span></code> will happily calculate a result for you, but it will not be the one you are looking for. If instead, we had written <code class="docutils literal notranslate"><span class="pre">pg.linear_regression(df['dan_grump'],</span> <span class="pre">df['dan_sleep'])</span></code>, we would get the following:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">modx</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">])</span>
<span class="n">modx</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>coef</th>
      <th>se</th>
      <th>T</th>
      <th>pval</th>
      <th>r2</th>
      <th>adj_r2</th>
      <th>CI[2.5%]</th>
      <th>CI[97.5%]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Intercept</td>
      <td>12.78</td>
      <td>0.28</td>
      <td>45.27</td>
      <td>0.0</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>12.22</td>
      <td>13.34</td>
    </tr>
    <tr>
      <th>1</th>
      <td>dan_grump</td>
      <td>-0.09</td>
      <td>0.00</td>
      <td>-20.85</td>
      <td>0.0</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>-0.10</td>
      <td>-0.08</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The output looks valid enough on the face of it, and it is even statistically significant. But in this model, we just predicted my son’s sleepiness as a function of my grumpiness, which is madness! Reversing the direction of causality would make a great scifi movie<a class="footnote-reference brackets" href="#notenolan" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a>, but it’s no good in statistics. So remember, predictor first, outcome second<a class="footnote-reference brackets" href="#noteformula" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a></p>
</section>
<section id="interpreting-the-estimated-model">
<h3><span class="section-number">16.2.2. </span>Interpreting the estimated model<a class="headerlink" href="#interpreting-the-estimated-model" title="Permalink to this heading">#</a></h3>
<p>The most important thing to be able to understand is how to interpret these coefficients. Let’s start with <span class="math notranslate nohighlight">\(\hat{b}_1\)</span>, the slope. If we remember the definition of the slope, a regression coefficient of <span class="math notranslate nohighlight">\(\hat{b}_1 = -8.94\)</span> means that if I increase <span class="math notranslate nohighlight">\(X_i\)</span> by 1, then I’m decreasing <span class="math notranslate nohighlight">\(Y_i\)</span> by 8.94. That is, each additional hour of sleep that I gain will improve my mood, reducing my grumpiness by 8.94 grumpiness points. What about the intercept? Well, since <span class="math notranslate nohighlight">\(\hat{b}_0\)</span> corresponds to “the expected value of <span class="math notranslate nohighlight">\(Y_i\)</span> when <span class="math notranslate nohighlight">\(X_i\)</span> equals 0”, it’s pretty straightforward. It implies that if I get zero hours of sleep (<span class="math notranslate nohighlight">\(X_i =0\)</span>) then my grumpiness will go off the scale, to an insane value of (<span class="math notranslate nohighlight">\(Y_i = 125.96\)</span>). Best to be avoided, I think.</p>
</section>
</section>
<section id="multiple-linear-regression">
<span id="multipleregression"></span><h2><span class="section-number">16.3. </span>Multiple linear regression<a class="headerlink" href="#multiple-linear-regression" title="Permalink to this heading">#</a></h2>
<p>The simple linear regression model that we’ve discussed up to this point assumes that there’s a single predictor variable that you’re interested in, in this case <code class="docutils literal notranslate"><span class="pre">dan_sleep</span></code>. In fact, up to this point, <em>every</em> statistical tool that we’ve talked about has assumed that your analysis uses one predictor variable and one outcome variable. However, in many (perhaps most) research projects you actually have multiple predictors that you want to examine. If so, it would be nice to be able to extend the linear regression framework to be able to include multiple predictors. Perhaps some kind of <strong><em>multiple regression</em></strong> model would be in order?</p>
<p>Multiple regression is conceptually very simple. All we do is add more terms to our regression equation. Let’s suppose that we’ve got two variables that we’re interested in; perhaps we want to use both <code class="docutils literal notranslate"><span class="pre">dan_sleep</span></code> and <code class="docutils literal notranslate"><span class="pre">baby_sleep</span></code> to predict the <code class="docutils literal notranslate"><span class="pre">dan_grump</span></code> variable. As before, we let <span class="math notranslate nohighlight">\(Y_i\)</span> refer to my grumpiness on the <span class="math notranslate nohighlight">\(i\)</span>-th day. But now we have two <span class="math notranslate nohighlight">\(X\)</span> variables: the first corresponding to the amount of sleep I got and the second corresponding to the amount of sleep my son got. So we’ll let <span class="math notranslate nohighlight">\(X_{i1}\)</span> refer to the hours I slept on the <span class="math notranslate nohighlight">\(i\)</span>-th day, and <span class="math notranslate nohighlight">\(X_{i2}\)</span> refers to the hours that the baby slept on that day. If so, then we can write our regression model like this:</p>
<div class="math notranslate nohighlight">
\[
Y_i = b_2 X_{i2} + b_1 X_{i1} + b_0 + \epsilon_i
\]</div>
<p>As before, <span class="math notranslate nohighlight">\(\epsilon_i\)</span> is the residual associated with the <span class="math notranslate nohighlight">\(i\)</span>-th observation, <span class="math notranslate nohighlight">\(\epsilon_i = {Y}_i - \hat{Y}_i\)</span>. In this model, we now have three coefficients that need to be estimated: <span class="math notranslate nohighlight">\(b_0\)</span> is the intercept, <span class="math notranslate nohighlight">\(b_1\)</span> is the coefficient associated with my sleep, and <span class="math notranslate nohighlight">\(b_2\)</span> is the coefficient associated with my son’s sleep. However, although the number of coefficients that need to be estimated has changed, the basic idea of how the estimation works is unchanged: our estimated coefficients <span class="math notranslate nohighlight">\(\hat{b}_0\)</span>, <span class="math notranslate nohighlight">\(\hat{b}_1\)</span> and <span class="math notranslate nohighlight">\(\hat{b}_2\)</span> are those that minimise the sum squared residuals.</p>
</section>
<section id="multiple-linear-regression-in-python">
<span id="pingouinmultiplelinearregression"></span><h2><span class="section-number">16.4. </span>Multiple Linear Regression in Python<a class="headerlink" href="#multiple-linear-regression-in-python" title="Permalink to this heading">#</a></h2>
<p>Doing mulitiple linear regression in <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> is just as easy as adding some more predictor variables, like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mod2</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> <span class="s1">&#39;baby_sleep&#39;</span><span class="p">]],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Still, there is one thing to watch out for. If you look carefully at the command above, you will notice that not only have we added a new predictor (<code class="docutils literal notranslate"><span class="pre">baby_sleep</span></code>), we have also added some extra brackets. While before our predictor variable was <code class="docutils literal notranslate"><span class="pre">['dan_sleep']</span></code>, now we have <code class="docutils literal notranslate"><span class="pre">[['dan_sleep',</span> <span class="pre">'baby_sleep']]</span></code>. Why the extra set of <code class="docutils literal notranslate"><span class="pre">[]</span></code>?</p>
<p>This is because we are using the brackets in two different ways. When we wrote <code class="docutils literal notranslate"><span class="pre">['dan_sleep']</span></code>, the square brackets meant “select the column in the <code class="docutils literal notranslate"><span class="pre">pandas</span></code> dataframe with the header ‘dan_sleep’”. But now we are giving <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> a <em>list</em> of columns to select, and <code class="docutils literal notranslate"><span class="pre">list</span></code> objects are <em>also</em> defined by square brackets in Python. To keep things clear, another way to achieve the same result would be to define the list of predictor variables outside the call to <code class="docutils literal notranslate"><span class="pre">pingouin</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> <span class="s1">&#39;baby_sleep&#39;</span><span class="p">]</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="s1">&#39;dan_grump&#39;</span>

<span class="n">mod2</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">predictors</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="n">outcome</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>You could even do all the work outside of <code class="docutils literal notranslate"><span class="pre">pinguoin</span></code>, like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictors</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> <span class="s1">&#39;baby_sleep&#39;</span><span class="p">]]</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">]</span>

<span class="n">mod2</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">predictors</span><span class="p">,</span> <span class="n">outcome</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>All three of these will give the same result, so it’s up to you choose what makes most sense to you. But now it’s time to take a look at the results:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mod2</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>coef</th>
      <th>se</th>
      <th>T</th>
      <th>pval</th>
      <th>r2</th>
      <th>adj_r2</th>
      <th>CI[2.5%]</th>
      <th>CI[97.5%]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Intercept</td>
      <td>125.97</td>
      <td>3.04</td>
      <td>41.42</td>
      <td>0.00</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>119.93</td>
      <td>132.00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>dan_sleep</td>
      <td>-8.95</td>
      <td>0.55</td>
      <td>-16.17</td>
      <td>0.00</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>-10.05</td>
      <td>-7.85</td>
    </tr>
    <tr>
      <th>2</th>
      <td>baby_sleep</td>
      <td>0.01</td>
      <td>0.27</td>
      <td>0.04</td>
      <td>0.97</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>-0.53</td>
      <td>0.55</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The coefficient associated with dan_sleep is quite large, suggesting that every hour of sleep I lose makes me a lot grumpier. However, the coefficient for baby_sleep is very small, suggesting that it doesn’t really matter how much sleep my son gets, not really. What matters as far as my grumpiness goes is how much sleep <em>I</em> get. Although conceptually similar, multiple linear regressions are much harder to visualize than a simple linear regression with only one predictor. To get a sense of what this multiple regression model with two predictors looks like, <a class="reference internal" href="#fig-sleep-regressions-3d"><span class="std std-numref">Fig. 16.4</span></a> shows a 3D plot that plots all three variables, along with the regression model itself.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>

<span class="c1"># style the plot</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;whitegrid&quot;</span><span class="p">)</span>

<span class="c1"># construct 3d plot space</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span> 
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span> <span class="o">=</span> <span class="s1">&#39;3d&#39;</span><span class="p">)</span>

<span class="c1"># define axes</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;baby_sleep&#39;</span><span class="p">]</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">]</span>

<span class="c1"># set axis labels</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;dan_sleep&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;baby_sleep&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s2">&quot;dan_grump&quot;</span><span class="p">)</span>


<span class="c1"># get intercept and regression coefficients from the lmm model</span>
<span class="n">coefs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">mod2</span><span class="p">[</span><span class="s1">&#39;coef&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">:])</span>
<span class="n">intercept</span> <span class="o">=</span> <span class="n">mod2</span><span class="p">[</span><span class="s1">&#39;coef&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># create a 3d plane representation of the lmm predictions</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">12</span><span class="p">),</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">12</span><span class="p">),</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
<span class="n">zs</span> <span class="o">=</span> <span class="n">xs</span><span class="o">*</span><span class="n">coefs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">ys</span><span class="o">*</span><span class="n">coefs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">intercept</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">ys</span><span class="p">,</span><span class="n">zs</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># plot the data and plane</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">ys</span><span class="p">,</span><span class="n">zs</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span><span class="p">)</span>

<span class="c1"># adjust the viewing angle</span>
<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span><span class="mi">97</span><span class="p">)</span>



<span class="c1"># plot the figure in the book with the caption (and no duplicate figure)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;sleep_regressions_3d-fig&quot;</span><span class="p">,</span> <span class="n">fig</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<figure class="align-default" id="fig-sleep-regressions-3d" style="width: 600px">
<img alt="_images/9e768152ad47cc239fb15addfbd00b5e845304ffa65a4b27f62382decd02f69a.png" src="_images/9e768152ad47cc239fb15addfbd00b5e845304ffa65a4b27f62382decd02f69a.png" />
<figcaption>
<p><span class="caption-number">Fig. 16.4 </span><span class="caption-text">A 3D visualisation of a multiple regression model. There are two predictors in the model, <code class="docutils literal notranslate"><span class="pre">dan_sleep</span></code> and <code class="docutils literal notranslate"><span class="pre">baby_sleep</span></code>; the outcome variable is <code class="docutils literal notranslate"><span class="pre">dan.grump</span></code>. Together, these three variables form a 3D space: each observation (blue dots) is a point in this space. In much the same way that a simple linear regression model forms a line in 2D space, this multiple regression model forms a plane in 3D space. When we estimate the regression coefficients, what we’re trying to do is find a plane that is as close to all the blue dots as possible.</span><a class="headerlink" href="#fig-sleep-regressions-3d" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<section id="formula-for-the-general-case">
<h3><span class="section-number">16.4.1. </span>Formula for the general case<a class="headerlink" href="#formula-for-the-general-case" title="Permalink to this heading">#</a></h3>
<p>The equation that I gave above shows you what a multiple regression model looks like when you include two predictors. Not surprisingly, then, if you want more than two predictors, all you have to do is add more <span class="math notranslate nohighlight">\(X\)</span> terms and more <span class="math notranslate nohighlight">\(b\)</span> coefficients. In other words, if you have <span class="math notranslate nohighlight">\(K\)</span> predictor variables in the model then the regression equation looks like this:</p>
<div class="math notranslate nohighlight">
\[
Y_i = \left( \sum_{k=1}^K b_{k} X_{ik} \right) + b_0 + \epsilon_i
\]</div>
</section>
</section>
<section id="quantifying-the-fit-of-the-regression-model">
<span id="r2"></span><h2><span class="section-number">16.5. </span>Quantifying the fit of the regression model<a class="headerlink" href="#quantifying-the-fit-of-the-regression-model" title="Permalink to this heading">#</a></h2>
<p>So we now know how to estimate the coefficients of a linear regression model. The problem is, we don’t yet know if this regression model is any good. For example, our multiple linear regression model <code class="docutils literal notranslate"><span class="pre">mod2</span></code> <em>claims</em> that every hour of sleep will improve my mood by quite a lot, but it might just be rubbish. Remember, the regression model only produces a prediction <span class="math notranslate nohighlight">\(\hat{Y}_i\)</span> about what my mood is like: my actual mood is <span class="math notranslate nohighlight">\(Y_i\)</span>. If these two are very close, then the regression model has done a good job. If they are very different, then it has done a bad job.</p>
<section id="the-r-2-value">
<h3><span class="section-number">16.5.1. </span>The <span class="math notranslate nohighlight">\(R^2\)</span> value<a class="headerlink" href="#the-r-2-value" title="Permalink to this heading">#</a></h3>
<p>Once again, let’s wrap a little bit of mathematics around this. First, we’ve got the sum of the squared residuals:</p>
<div class="math notranslate nohighlight">
\[
\text{SS}_{res} = \sum_i (Y_i - \hat{Y}_i)^2
\]</div>
<p>which we would hope to be pretty small. Specifically, what we’d like is for it to be very small in comparison to the total variability in the outcome variable,</p>
<div class="math notranslate nohighlight">
\[
\text{SS}_{tot} = \sum_i (Y_i - \bar{Y})^2
\]</div>
<p>While we’re here, let’s calculate these values in Python. Just to make my Python commands look a bit more similar to the mathematical equations, I’ll create variables <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">Y</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">]</span> <span class="c1"># the predictor</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">]</span> <span class="c1"># the outcome</span>
</pre></div>
</div>
</div>
</div>
<p>First, lets just examine the output for the simple model that uses only a single predictor:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mod1</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">mod1</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>coef</th>
      <th>se</th>
      <th>T</th>
      <th>pval</th>
      <th>r2</th>
      <th>adj_r2</th>
      <th>CI[2.5%]</th>
      <th>CI[97.5%]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Intercept</td>
      <td>125.96</td>
      <td>3.02</td>
      <td>41.76</td>
      <td>0.0</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>119.97</td>
      <td>131.94</td>
    </tr>
    <tr>
      <th>1</th>
      <td>dan_sleep</td>
      <td>-8.94</td>
      <td>0.43</td>
      <td>-20.85</td>
      <td>0.0</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>-9.79</td>
      <td>-8.09</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>In this output, we can see that Python has calculated an intercept of 125.96 and a regression coefficient (<span class="math notranslate nohighlight">\(beta\)</span>) of -8.94. So for every hour of sleep I get, the model estimates that this will correspond to a decrease in grumpiness of about 9 on my incredibly scientific grumpiness scale. We can use this information to calculate <span class="math notranslate nohighlight">\(\hat{Y}\)</span>, that is, the values that the model <em>predicts</em> for the outcome measure, as opposed to <span class="math notranslate nohighlight">\(Y\)</span>, which are the actual data we observed. So, for each value of the predictor variable X, we multiply that value by the regression coefficient -8.84, and add the intercept 125.97:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Y_pred</span> <span class="o">=</span> <span class="o">-</span><span class="mf">8.94</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="mf">125.97</span>
</pre></div>
</div>
</div>
</div>
<p>Okay, now that we’ve got a variable which stores the regression model predictions for how grumpy I will be on any given day, let’s calculate our sum of squared residuals. We would do that using the following command:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">SS_resid</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">Y_pred</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="p">)</span>
<span class="n">SS_resid</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1838.7224883200004
</pre></div>
</div>
</div>
</div>
<p>Wonderful. A big number that doesn’t mean very much. Still, let’s forge boldly onwards anyway, and calculate the total sum of squares as well. That’s also pretty simple:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">SS_tot</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span> <span class="p">)</span>
<span class="n">SS_tot</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>9998.590000000002
</pre></div>
</div>
</div>
</div>
<p>Hm. Well, it’s a much bigger number than the last one, so this does suggest that our regression model was making good predictions. But it’s not very interpretable.</p>
<p>Perhaps we can fix this. What we’d like to do is to convert these two fairly meaningless numbers into one number. A nice, interpretable number, which for no particular reason we’ll call <span class="math notranslate nohighlight">\(R^2\)</span>. What we would like is for the value of <span class="math notranslate nohighlight">\(R^2\)</span> to be equal to 1 if the regression model makes no errors in predicting the data. In other words, if it turns out that the residual errors are zero, that is, if <span class="math notranslate nohighlight">\(\text{SS}_{res} = 0\)</span>, then we expect <span class="math notranslate nohighlight">\(R^2 = 1\)</span>. Similarly, if the model is completely useless, we would like <span class="math notranslate nohighlight">\(R^2\)</span> to be equal to 0. What do I mean by “useless”? Tempting as it is demand that the regression model move out of the house, cut its hair and get a real job, I’m probably going to have to pick a more practical definition: in this case, all I mean is that the residual sum of squares is no smaller than the total sum of squares, <span class="math notranslate nohighlight">\(\text{SS}_{res} = \text{SS}_{tot}\)</span>. Wait, why don’t we do exactly that? In fact, the formula that provides us with our <span class="math notranslate nohighlight">\(R^2\)</span> value is pretty simple to write down,</p>
<div class="math notranslate nohighlight">
\[
R^2 = 1 - \frac{\text{SS}_{res}}{\text{SS}_{tot}}
\]</div>
<p>and equally simple to calculate in Python:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">R2</span> <span class="o">=</span> <span class="mi">1</span><span class="o">-</span> <span class="p">(</span><span class="n">SS_resid</span> <span class="o">/</span> <span class="n">SS_tot</span><span class="p">)</span>
<span class="n">R2</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.816101821524835
</pre></div>
</div>
</div>
</div>
<p>The <span class="math notranslate nohighlight">\(R^2\)</span> value, sometimes called the <strong><em>coefficient of determination</em></strong><a class="footnote-reference brackets" href="#notenever" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a> has a simple interpretation: it is the <em>proportion</em> of the variance in the outcome variable that can be accounted for by the predictor. So in this case, the fact that we have obtained <span class="math notranslate nohighlight">\(R^2 = .816\)</span> means that the predictor (<code class="docutils literal notranslate"><span class="pre">my_sleep</span></code>) explains 81.6% of the variance in the outcome (<code class="docutils literal notranslate"><span class="pre">my_grump</span></code>).</p>
<p>Naturally, you don’t actually need to type in all these commands yourself if you want to obtain the <span class="math notranslate nohighlight">\(R^2\)</span> value for your regression model. And as you have probably already noticed, <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> calculates <span class="math notranslate nohighlight">\(R^2\)</span>  for us without even being asked to. But there’s another property of <span class="math notranslate nohighlight">\(R^2\)</span> that I want to point out.</p>
</section>
<section id="the-relationship-between-regression-and-correlation">
<h3><span class="section-number">16.5.2. </span>The relationship between regression and correlation<a class="headerlink" href="#the-relationship-between-regression-and-correlation" title="Permalink to this heading">#</a></h3>
<p>At this point we can revisit my earlier claim that regression, in this very simple form that I’ve discussed so far, is basically the same thing as a correlation. Previously, we used the symbol <span class="math notranslate nohighlight">\(r\)</span> to denote a Pearson correlation. Might there be some relationship between the value of the correlation coefficient <span class="math notranslate nohighlight">\(r\)</span> and the <span class="math notranslate nohighlight">\(R^2\)</span> value from linear regression? Of course there is: the squared correlation <span class="math notranslate nohighlight">\(r^2\)</span> is identical to the <span class="math notranslate nohighlight">\(R^2\)</span> value for a linear regression with only a single predictor. To illustrate this, here’s the squared correlation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">r</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">corr</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>  <span class="c1"># calculate the correlation</span>
<span class="n">r</span><span class="o">**</span><span class="mi">2</span>    <span class="c1"># print the squared correlation</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8161027191478786
</pre></div>
</div>
</div>
</div>
<p>Yep, same number. In other words, running a Pearson correlation is more or less equivalent to running a linear regression model that uses only one predictor variable.</p>
</section>
<section id="the-adjusted-r-2-value">
<h3><span class="section-number">16.5.3. </span>The adjusted <span class="math notranslate nohighlight">\(R^2\)</span> value<a class="headerlink" href="#the-adjusted-r-2-value" title="Permalink to this heading">#</a></h3>
<p>One final thing to point out before moving on. It’s quite common for people to report a slightly different measure of model performance, known as “adjusted <span class="math notranslate nohighlight">\(R^2\)</span>”. The motivation behind calculating the adjusted <span class="math notranslate nohighlight">\(R^2\)</span> value is the observation that adding more predictors into the model will <em>always</em> cause the <span class="math notranslate nohighlight">\(R^2\)</span> value to increase (or at least not decrease). The adjusted <span class="math notranslate nohighlight">\(R^2\)</span> value introduces a slight change to the calculation, as follows. For a regression model with <span class="math notranslate nohighlight">\(K\)</span> predictors, fit to a data set containing <span class="math notranslate nohighlight">\(N\)</span> observations, the adjusted <span class="math notranslate nohighlight">\(R^2\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\text{adj. } R^2 = 1 - \left(\frac{\text{SS}_{res}}{\text{SS}_{tot}} \times \frac{N-1}{N-K-1} \right)
\]</div>
<p>This adjustment is an attempt to take the degrees of freedom into account. The big advantage of the adjusted <span class="math notranslate nohighlight">\(R^2\)</span> value is that when you add more predictors to the model, the adjusted <span class="math notranslate nohighlight">\(R^2\)</span> value will only increase if the new variables improve the model performance more than you’d expect by chance. The big disadvantage is that the adjusted <span class="math notranslate nohighlight">\(R^2\)</span> value <em>can’t</em> be interpreted in the elegant way that <span class="math notranslate nohighlight">\(R^2\)</span> can. <span class="math notranslate nohighlight">\(R^2\)</span> has a simple interpretation as the proportion of variance in the outcome variable that is explained by the regression model; to my knowledge, no equivalent interpretation exists for adjusted <span class="math notranslate nohighlight">\(R^2\)</span>.</p>
<p>An obvious question then, is whether you should report <span class="math notranslate nohighlight">\(R^2\)</span> or adjusted <span class="math notranslate nohighlight">\(R^2\)</span>. This is probably a matter of personal preference. If you care more about interpretability, then <span class="math notranslate nohighlight">\(R^2\)</span> is better. If you care more about correcting for bias, then adjusted <span class="math notranslate nohighlight">\(R^2\)</span> is probably better. Speaking just for myself, I prefer <span class="math notranslate nohighlight">\(R^2\)</span>: my feeling is that it’s more important to be able to interpret your measure of model performance. Besides, as we’ll soon see in the upcoming section on hypothesis tests for regression models, which I am going to link to even though it is <a class="reference internal" href="#regressiontests"><span class="std std-ref">literally the next section in this book</span></a>, if you’re worried that the improvement in <span class="math notranslate nohighlight">\(R^2\)</span> that you get by adding a predictor is just due to chance and not because it’s a better model, well, we’ve got hypothesis tests for that.</p>
</section>
</section>
<section id="hypothesis-tests-for-regression-models">
<span id="regressiontests"></span><h2><span class="section-number">16.6. </span>Hypothesis tests for regression models<a class="headerlink" href="#hypothesis-tests-for-regression-models" title="Permalink to this heading">#</a></h2>
<p>So far we’ve talked about what a regression model is, how the coefficients of a regression model are estimated, and how we quantify the performance of the model (the last of these, incidentally, is basically our measure of effect size). The next thing we need to talk about is hypothesis tests. There are two different (but related) kinds of hypothesis tests that we need to talk about: those in which we test whether the regression model as a whole is performing significantly better than a null model; and those in which we test whether a particular regression coefficient is significantly different from zero.</p>
<p>At this point, you’re probably groaning internally, thinking that I’m going to introduce a whole new collection of tests. You’re probably sick of hypothesis tests by now, and don’t want to learn any new ones. Me too. I’m so sick of hypothesis tests that I’m going to shamelessly reuse the <span class="math notranslate nohighlight">\(F\)</span>-test from the <a class="reference internal" href="05.03-anova.html#anova"><span class="std std-ref">chapter on ANOVAs</span></a> and the <span class="math notranslate nohighlight">\(t\)</span>-test from <a class="reference internal" href="05.02-ttest.html#ttest"><span class="std std-ref">the chapter on t-tests</span></a>. In fact, all I’m going to do in this section is show you how those tests are imported wholesale into the regression framework.</p>
<section id="testing-the-model-as-a-whole">
<h3><span class="section-number">16.6.1. </span>Testing the model as a whole<a class="headerlink" href="#testing-the-model-as-a-whole" title="Permalink to this heading">#</a></h3>
<p>Okay, suppose you’ve estimated your regression model. The first hypothesis test you might want to try is one in which the null hypothesis that there is <em>no relationship</em> between the predictors and the outcome, and the alternative hypothesis is that <em>the data are distributed in exactly the way that the regression model predicts</em>. Formally, our “null model” corresponds to the fairly trivial “regression” model in which we include 0 predictors, and only include the intercept term <span class="math notranslate nohighlight">\(b_0\)</span></p>
<div class="math notranslate nohighlight">
\[
H_0: Y_i = b_0 + \epsilon_i
\]</div>
<p>If our regression model has <span class="math notranslate nohighlight">\(K\)</span> predictors, the “alternative model” is described using the usual formula for a multiple regression model:</p>
<div class="math notranslate nohighlight">
\[
H_1: Y_i = \left( \sum_{k=1}^K b_{k} X_{ik} \right) + b_0 + \epsilon_i
\]</div>
<p>How can we test these two hypotheses against each other? The trick is to understand that just like we did with ANOVA, it’s possible to divide up the total variance <span class="math notranslate nohighlight">\(\text{SS}_{tot}\)</span> into the sum of the residual variance <span class="math notranslate nohighlight">\(\text{SS}_{res}\)</span> and the regression model variance <span class="math notranslate nohighlight">\(\text{SS}_{mod}\)</span>. I’ll skip over the technicalities, since we covered most of them in the <a class="reference internal" href="05.03-anova.html#anova"><span class="std std-ref">ANOVA chapter</span></a>, and just note that:</p>
<div class="math notranslate nohighlight">
\[
\text{SS}_{mod} = \text{SS}_{tot} - \text{SS}_{res}
\]</div>
<p>And, just like we did with the ANOVA, we can convert the sums of squares into mean squares by dividing by the degrees of freedom.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{rcl}
\text{MS}_{mod} &amp;=&amp; \displaystyle\frac{\text{SS}_{mod} }{df_{mod}} \\ \\
\text{MS}_{res} &amp;=&amp; \displaystyle\frac{\text{SS}_{res} }{df_{res} }
\end{array}
\end{split}\]</div>
<p>So, how many degrees of freedom do we have? As you might expect, the <span class="math notranslate nohighlight">\(df\)</span> associated with the model is closely tied to the number of predictors that we’ve included. In fact, it turns out that <span class="math notranslate nohighlight">\(df_{mod} = K\)</span>. For the residuals, the total degrees of freedom is <span class="math notranslate nohighlight">\(df_{res} = N -K - 1\)</span>.</p>
<p>Now that we’ve got our mean square values, you’re probably going to be entirely unsurprised (possibly even bored) to discover that we can calculate an <span class="math notranslate nohighlight">\(F\)</span>-statistic like this:</p>
<div class="math notranslate nohighlight">
\[
F =  \frac{\text{MS}_{mod}}{\text{MS}_{res}}
\]</div>
<p>and the degrees of freedom associated with this are <span class="math notranslate nohighlight">\(K\)</span> and <span class="math notranslate nohighlight">\(N-K-1\)</span>. This <span class="math notranslate nohighlight">\(F\)</span> statistic has exactly the same interpretation as the one we introduced <a class="reference internal" href="05.03-anova.html#anova"><span class="std std-ref">when learning about ANOVAs</span></a>. Large <span class="math notranslate nohighlight">\(F\)</span> values indicate that the null hypothesis is performing poorly in comparison to the alternative hypothesis.</p>
<p>“Ok, this is fine”, I hear you say, “but now show me the easy way! Show me how easy it is to get an <span class="math notranslate nohighlight">\(F\)</span> statistic from <code class="docutils literal notranslate"><span class="pre">pingouin</span></code>! <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> makes everything so much easier! Surely <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> does this for me as well?”</p>
<p>Yeah. About that… actually, as of the time of writing (Tuesday the 17th of May, 2022), <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> does <em>not</em> automatically calculate the <span class="math notranslate nohighlight">\(F\)</span> statistic for the model for you. This seems like kind of a strange omission to me, since it is pretty normal to report overall <span class="math notranslate nohighlight">\(F\)</span> and <span class="math notranslate nohighlight">\(p\)</span> values for a model, and <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> seems to be all about making the normal things easy. So, I can only assume this will get added at some point, but for now, sadly, we are left to ourselves on this one.</p>
<p>I should mention that there are other statistics packages for Python that will do this for you. <a class="reference external" href="https://www.statsmodels.org/stable/regression.html">statsmodels</a> comes to mind, for instance. But this is opening a whole new can of worms that I’d rather avoid for now, so instead I provide you with code to calculate the <span class="math notranslate nohighlight">\(F\)</span> statistic and <span class="math notranslate nohighlight">\(p\)</span>-value for the model “manually” below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span> <span class="k">as</span> <span class="n">st</span>

<span class="c1"># your predictor and outcome variables (aka, &quot;the data&quot;)</span>
<span class="n">predictors</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> <span class="s1">&#39;baby_sleep&#39;</span><span class="p">]]</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">]</span>

<span class="c1"># model the data, and store the model information in a variable called &quot;mod&quot;</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">predictors</span><span class="p">,</span> <span class="n">outcome</span><span class="p">)</span>


<span class="c1"># call the outcome data &quot;Y&quot;, just for the sake of generalizability</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">outcome</span>

<span class="c1"># get the model residuals from the model object</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">residuals_</span>

<span class="c1"># calculate the residual, the model, and the total sums of squares</span>
<span class="n">SS_res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">res</span><span class="p">))</span>
<span class="n">SS_tot</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span> <span class="p">)</span>
<span class="n">SS_mod</span> <span class="o">=</span> <span class="n">SS_tot</span> <span class="o">-</span> <span class="n">SS_res</span>

<span class="c1"># get the degrees of freedom for the model and the residuals</span>
<span class="n">df_mod</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">df_model_</span>
<span class="n">df_res</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">df_resid_</span>

<span class="c1"># caluculate the mean squares for the model and the residuals</span>
<span class="n">MS_mod</span> <span class="o">=</span> <span class="n">SS_mod</span> <span class="o">/</span> <span class="n">df_mod</span>
<span class="n">MS_res</span> <span class="o">=</span> <span class="n">SS_res</span> <span class="o">/</span> <span class="n">df_res</span>

<span class="c1"># calculate the F-statistic</span>
<span class="n">F</span> <span class="o">=</span> <span class="n">MS_mod</span> <span class="o">/</span> <span class="n">MS_res</span>

<span class="c1"># estimate the p-value</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">f</span><span class="o">.</span><span class="n">sf</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">df_mod</span><span class="p">,</span> <span class="n">df_res</span><span class="p">)</span>

<span class="c1"># display the results</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;F=&quot;</span><span class="p">,</span><span class="n">F</span><span class="p">,</span> <span class="s2">&quot;p=&quot;</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>F= 215.2382865368443 p= 2.1457300163209325e-36
</pre></div>
</div>
</div>
</div>
</section>
<section id="an-f-test-function">
<span id="ftestfunction"></span><h3><span class="section-number">16.6.2. </span>An F-test function<a class="headerlink" href="#an-f-test-function" title="Permalink to this heading">#</a></h3>
<p>A more compact way to do this would be to take everything I have done above and put it inside a <a class="reference internal" href="03.04-basic_programming.html#functions"><span class="std std-ref">function</span></a>. I’ve done this below, not least so that I will be able to copy/paste from it myself at some later date. Here is a function called <code class="docutils literal notranslate"><span class="pre">regression_f</span></code> that takes as its arguments a list of predictors, and an outcome variable, and spits out the <span class="math notranslate nohighlight">\(F\)</span> and <span class="math notranslate nohighlight">\(p\)</span> values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">regression_f</span><span class="p">(</span><span class="n">predictors</span><span class="p">,</span> <span class="n">outcome</span><span class="p">):</span>
    <span class="n">mod</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">predictors</span><span class="p">,</span> <span class="n">outcome</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">outcome</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">residuals_</span>
    <span class="n">SS_res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">res</span><span class="p">))</span>
    <span class="n">SS_tot</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span> <span class="p">)</span>
    <span class="n">SS_mod</span> <span class="o">=</span> <span class="n">SS_tot</span> <span class="o">-</span> <span class="n">SS_res</span>
    <span class="n">df_mod</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">df_model_</span>
    <span class="n">df_res</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">df_resid_</span>
    <span class="n">MS_mod</span> <span class="o">=</span> <span class="n">SS_mod</span> <span class="o">/</span> <span class="n">df_mod</span>
    <span class="n">MS_res</span> <span class="o">=</span> <span class="n">SS_res</span> <span class="o">/</span> <span class="n">df_res</span>
    <span class="n">F</span> <span class="o">=</span> <span class="n">MS_mod</span> <span class="o">/</span> <span class="n">MS_res</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">f</span><span class="o">.</span><span class="n">sf</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">df_mod</span><span class="p">,</span> <span class="n">df_res</span><span class="p">)</span>
    <span class="k">return</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Once we have run the function, all we need to do is plug in our values, and <code class="docutils literal notranslate"><span class="pre">regression_f</span></code> does the rest:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictors</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> <span class="s1">&#39;baby_sleep&#39;</span><span class="p">]]</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">]</span>

<span class="n">regression_f</span><span class="p">(</span><span class="n">predictors</span><span class="p">,</span> <span class="n">outcome</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(215.2382865368443, 2.1457300163209325e-36)
</pre></div>
</div>
</div>
</div>
</section>
<section id="tests-for-individual-coefficients">
<h3><span class="section-number">16.6.3. </span>Tests for individual coefficients<a class="headerlink" href="#tests-for-individual-coefficients" title="Permalink to this heading">#</a></h3>
<p>The <span class="math notranslate nohighlight">\(F\)</span>-test that we’ve just introduced is useful for checking that the model as a whole is performing better than chance. This is important: if your regression model doesn’t produce a significant result for the <span class="math notranslate nohighlight">\(F\)</span>-test then you probably don’t have a very good regression model (or, quite possibly, you don’t have very good data). However, while failing this test is a pretty strong indicator that the model has problems, <em>passing</em> the test (i.e., rejecting the null) doesn’t imply that the model is good! Why is that, you might be wondering? The answer to that can be found by looking at the coefficients for the multiple linear regression model we calculated earlier:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictors</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> <span class="s1">&#39;baby_sleep&#39;</span><span class="p">]]</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">]</span>

<span class="n">mod2</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">predictors</span><span class="p">,</span> <span class="n">outcome</span><span class="p">)</span>
<span class="n">mod2</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>coef</th>
      <th>se</th>
      <th>T</th>
      <th>pval</th>
      <th>r2</th>
      <th>adj_r2</th>
      <th>CI[2.5%]</th>
      <th>CI[97.5%]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Intercept</td>
      <td>125.97</td>
      <td>3.04</td>
      <td>41.42</td>
      <td>0.00</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>119.93</td>
      <td>132.00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>dan_sleep</td>
      <td>-8.95</td>
      <td>0.55</td>
      <td>-16.17</td>
      <td>0.00</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>-10.05</td>
      <td>-7.85</td>
    </tr>
    <tr>
      <th>2</th>
      <td>baby_sleep</td>
      <td>0.01</td>
      <td>0.27</td>
      <td>0.04</td>
      <td>0.97</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>-0.53</td>
      <td>0.55</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>I can’t help but notice that the estimated regression coefficient for the <code class="docutils literal notranslate"><span class="pre">baby_sleep</span></code> variable is tiny (0.01), relative to the value that we get for <code class="docutils literal notranslate"><span class="pre">dan_sleep</span></code> (-8.95). Given that these two variables are absolutely on the same scale (they’re both measured in “hours slept”), I find this suspicious. In fact, I’m beginning to suspect that it’s really only the amount of sleep that <em>I</em> get that matters in order to predict my grumpiness.</p>
<p>Once again, we can reuse a hypothesis test that we discussed earlier, this time the <span class="math notranslate nohighlight">\(t\)</span>-test. The test that we’re interested has a null hypothesis that the true regression coefficient is zero (<span class="math notranslate nohighlight">\(b = 0\)</span>), which is to be tested against the alternative hypothesis that it isn’t (<span class="math notranslate nohighlight">\(b \neq 0\)</span>). That is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{rl}
H_0: &amp; b = 0 \\
H_1: &amp; b \neq 0 
\end{array}
\end{split}\]</div>
<p>How can we test this? Well, if the <a class="reference internal" href="04.03-estimation.html#clt"><span class="std std-ref">central limit theorem</span></a> is kind to us, we might be able to guess that the sampling distribution of <span class="math notranslate nohighlight">\(\hat{b}\)</span>, the estimated regression coefficient, is a normal distribution with mean centred on <span class="math notranslate nohighlight">\(b\)</span>. What that would mean is that if the null hypothesis were true, then the sampling distribution of <span class="math notranslate nohighlight">\(\hat{b}\)</span> has mean zero and unknown standard deviation. Assuming that we can come up with a good estimate for the <a class="reference internal" href="04.03-estimation.html#clt"><span class="std std-ref">standard error</span></a> of the regression coefficient, <span class="math notranslate nohighlight">\(\text{SE}({\hat{b}})\)</span>, then we’re in luck. That’s <em>exactly</em> the situation for which we introduced the one-sample <span class="math notranslate nohighlight">\(t\)</span> way back in <a class="reference internal" href="05.02-ttest.html#ttest"><span class="std std-ref">the chapter on t-tests</span></a>. So let’s define a <span class="math notranslate nohighlight">\(t\)</span>-statistic like this,</p>
<div class="math notranslate nohighlight">
\[
t = \frac{\hat{b}}{\text{SE}({\hat{b})}}
\]</div>
<p>I’ll skip over the reasons why, but our degrees of freedom in this case are <span class="math notranslate nohighlight">\(df = N- K- 1\)</span>. Irritatingly, the estimate of the standard error of the regression coefficient, <span class="math notranslate nohighlight">\(\text{SE}({\hat{b}})\)</span>, is not as easy to calculate as the standard error of the mean that we used for the simpler <span class="math notranslate nohighlight">\(t\)</span>-tests <a class="reference internal" href="05.02-ttest.html#ttest"><span class="std std-ref">earlier</span></a>. In fact, the formula is somewhat ugly, and not terribly helpful to look at. For our purposes it’s sufficient to point out that the standard error of the  estimated regression coefficient depends on both the predictor and outcome variables, and is somewhat sensitive to violations of the homogeneity of variance assumption (discussed shortly).</p>
<p>In any case, this <span class="math notranslate nohighlight">\(t\)</span>-statistic can be interpreted in the same way as the <span class="math notranslate nohighlight">\(t\)</span>-statistics that we discussed <a class="reference internal" href="05.02-ttest.html#ttest"><span class="std std-ref">earlier</span></a>. Assuming that you have a two-sided alternative (i.e., you don’t really care if <span class="math notranslate nohighlight">\(b &gt;0\)</span> or <span class="math notranslate nohighlight">\(b &lt; 0\)</span>), then it’s the extreme values of <span class="math notranslate nohighlight">\(t\)</span> (i.e., a lot less than zero or a lot greater than zero) that suggest that you should reject the null hypothesis.</p>
<p>Now we are in a position to understand all the values in the multiple regression table provided by <code class="docutils literal notranslate"><span class="pre">pingouin</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mod2</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>coef</th>
      <th>se</th>
      <th>T</th>
      <th>pval</th>
      <th>r2</th>
      <th>adj_r2</th>
      <th>CI[2.5%]</th>
      <th>CI[97.5%]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Intercept</td>
      <td>125.97</td>
      <td>3.04</td>
      <td>41.42</td>
      <td>0.00</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>119.93</td>
      <td>132.00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>dan_sleep</td>
      <td>-8.95</td>
      <td>0.55</td>
      <td>-16.17</td>
      <td>0.00</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>-10.05</td>
      <td>-7.85</td>
    </tr>
    <tr>
      <th>2</th>
      <td>baby_sleep</td>
      <td>0.01</td>
      <td>0.27</td>
      <td>0.04</td>
      <td>0.97</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>-0.53</td>
      <td>0.55</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Each row in this table refers to one of the coefficients in the regression model. The first row is the intercept term, and the later ones look at each of the predictors. The columns give you all of the relevant information. The <code class="docutils literal notranslate"><span class="pre">coef</span></code> column is the actual estimate of <span class="math notranslate nohighlight">\(b\)</span> (e.g., 125.96 for the intercept, -8.9 for the <code class="docutils literal notranslate"><span class="pre">dan_sleep</span></code> predictor, and 0.01 for the <code class="docutils literal notranslate"><span class="pre">baby_sleep</span></code> predictor). The <code class="docutils literal notranslate"><span class="pre">se</span></code> column is the standard error estimate <span class="math notranslate nohighlight">\(\hat\sigma_b\)</span>. The <code class="docutils literal notranslate"><span class="pre">T</span></code> column gives you the <span class="math notranslate nohighlight">\(t\)</span>-statistic, and it’s worth noticing that in this table <span class="math notranslate nohighlight">\(t= \hat{b}/\text{SE}({\hat{b}})\)</span> every time. The <code class="docutils literal notranslate"><span class="pre">pval</span></code> column gives you the actual <span class="math notranslate nohighlight">\(p\)</span> value for each of these tests.<a class="footnote-reference brackets" href="#notecorrection" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a> The <code class="docutils literal notranslate"><span class="pre">r2</span></code>and àdj_r2` columns give the <span class="math notranslate nohighlight">\(R^2\)</span> value and the adjusted <span class="math notranslate nohighlight">\(R^2\)</span> for the model, and the last two columns give us the upper and lower <a class="reference internal" href="04.03-estimation.html#ci"><span class="std std-ref">confidence interval</span></a> bounds for each estimate.</p>
<p>If we add our <a class="reference internal" href="#ftestfunction"><span class="std std-ref">F-test results</span></a> to the mix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span> <span class="o">=</span> <span class="n">regression_f</span><span class="p">(</span><span class="n">predictors</span><span class="p">,</span> <span class="n">outcome</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;F:&quot;</span><span class="p">,</span> <span class="n">f</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="s2">&quot;p:&quot;</span><span class="p">,</span> <span class="n">f</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>F: 215.24 p: 2.1457300163209325e-36
</pre></div>
</div>
</div>
</div>
<p>we have everything we need to evaluate our model. In this case, the model performs significantly better than you’d expect by chance (<span class="math notranslate nohighlight">\(F(2,97) = 215.2\)</span>, <span class="math notranslate nohighlight">\(p&lt;.001\)</span>), which isn’t all that surprising: the <span class="math notranslate nohighlight">\(R^2 = .812\)</span> value indicate that the regression model accounts for 81.2% of the variability in the outcome measure. However, when we look back up at the <span class="math notranslate nohighlight">\(t\)</span>-tests for each of the individual coefficients, we have pretty strong evidence that the <code class="docutils literal notranslate"><span class="pre">baby_sleep</span></code> variable has no significant effect; all the work is being done by the <code class="docutils literal notranslate"><span class="pre">dan_sleep</span></code> variable. Taken together, these results suggest that <code class="docutils literal notranslate"><span class="pre">mod2</span></code> is actually the wrong model for the data: you’d probably be better off dropping the <code class="docutils literal notranslate"><span class="pre">baby_sleep</span></code> predictor entirely. In other words, the <code class="docutils literal notranslate"><span class="pre">mod1</span></code> model that we started with is the better model.</p>
</section>
</section>
<section id="testing-the-significance-of-a-correlation">
<span id="corrhyp"></span><h2><span class="section-number">16.7. </span>Testing the significance of a correlation<a class="headerlink" href="#testing-the-significance-of-a-correlation" title="Permalink to this heading">#</a></h2>
<section id="hypothesis-tests-for-a-single-correlation">
<h3><span class="section-number">16.7.1. </span>Hypothesis tests for a single correlation<a class="headerlink" href="#hypothesis-tests-for-a-single-correlation" title="Permalink to this heading">#</a></h3>
<p>I don’t want to spend too much time on this, but it’s worth very briefly returning to the point I made earlier, that Pearson correlations are basically the same thing as linear regressions with only a single predictor added to the model. What this means is that the hypothesis tests that I just described in a regression context can also be applied to correlation coefficients. To see this, let’s just revist our <code class="docutils literal notranslate"><span class="pre">mod1</span></code> model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">]</span> <span class="c1"># the predictor</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">]</span> <span class="c1"># the outcome</span>
<span class="n">mod1</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">mod1</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>coef</th>
      <th>se</th>
      <th>T</th>
      <th>pval</th>
      <th>r2</th>
      <th>adj_r2</th>
      <th>CI[2.5%]</th>
      <th>CI[97.5%]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Intercept</td>
      <td>125.96</td>
      <td>3.02</td>
      <td>41.76</td>
      <td>0.0</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>119.97</td>
      <td>131.94</td>
    </tr>
    <tr>
      <th>1</th>
      <td>dan_sleep</td>
      <td>-8.94</td>
      <td>0.43</td>
      <td>-20.85</td>
      <td>0.0</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>-9.79</td>
      <td>-8.09</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The important thing to note here is the <span class="math notranslate nohighlight">\(t\)</span> test associated with the predictor, in which we get a result of <span class="math notranslate nohighlight">\(t(98) = -20.85\)</span>, <span class="math notranslate nohighlight">\(p&lt;.001\)</span>. Now let’s compare this to the output of the <code class="docutils literal notranslate"><span class="pre">corr</span></code> function from <code class="docutils literal notranslate"><span class="pre">pinguoin</span></code>, which runs a hypothesis test to see if the observed correlation between two variables is significantly different from 0.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pg</span><span class="o">.</span><span class="n">corr</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>n</th>
      <th>r</th>
      <th>CI95%</th>
      <th>p-val</th>
      <th>BF10</th>
      <th>power</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>pearson</th>
      <td>100</td>
      <td>-0.903384</td>
      <td>[-0.93, -0.86]</td>
      <td>8.176426e-38</td>
      <td>2.591e+34</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now, just like the <span class="math notranslate nohighlight">\(F\)</span>-test from earlier, <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> unfortunately doesn’t calculate a <span class="math notranslate nohighlight">\(t\)</span>-statistic for us automatically when running a correlation. But the formula for the <span class="math notranslate nohighlight">\(t\)</span>-statistic of a Pearson correlation is just</p>
<div class="math notranslate nohighlight">
\[
t = r\sqrt{\frac{n-2}{1-r^2}}
\]</div>
<p>so, with the output from <code class="docutils literal notranslate"><span class="pre">pg.corr(X,Y)</span></code> above, it’s not too difficult to find <span class="math notranslate nohighlight">\(t\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">sqrt</span>

<span class="n">t</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.903384</span><span class="o">*</span><span class="n">sqrt</span><span class="p">((</span><span class="mi">100</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="p">(</span><span class="o">-</span><span class="mf">0.903384</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
<span class="n">t</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-20.85439996017091
</pre></div>
</div>
</div>
</div>
<p>Look familiar? -20.85 was the same <span class="math notranslate nohighlight">\(t\)</span>-value that we got when we ran the regression model. That’s because the test for the significance of a correlation is identical to the <span class="math notranslate nohighlight">\(t\)</span> test that we run on a coefficient in a regression model.</p>
</section>
<section id="hypothesis-tests-for-all-pairwise-correlations">
<span id="corrhyp2"></span><h3><span class="section-number">16.7.2. </span>Hypothesis tests for all pairwise correlations<a class="headerlink" href="#hypothesis-tests-for-all-pairwise-correlations" title="Permalink to this heading">#</a></h3>
<p>Okay, one more digression before I return to regression properly. In the previous section I talked about you can run a hypothesis test on a single correlation. But we aren’t restricted to computing a single correlation: you can compute <em>all</em> pairwise correlations among the variables in your data set. This leads people to the natural question: can we also run hypothesis tests on all of the pairwise correlations in our data using <code class="docutils literal notranslate"><span class="pre">pg.corr</span></code>?</p>
<p>The answer is no, and there’s a very good reason for this. Testing a single correlation is fine: if you’ve got some reason to be asking “is A related to B?”, then you should absolutely run a test to see if there’s a significant correlation. But if you’ve got variables A, B, C, D and E and you’re thinking about testing the correlations among all possible pairs of these, a statistician would want to ask: what’s your hypothesis? If you’re in the position of wanting to test all possible pairs of variables, then you’re pretty clearly on a fishing expedition, hunting around in search of significant effects when you don’t actually have a clear research hypothesis in mind. This is <em>dangerous</em>, and perhaps the authors of the <code class="docutils literal notranslate"><span class="pre">corr</span></code> function didn’t want to endorse this sort of behavior. <code class="docutils literal notranslate"><span class="pre">corr</span></code> does have the nice feature that you can call it as an attribute of your dataframe, so for our parenthood data, if we want to see all the parwise correlations in the data, you can simply write</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>dan_sleep</th>
      <th>baby_sleep</th>
      <th>dan_grump</th>
      <th>day</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>dan_sleep</th>
      <td>1.000000</td>
      <td>0.627949</td>
      <td>-0.903384</td>
      <td>-0.098408</td>
    </tr>
    <tr>
      <th>baby_sleep</th>
      <td>0.627949</td>
      <td>1.000000</td>
      <td>-0.565964</td>
      <td>-0.010434</td>
    </tr>
    <tr>
      <th>dan_grump</th>
      <td>-0.903384</td>
      <td>-0.565964</td>
      <td>1.000000</td>
      <td>0.076479</td>
    </tr>
    <tr>
      <th>day</th>
      <td>-0.098408</td>
      <td>-0.010434</td>
      <td>0.076479</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>and you get a nice correlation matrix, but no p-values.</p>
<p>On the other hand… a somewhat less hardline view might be to argue we’ve encountered this situation before, back when we talked about <em>post hoc tests</em> in ANOVA. When running post hoc tests, we didn’t have any specific comparisons in mind, so what we did was apply a correction (e.g., Bonferroni, Holm, etc) in order to avoid the possibility of an inflated Type I error rate. From this perspective, it’s okay to run hypothesis tests on all your pairwise correlations, but you must treat them as post hoc analyses, and if so you need to apply a correction for multiple comparisons. <code class="docutils literal notranslate"><span class="pre">rcorr</span></code>, also from <code class="docutils literal notranslate"><span class="pre">pingouin</span></code>, lets you do this. You can use the <code class="docutils literal notranslate"><span class="pre">padjust</span></code> argument to specify what kind of correction you would like to apply; here I have chosen a Bonferroni correction:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">rcorr</span><span class="p">(</span><span class="n">padjust</span> <span class="o">=</span> <span class="s1">&#39;bonf&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>dan_sleep</th>
      <th>baby_sleep</th>
      <th>dan_grump</th>
      <th>day</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>dan_sleep</th>
      <td>-</td>
      <td>***</td>
      <td>***</td>
      <td></td>
    </tr>
    <tr>
      <th>baby_sleep</th>
      <td>0.628</td>
      <td>-</td>
      <td>***</td>
      <td></td>
    </tr>
    <tr>
      <th>dan_grump</th>
      <td>-0.903</td>
      <td>-0.566</td>
      <td>-</td>
      <td></td>
    </tr>
    <tr>
      <th>day</th>
      <td>-0.098</td>
      <td>-0.01</td>
      <td>0.076</td>
      <td>-</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The little stars indicate the “significance level”: one star for <span class="math notranslate nohighlight">\(p&lt;0.05\)</span>, two stars for <span class="math notranslate nohighlight">\(p&lt;0.01\)</span>, and three stars for <span class="math notranslate nohighlight">\(p&lt;0.001\)</span>.</p>
<p>So there you have it. If you really desperately want to do pairwise hypothesis tests on your correlations, the <code class="docutils literal notranslate"><span class="pre">rcorr</span></code> function will let you do it. But please, <strong>please</strong> be careful. I can’t count the number of times I’ve had a student panicking in my office because they’ve run these pairwise correlation tests, and they get one or two significant results that don’t make any sense. For some reason, the moment people see those little significance stars appear, they feel compelled to throw away all common sense and assume that the results must correspond to something real that requires an explanation. In most such cases, my experience has been that the right answer is “it’s a Type I error”. Remember when we talked about the <a class="reference internal" href="05.03-anova.html#fwer"><span class="std std-ref">family-wise error rate</span></a>? The more tests you do on the same data, the greater your chances of finding statistically significant results just by, uh, chance.</p>
</section>
<section id="calculating-standardised-regression-coefficients">
<span id="regressioncoefs"></span><h3><span class="section-number">16.7.3. </span>Calculating standardised regression coefficients<a class="headerlink" href="#calculating-standardised-regression-coefficients" title="Permalink to this heading">#</a></h3>
<p>One more thing that you might want to do is to calculate “standardised” regression coefficients, often denoted <span class="math notranslate nohighlight">\(\beta\)</span>. The rationale behind standardised coefficients goes like this. In a lot of situations, your variables are on fundamentally different scales. Suppose, for example, my regression model aims to predict people’s IQ scores, using their educational attainment (number of years of education) and their income as predictors. Obviously, educational attainment and income are not on the same scales: the number of years of schooling can only vary by 10s of years, whereas income would vary by 10,000s of dollars (or more). The units of measurement have a big influence on the regression coefficients: the <span class="math notranslate nohighlight">\(b\)</span> coefficients only make sense when interpreted in light of the units, both of the predictor variables and the outcome variable. This makes it very difficult to compare the coefficients of different predictors. Yet there are situations where you really do want to make comparisons between different coefficients. Specifically, you might want some kind of standard measure of which predictors have the strongest relationship to the outcome. This is what <strong><em>standardised coefficients</em></strong> aim to do.</p>
<p>The basic idea is quite simple: the standardised coefficients are the coefficients that you would have obtained if you’d converted all the variables to <span class="xref myst">standard scores</span> <span class="math notranslate nohighlight">\(z\)</span>-scores before running the regression.<a class="footnote-reference brackets" href="#noteregressors" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a>  The idea here is that, by converting all the predictors to <span class="math notranslate nohighlight">\(z\)</span>-scores, they all go into the regression on the same scale, thereby removing the problem of having variables on different scales. Regardless of what the original variables were, a <span class="math notranslate nohighlight">\(\beta\)</span> value of 1 means that an increase in the predictor of 1 standard deviation will produce a corresponding 1 standard deviation increase in the outcome variable. Therefore, if variable A has a larger absolute value of <span class="math notranslate nohighlight">\(\beta\)</span> than variable B, it is deemed to have a stronger relationship with the outcome. Or at least that’s the idea: it’s worth being a little cautious here, since this does rely very heavily on the assumption that “a 1 standard deviation change” is fundamentally the same kind of thing for all variables. It’s not always obvious that this is true.</p>
<p>Still, let’s give it a try on the <code class="docutils literal notranslate"><span class="pre">parenthood</span></code> data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="kn">import</span> <span class="nn">pingouin</span> <span class="k">as</span> <span class="nn">pg</span>

<span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_sleep_standard&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">zscore</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">])</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;baby_sleep_standard&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">zscore</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;baby_sleep&#39;</span><span class="p">])</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">zscore</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">])</span>


<span class="n">predictors</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;dan_sleep_standard&#39;</span><span class="p">,</span> <span class="s1">&#39;baby_sleep_standard&#39;</span><span class="p">]]</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">]</span>

<span class="n">mod3</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">predictors</span><span class="p">,</span> <span class="n">outcome</span><span class="p">)</span>
<span class="n">mod3</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>coef</th>
      <th>se</th>
      <th>T</th>
      <th>pval</th>
      <th>r2</th>
      <th>adj_r2</th>
      <th>CI[2.5%]</th>
      <th>CI[97.5%]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Intercept</td>
      <td>-0.0000</td>
      <td>0.0435</td>
      <td>-0.0000</td>
      <td>1.0000</td>
      <td>0.8161</td>
      <td>0.8123</td>
      <td>-0.0864</td>
      <td>0.0864</td>
    </tr>
    <tr>
      <th>1</th>
      <td>dan_sleep_standard</td>
      <td>-0.9047</td>
      <td>0.0559</td>
      <td>-16.1715</td>
      <td>0.0000</td>
      <td>0.8161</td>
      <td>0.8123</td>
      <td>-1.0158</td>
      <td>-0.7937</td>
    </tr>
    <tr>
      <th>2</th>
      <td>baby_sleep_standard</td>
      <td>0.0022</td>
      <td>0.0559</td>
      <td>0.0388</td>
      <td>0.9691</td>
      <td>0.8161</td>
      <td>0.8123</td>
      <td>-0.1089</td>
      <td>0.1132</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>This clearly shows that the <code class="docutils literal notranslate"><span class="pre">dan_sleep</span></code> variable has a much stronger effect than the <code class="docutils literal notranslate"><span class="pre">baby_sleep</span></code> variable. However, this is a perfect example of a situation where it would probably make sense to use the original coefficients <span class="math notranslate nohighlight">\(b\)</span> rather than the standardised coefficients <span class="math notranslate nohighlight">\(\beta\)</span>. After all, my sleep and the baby’s sleep are <em>already</em> on the same scale: number of hours slept. Why complicate matters by converting these to <span class="math notranslate nohighlight">\(z\)</span>-scores?</p>
</section>
</section>
<section id="assumptions-of-regression">
<span id="regressionassumptions"></span><h2><span class="section-number">16.8. </span>Assumptions of regression<a class="headerlink" href="#assumptions-of-regression" title="Permalink to this heading">#</a></h2>
<p>The linear regression model that I’ve been discussing relies on several assumptions. In the section on <a class="reference internal" href="#regressiondiagnostics"><span class="std std-ref">regression diagnostics</span></a> we’ll talk a lot more about how to check that these assumptions are being met, but first, let’s have a look at each of them.</p>
<ul class="simple">
<li><p><em>Normality</em>. Like half the models in statistics, standard linear regression relies on an assumption of normality. Specifically, it assumes that the <em>residuals</em> are normally distributed. It’s actually okay if the predictors <span class="math notranslate nohighlight">\(X\)</span> and the outcome <span class="math notranslate nohighlight">\(Y\)</span> are non-normal, so long as the residuals <span class="math notranslate nohighlight">\(\epsilon\)</span> are normal. See <a class="reference internal" href="#regressionnormality"><span class="std std-ref">Checking the normality of the residuals</span></a>.</p></li>
<li><p><em>Linearity</em>. A pretty fundamental assumption of the linear regression model is that relationship between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> actually be linear! Regardless of whether it’s a simple regression or a multiple regression, we assume that the relatiships involved are linear. See <a class="reference internal" href="#regressionlinearity"><span class="std std-ref">Checking the linearity of the relationship</span></a>.</p></li>
<li><p><em>Homogeneity of variance</em>. Strictly speaking, the regression model assumes that each residual <span class="math notranslate nohighlight">\(\epsilon_i\)</span> is generated from a normal distribution with mean 0, and (more importantly for the current purposes) with a standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> that is the same for every single residual. In practice, it’s impossible to test the assumption that every residual is identically distributed. Instead, what we care about is that the standard deviation of the residual is the same for all values of <span class="math notranslate nohighlight">\(\hat{Y}\)</span>, and (if we’re being especially paranoid) all values of every predictor <span class="math notranslate nohighlight">\(X\)</span> in the model. See <a class="reference internal" href="#regressionhomogeneity"><span class="std std-ref">Checking the homogeneity of variance</span></a>.</p></li>
<li><p><em>Uncorrelated predictors</em>. The idea here is that, is a multiple regression model, you don’t want your predictors to be too strongly correlated with each other. This isn’t  “technically” an assumption of the regression model, but in practice it’s required. Predictors that are too strongly correlated with each other (referred to as “collinearity”) can cause problems when evaluating the model. See <a class="reference internal" href="#regressioncollinearity"><span class="std std-ref">Checking for collinearity</span></a></p></li>
<li><p><em>Residuals are independent of each other</em>. This is really just a “catch all” assumption, to the effect that “there’s nothing else funny going on in the residuals”. If there is something weird (e.g., the residuals all depend heavily on some other unmeasured variable) going on, it might screw things up.</p></li>
<li><p><em>No “bad” outliers</em>. Again, not actually a technical assumption of the model (or rather, it’s sort of implied by all the others), but there is an implicit assumption that your regression model isn’t being too strongly influenced by one or two anomalous data points; since this raises questions about the adequacy of the model, and the trustworthiness of the data in some cases. See <a class="reference internal" href="#regressionoutliers"><span class="std std-ref">Three kinds of anomalous data</span></a>.</p></li>
</ul>
</section>
<section id="model-checking">
<span id="regressiondiagnostics"></span><h2><span class="section-number">16.9. </span>Model checking<a class="headerlink" href="#model-checking" title="Permalink to this heading">#</a></h2>
<p>The main focus of this section is <strong><em>regression diagnostics</em></strong>, a term that refers to the art of checking that the assumptions of your regression model have been met, figuring out how to fix the model if the assumptions are violated, and generally to check that nothing “funny” is going on. I refer to this as the “art” of model checking with good reason: it’s not easy, and while there are a lot of fairly standardised tools that you can use to diagnose and maybe even cure the problems that ail your model (if there are any, that is!), you really do need to exercise a certain amount of judgment when doing this. It’s easy to get lost in all the details of checking this thing or that thing, and it’s quite exhausting to try to remember what all the different things are. This has the very nasty side effect that a lot of people get frustrated when trying to learn <em>all</em> the tools, so instead they decide not to do <em>any</em> model checking. This is a bit of a worry!</p>
<p>In this section, I describe several different things you can do to check that your regression model is doing what it’s supposed to. It doesn’t cover the full range of things you could do, but it’s still much more detailed than what I see a lot of people doing in practice; and I don’t usually cover all of this in my intro stats class myself. However, I do think it’s important that you get a sense of what tools are at your disposal, so I’ll try to introduce a bunch of them here.</p>
<section id="three-kinds-of-residuals">
<h3><span class="section-number">16.9.1. </span>Three kinds of residuals<a class="headerlink" href="#three-kinds-of-residuals" title="Permalink to this heading">#</a></h3>
<p>The majority of regression diagnostics revolve around looking at the residuals, and by now you’ve probably formed a sufficiently pessimistic theory of statistics to be able to guess that – precisely <em>because</em> of the fact that we care a lot about the residuals – there are several different kinds of  residual that we might consider. In particular, the following three kinds of residual are referred to in this section: “ordinary residuals”, “standardised residuals”, and “Studentised residuals”. There is a fourth kind that you’ll see referred to in some of the figures, and that’s the “Pearson residual”: however, for the models that we’re talking about in this chapter, the Pearson residual is identical to the ordinary residual.</p>
<p>The first and simplest kind of residuals that we care about are <strong><em>ordinary residuals</em></strong>. These are the actual, raw residuals that I’ve been talking about throughout this chapter. The ordinary residual is just the difference between the fitted value <span class="math notranslate nohighlight">\(\hat{Y}_i\)</span> and the observed value <span class="math notranslate nohighlight">\(Y_i\)</span>. I’ve been using the notation <span class="math notranslate nohighlight">\(\epsilon_i\)</span> to refer to the <span class="math notranslate nohighlight">\(i\)</span>-th ordinary residual, and by gum I’m going to stick to it. With this in mind, we have the very simple equation</p>
<div class="math notranslate nohighlight">
\[
\epsilon_i = Y_i - \hat{Y}_i
\]</div>
<p>This is of course what we saw earlier, and unless I specifically refer to some other kind of residual, this is the one I’m talking about. So there’s nothing new here: I just wanted to repeat myself. In any case, if you have run your regression model using <code class="docutils literal notranslate"><span class="pre">pingouin</span></code>, you can access the residuals from your model (in our case, our <code class="docutils literal notranslate"><span class="pre">mod2</span></code>)like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">res</span> <span class="o">=</span> <span class="n">mod2</span><span class="o">.</span><span class="n">residuals_</span>
</pre></div>
</div>
</div>
</div>
<p>One drawback to using ordinary residuals is that they’re always on a different scale, depending on what the outcome variable is and how good the regression model is. That is, unless you’ve decided to run a regression model without an intercept term, the ordinary residuals will have mean 0; but the variance is different for every regression. In a lot of contexts, especially where you’re only interested in the <em>pattern</em> of the residuals and not their actual values, it’s convenient to estimate the <strong><em>standardised residuals</em></strong>, which are normalised in such a way as to have standard deviation 1. The way we calculate these is to divide the ordinary residual by an estimate of the (population) standard deviation of these residuals. For technical reasons, mumble mumble, the formula for this is:</p>
<div class="math notranslate nohighlight">
\[
\epsilon_{i}^\prime = \frac{\epsilon_i}{\hat{\sigma} \sqrt{1-h_i}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat\sigma\)</span> in this context is the estimated population standard deviation of the ordinary residuals, and <span class="math notranslate nohighlight">\(h_i\)</span> is the “hat value” of the <span class="math notranslate nohighlight">\(i\)</span>th observation. I haven’t explained hat values to you yet (but have no fear,<a class="footnote-reference brackets" href="#notehope" id="id10" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a> it’s coming shortly), so this won’t make a lot of sense. For now, it’s enough to interpret the standardised residuals as if we’d converted the ordinary residuals to <span class="math notranslate nohighlight">\(z\)</span>-scores. In fact, that is more or less the truth, it’s just that we’re being a bit fancier. Now, unfortunately, <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> does not provide standardized residuals, so if we want to inspect these, the best option is probably <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>

<span class="n">predictors</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> <span class="s1">&#39;baby_sleep&#39;</span><span class="p">]]</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">]</span>

<span class="c1">## fit regression model</span>
<span class="n">predictors</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">predictors</span><span class="p">)</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">outcome</span><span class="p">,</span> <span class="n">predictors</span><span class="p">)</span>
<span class="n">est</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1">#obtain standardized residuals</span>
<span class="n">influence</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">get_influence</span><span class="p">()</span>
<span class="n">res_standard</span> <span class="o">=</span> <span class="n">influence</span><span class="o">.</span><span class="n">resid_studentized_internal</span>
</pre></div>
</div>
</div>
</div>
<p>The third kind of residuals are <strong><em>Studentised residuals</em></strong> (also called “jackknifed residuals”) and they’re even fancier than standardised residuals. Again, the idea is to take the ordinary residual and divide it by some quantity in order to estimate some standardised notion of the residual, but the formula for doing the calculations this time is subtly different:</p>
<div class="math notranslate nohighlight">
\[
\epsilon_{i}^* = \frac{\epsilon_i}{\hat{\sigma}_{(-i)} \sqrt{1-h_i}}
\]</div>
<p>Notice that our estimate of the standard deviation here is written <span class="math notranslate nohighlight">\(\hat{\sigma}_{(-i)}\)</span>. What this corresponds to is the estimate of the residual standard deviation that you <em>would have obtained</em>, if you just deleted the <span class="math notranslate nohighlight">\(i\)</span>th observation from the data set. This sounds like the sort of thing that would be a nightmare to calculate, since it seems to be saying that you have to run <span class="math notranslate nohighlight">\(N\)</span> new regression models (even a modern computer might grumble a bit at that, especially if you’ve got a large data set). Fortunately, some terribly clever person has shown that this standard deviation estimate is actually given by the following equation:</p>
<div class="math notranslate nohighlight">
\[
\hat\sigma_{(-i)} = \hat{\sigma} \ \sqrt{\frac{N-K-1 - {\epsilon_{i}^\prime}^2}{N-K-2}}
\]</div>
<p>Isn’t that a pip?</p>
<p>If you ever need to calculate studentised residuals yourself, this is also possible using <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>. Since we have already used <code class="docutils literal notranslate"><span class="pre">statmodels</span></code> to estimate our model above, when we calculated the standardized residuals, we can just re-use our model estimate <code class="docutils literal notranslate"><span class="pre">est</span></code> from before, and the first column of the resulting dataframe gives us our studentized residuals:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stud_res</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">outlier_test</span><span class="p">()</span>
<span class="n">stud_res</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>student_resid</th>
      <th>unadj_p</th>
      <th>bonf(p)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.494821</td>
      <td>0.621857</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.105570</td>
      <td>0.271676</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.461729</td>
      <td>0.645321</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.475346</td>
      <td>0.635620</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.166721</td>
      <td>0.867940</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Before moving on, I should point out that you don’t often need to manually extract these residuals yourself, even though they are at the heart of almost all regression diagnostics. Most of the time the various functions that run the diagnostics will take care of these calculations for you.</p>
</section>
<section id="three-kinds-of-anomalous-data">
<span id="regressionoutliers"></span><h3><span class="section-number">16.9.2. </span>Three kinds of anomalous data<a class="headerlink" href="#three-kinds-of-anomalous-data" title="Permalink to this heading">#</a></h3>
<p>One danger that you can run into with linear regression models is that your analysis might be disproportionately sensitive to a smallish number of “unusual” or “anomalous” observations. In the context of linear regression, there are three conceptually distinct ways in which an observation might be called “anomalous”. All three are interesting, but they have rather different implications for your analysis.</p>
<p>The first kind of unusual observation is an <strong><em>outlier</em></strong>. The definition of an outlier (in this context) is an observation that is very different from what the regression model predicts. An example is shown in <a class="reference internal" href="#fig-outlier"><span class="std std-numref">Fig. 16.5</span></a>. In practice, we operationalise this concept by saying that an outlier is an observation that has a very large Studentised residual, <span class="math notranslate nohighlight">\(\epsilon_i^*\)</span>. Outliers are interesting: a big outlier <em>might</em> correspond to junk data – e.g., the variables might have been entered incorrectly, or some other defect may be detectable. Note that you shouldn’t throw an observation away just because it’s an outlier. But the fact that it’s an outlier is often a cue to look more closely at that case, and try to find out why it’s so different.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">curve_fit</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>


<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">3.6</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
     <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">1.7</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">],</span>
     <span class="s1">&#39;y2&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">1.7</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">]</span>
    <span class="p">})</span>



<span class="c1"># fit linear regression model and save parameters</span>
<span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span>

<span class="n">initialParameters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>

<span class="n">fittedParameters</span><span class="p">,</span> <span class="n">pcov</span> <span class="o">=</span> <span class="n">curve_fit</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">initialParameters</span><span class="p">)</span>

<span class="n">modelPredictions</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="o">*</span><span class="n">fittedParameters</span><span class="p">)</span> 

<span class="n">xModel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]),</span> <span class="nb">max</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]))</span>
<span class="n">yModel</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">xModel</span><span class="p">,</span> <span class="o">*</span><span class="n">fittedParameters</span><span class="p">)</span>


<span class="c1"># plot data</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span> 
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">()</span>


<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>

<span class="c1"># add regression line</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xModel</span><span class="p">,</span> <span class="n">yModel</span><span class="p">)</span>


<span class="n">initialParameters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>

<span class="n">fittedParameters</span><span class="p">,</span> <span class="n">pcov</span> <span class="o">=</span> <span class="n">curve_fit</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;y2&#39;</span><span class="p">],</span> <span class="n">initialParameters</span><span class="p">)</span>

<span class="n">modelPredictions</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="o">*</span><span class="n">fittedParameters</span><span class="p">)</span> 

<span class="n">xModel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]),</span> <span class="nb">max</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]))</span>
<span class="n">yModel</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">xModel</span><span class="p">,</span> <span class="o">*</span><span class="n">fittedParameters</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xModel</span><span class="p">,</span> <span class="n">yModel</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mf">2.4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.4</span> <span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;outlier-fig&quot;</span><span class="p">,</span> <span class="n">fig</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<figure class="align-default" id="fig-outlier" style="width: 600px">
<img alt="_images/43d60aac2a02d50e587f5e0cf1fc7b04bd880c73542bd8e7c8063d581b4d9aa0.png" src="_images/43d60aac2a02d50e587f5e0cf1fc7b04bd880c73542bd8e7c8063d581b4d9aa0.png" />
<figcaption>
<p><span class="caption-number">Fig. 16.5 </span><span class="caption-text">An illustration of outliers. The orange line plots the regression line estimated when the anomalous (red) data point is included, and the dotted line shows the residual for the outlier. The blue line shows the regression line that would have been estimated without the anomalous observation included. The outlier has an unusual value on the outcome (y axis location) but not the predictor (x axis location), and lies a long way from the regression line.</span><a class="headerlink" href="#fig-outlier" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The second way in which an observation can be unusual is if it has high <strong><em>leverage</em></strong>: this happens when the observation is very different from all the other observations. This doesn’t necessarily have to correspond to a large residual: if the observation happens to be unusual on all variables in precisely the same way, it can actually lie very close to the regression line. An example of this is shown in panel A of <a class="reference internal" href="#fig-leverage-influence"><span class="std std-numref">Fig. 16.6</span></a>. The leverage of an observation is operationalised in terms of its <em>hat value</em>, usually written <span class="math notranslate nohighlight">\(h_i\)</span>. The formula for the hat value is rather complicated<a class="footnote-reference brackets" href="#notehatmatrix" id="id11" role="doc-noteref"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></a> but its interpretation is not: <span class="math notranslate nohighlight">\(h_i\)</span> is a measure of the extent to which the <span class="math notranslate nohighlight">\(i\)</span>-th observation is “in control” of where the regression line ends up going. We won’t bother extracting the hat values here, but if you want to do this, you can use the <code class="docutils literal notranslate"><span class="pre">get_influence</span></code> method from the <code class="docutils literal notranslate"><span class="pre">statsmodels.api</span></code> package to inspect the relative influence of data points. For now, it is enough to get an intuitive, visual idea of what leverage can mean.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">curve_fit</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">string</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">3.6</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
     <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">1.7</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">7.8</span><span class="p">],</span>
     <span class="s1">&#39;y2&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">1.7</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">7.4</span><span class="p">]</span>
    <span class="p">})</span>



<span class="c1"># fit linear regression model and save parameters</span>
<span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span>

<span class="n">initialParameters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>

<span class="n">fittedParameters</span><span class="p">,</span> <span class="n">pcov</span> <span class="o">=</span> <span class="n">curve_fit</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">initialParameters</span><span class="p">)</span>

<span class="n">modelPredictions</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="o">*</span><span class="n">fittedParameters</span><span class="p">)</span> 

<span class="n">xModel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]),</span> <span class="nb">max</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]))</span>
<span class="n">yModel</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">xModel</span><span class="p">,</span> <span class="o">*</span><span class="n">fittedParameters</span><span class="p">)</span>



<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># add regression line</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xModel</span><span class="p">,</span> <span class="n">yModel</span><span class="p">)</span>


<span class="n">initialParameters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>

<span class="n">fittedParameters</span><span class="p">,</span> <span class="n">pcov</span> <span class="o">=</span> <span class="n">curve_fit</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;y2&#39;</span><span class="p">],</span> <span class="n">initialParameters</span><span class="p">)</span>

<span class="n">modelPredictions</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="o">*</span><span class="n">fittedParameters</span><span class="p">)</span> 

<span class="n">xModel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]),</span> <span class="nb">max</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]))</span>
<span class="n">yModel</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">xModel</span><span class="p">,</span> <span class="o">*</span><span class="n">fittedParameters</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xModel</span><span class="p">,</span> <span class="n">yModel</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mf">7.4</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="p">[</span><span class="mf">7.4</span> <span class="p">,</span><span class="mf">7.6</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Plot 2</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">3.6</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
     <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">1.7</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">7.8</span><span class="p">],</span>
     <span class="s1">&#39;y2&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">1.7</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
    <span class="p">})</span>



<span class="c1"># fit linear regression model and save parameters</span>
<span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span>

<span class="n">initialParameters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>

<span class="n">fittedParameters</span><span class="p">,</span> <span class="n">pcov</span> <span class="o">=</span> <span class="n">curve_fit</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">initialParameters</span><span class="p">)</span>

<span class="n">modelPredictions</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="o">*</span><span class="n">fittedParameters</span><span class="p">)</span> 

<span class="n">xModel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]),</span> <span class="nb">max</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]))</span>
<span class="n">yModel</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">xModel</span><span class="p">,</span> <span class="o">*</span><span class="n">fittedParameters</span><span class="p">)</span>




<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># add regression line</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xModel</span><span class="p">,</span> <span class="n">yModel</span><span class="p">)</span>


<span class="n">initialParameters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>

<span class="n">fittedParameters</span><span class="p">,</span> <span class="n">pcov</span> <span class="o">=</span> <span class="n">curve_fit</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;y2&#39;</span><span class="p">],</span> <span class="n">initialParameters</span><span class="p">)</span>

<span class="n">modelPredictions</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="o">*</span><span class="n">fittedParameters</span><span class="p">)</span> 

<span class="n">xModel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]),</span> <span class="nb">max</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]))</span>
<span class="n">yModel</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">xModel</span><span class="p">,</span> <span class="o">*</span><span class="n">fittedParameters</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xModel</span><span class="p">,</span> <span class="n">yModel</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span> <span class="p">,</span><span class="mf">7.3</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>


<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">):</span>   
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="n">string</span><span class="o">.</span><span class="n">ascii_uppercase</span><span class="p">[</span><span class="n">n</span><span class="p">],</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> 
            <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>


<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;leverage-influence-fig&quot;</span><span class="p">,</span> <span class="n">fig</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<figure class="align-default" id="fig-leverage-influence" style="width: 600px">
<img alt="_images/f1763980cc3094bffa46642ba11fc653ff8e3c379fd0230b4dbbd3d9c1188329.png" src="_images/f1763980cc3094bffa46642ba11fc653ff8e3c379fd0230b4dbbd3d9c1188329.png" />
<figcaption>
<p><span class="caption-number">Fig. 16.6 </span><span class="caption-text">Outliers showing high leverage points (panel A) and high influence points (panel B).</span><a class="headerlink" href="#fig-leverage-influence" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>In general, if an observation lies far away from the other ones in terms of the predictor variables, it will have a large hat value (as a rough guide, high leverage is when the hat value is more than 2-3 times the average; and note that the sum of the hat values is constrained to be equal to <span class="math notranslate nohighlight">\(K+1\)</span>). High leverage points are also worth looking at in more detail, but they’re much less likely to be a cause for concern unless they are also outliers.</p>
<p>This brings us to our third measure of unusualness, the <strong><em>influence</em></strong> of an observation. A high influence observation is an outlier that has high leverage. That is, it is an observation that is very different to all the other ones in some respect, and also lies a long way from the regression line. This is illustrated in <a class="reference internal" href="#fig-leverage-influence"><span class="std std-numref">Fig. 16.6</span></a>, panel B. Notice the contrast to panel A, and to <a class="reference internal" href="#fig-outlier"><span class="std std-numref">Fig. 16.5</span></a>: outliers don’t move the regression line much, and neither do high leverage points. But something that is an outlier and has high leverage… that has a big effect on the regression line.</p>
<p>That’s why we call these points high influence; and it’s why they’re the biggest worry. We operationalise influence in terms of a measure known as <strong><em>Cook’s distance</em></strong>,</p>
<div class="math notranslate nohighlight">
\[
D_i = \frac{{\epsilon_i^*}^2 }{K+1} \times \frac{h_i}{1-h_i}
\]</div>
<p>Notice that this is a multiplication of something that measures the outlier-ness of the observation (the bit on the
left), and something that measures the leverage of the observation (the bit on the right). In other words, in order to have a large Cook’s distance, an observation must be a fairly substantial outlier <em>and</em> have high leverage.</p>
<p>Again, if you want to quantify Cook’s distance, this can be done using using <code class="docutils literal notranslate"><span class="pre">statsmodels.api</span></code>. I won’t go through this in detail, but if you are interested, you can click to show the code, and see how I got the Cook’s distance values from <code class="docutils literal notranslate"><span class="pre">statmodels</span></code>.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>

<span class="c1"># Define a figure with two panels</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Define our made-up data</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">3.6</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
     <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">1.7</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">7.8</span><span class="p">],</span>
     <span class="s1">&#39;y2&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">1.7</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
    <span class="p">})</span>

<span class="c1"># Get Cook&#39;s distance</span>

<span class="c1"># model the data using statsmodels.api OLS (ordinary least squares)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;y2&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># extract cook&#39;s distance</span>
<span class="n">influence</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">get_influence</span><span class="p">()</span>
<span class="n">cooks</span> <span class="o">=</span> <span class="n">influence</span><span class="o">.</span><span class="n">cooks_distance</span>

<span class="c1"># for plotting, make a dataframe with the x data, and the corresponding cook&#39;s distances</span>
<span class="n">df_cooks</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span>
     <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">cooks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="p">})</span>

<span class="c1"># Panel A</span>

<span class="c1"># plot Cook&#39;s distance against the x data points</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">df_cooks</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span> <span class="mi">4</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>



<span class="c1"># Panel B (same as before)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">3.6</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
     <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">1.7</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">7.8</span><span class="p">],</span>
     <span class="s1">&#39;y2&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">1.7</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
    <span class="p">})</span>



<span class="c1"># fit linear regression model and save parameters</span>
<span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span>


<span class="c1"># first regression line</span>
<span class="n">initialParameters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>

<span class="n">fittedParameters</span><span class="p">,</span> <span class="n">pcov</span> <span class="o">=</span> <span class="n">curve_fit</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">initialParameters</span><span class="p">)</span>

<span class="n">modelPredictions</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="o">*</span><span class="n">fittedParameters</span><span class="p">)</span> 

<span class="n">xModel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]),</span> <span class="nb">max</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]))</span>
<span class="n">yModel</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">xModel</span><span class="p">,</span> <span class="o">*</span><span class="n">fittedParameters</span><span class="p">)</span>


<span class="c1"># plot data points</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># plot first regression line</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xModel</span><span class="p">,</span> <span class="n">yModel</span><span class="p">)</span>


<span class="c1"># second regression line</span>

<span class="n">initialParameters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>

<span class="n">fittedParameters</span><span class="p">,</span> <span class="n">pcov</span> <span class="o">=</span> <span class="n">curve_fit</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;y2&#39;</span><span class="p">],</span> <span class="n">initialParameters</span><span class="p">)</span>

<span class="n">modelPredictions</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="o">*</span><span class="n">fittedParameters</span><span class="p">)</span> 

<span class="n">xModel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]),</span> <span class="nb">max</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]))</span>
<span class="n">yModel</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">xModel</span><span class="p">,</span> <span class="o">*</span><span class="n">fittedParameters</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xModel</span><span class="p">,</span> <span class="n">yModel</span><span class="p">)</span>

<span class="c1"># add red point to show &quot;outlier&quot;</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">)</span>

<span class="c1"># add dashed line to show change in residual</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span> <span class="p">,</span><span class="mf">7.3</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># add letter labels</span>
<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">):</span>   
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="n">string</span><span class="o">.</span><span class="n">ascii_uppercase</span><span class="p">[</span><span class="n">n</span><span class="p">],</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> 
            <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="c1"># prettify</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
<span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">:</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;cooks-fig&quot;</span><span class="p">,</span> <span class="n">fig</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<figure class="align-default" id="fig-cooks" style="width: 600px">
<img alt="_images/aef1a65df4182c071ea648d0ba2ef69a7c2e459a6e7f5b33581a7288a11bc489.png" src="_images/aef1a65df4182c071ea648d0ba2ef69a7c2e459a6e7f5b33581a7288a11bc489.png" />
<figcaption>
<p><span class="caption-number">Fig. 16.7 </span><span class="caption-text">A visualization of Cook’s distance as a means for identifying the influence of each data point. Panel B is the same as panel B in <a class="reference internal" href="#fig-leverage-influence"><span class="std std-numref">Fig. 16.6</span></a> above. The red dot show the effect on the entire regression line that sinking the final point from 7.4 to 5 has. Panel A shows Cook’s distance for the data set in which the data point at x = 8 is 5 (the orange line). The dashed lines show two possible cutoff points for a “large” Cook’s distance: either 1 or 4/N, in which N is the number of data points.</span><a class="headerlink" href="#fig-cooks" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>As a rough guide, Cook’s distance greater than 1 is often considered large (that’s what I typically use as a quick and dirty rule), though a quick scan of the internet and a few papers suggests that <span class="math notranslate nohighlight">\(4/N\)</span> has also been suggested as a possible rule of thumb. As the visualization in <a class="reference internal" href="#fig-cooks"><span class="std std-numref">Fig. 16.7</span></a> illustrates, the red data point at x = 8 has a large Cook’s distance.</p>
<p>An obvious question to ask next is, if you do have large values of Cook’s distance, what should you do? As always, there’s no hard and fast rules. Probably the first thing to do is to try running the regression with that point excluded and see what happens to the model performance and to the regression coefficients. If they really are substantially different, it’s time to start digging into your data set and your notes that you no doubt were scribbling as your ran your study; try to figure out <em>why</em> the point is so different. If you start to become convinced that this one data point is badly distorting your results, you might consider excluding it, but that’s less than ideal unless you have a solid explanation for why this particular case is qualitatively different from the others and therefore deserves to be handled separately.<a class="footnote-reference brackets" href="#noterobust" id="id12" role="doc-noteref"><span class="fn-bracket">[</span>12<span class="fn-bracket">]</span></a> To give an example, let’s delete the observation from day 64, the observation with the largest Cook’s distance for the <code class="docutils literal notranslate"><span class="pre">mod2</span></code> model, where we predicted my grumpiness on the basis of my sleep, and my baby’s sleep.</p>
<p>First, let’s get back to the original sleep data, and remind ourselves of what our model coefficients looked like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">pingouin</span> <span class="k">as</span> <span class="nn">pg</span>

<span class="n">file</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/ethanweed/pythonbook/main/Data/parenthood.csv&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>

<span class="n">predictors</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> <span class="s1">&#39;baby_sleep&#39;</span><span class="p">]]</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">]</span>

<span class="n">mod2</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">predictors</span><span class="p">,</span> <span class="n">outcome</span><span class="p">)</span>
<span class="n">mod2</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>coef</th>
      <th>se</th>
      <th>T</th>
      <th>pval</th>
      <th>r2</th>
      <th>adj_r2</th>
      <th>CI[2.5%]</th>
      <th>CI[97.5%]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Intercept</td>
      <td>125.97</td>
      <td>3.04</td>
      <td>41.42</td>
      <td>0.00</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>119.93</td>
      <td>132.00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>dan_sleep</td>
      <td>-8.95</td>
      <td>0.55</td>
      <td>-16.17</td>
      <td>0.00</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>-10.05</td>
      <td>-7.85</td>
    </tr>
    <tr>
      <th>2</th>
      <td>baby_sleep</td>
      <td>0.01</td>
      <td>0.27</td>
      <td>0.04</td>
      <td>0.97</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>-0.53</td>
      <td>0.55</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Then, we can remove the data from day 64, using the <code class="docutils literal notranslate"><span class="pre">drop()</span></code> method from <code class="docutils literal notranslate"><span class="pre">pandas</span></code>. Remember, as always, it’s Python, and Python is zero-indexed, so it starts counting the days at 0 and not 1, and that means that day 64 is on row 63!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="mi">63</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictors</span> <span class="o">=</span> <span class="n">df_2</span><span class="p">[[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> <span class="s1">&#39;baby_sleep&#39;</span><span class="p">]]</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">df_2</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">]</span>

<span class="n">mod3</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">predictors</span><span class="p">,</span> <span class="n">outcome</span><span class="p">)</span>
<span class="n">mod3</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>coef</th>
      <th>se</th>
      <th>T</th>
      <th>pval</th>
      <th>r2</th>
      <th>adj_r2</th>
      <th>CI[2.5%]</th>
      <th>CI[97.5%]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Intercept</td>
      <td>126.36</td>
      <td>3.00</td>
      <td>42.19</td>
      <td>0.00</td>
      <td>0.82</td>
      <td>0.82</td>
      <td>120.41</td>
      <td>132.30</td>
    </tr>
    <tr>
      <th>1</th>
      <td>dan_sleep</td>
      <td>-8.83</td>
      <td>0.55</td>
      <td>-16.13</td>
      <td>0.00</td>
      <td>0.82</td>
      <td>0.82</td>
      <td>-9.91</td>
      <td>-7.74</td>
    </tr>
    <tr>
      <th>2</th>
      <td>baby_sleep</td>
      <td>-0.13</td>
      <td>0.28</td>
      <td>-0.48</td>
      <td>0.63</td>
      <td>0.82</td>
      <td>0.82</td>
      <td>-0.68</td>
      <td>0.41</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>As you can see, those regression coefficients have barely changed in comparison to the values we got earlier. In other words, we really don’t have any problem as far as anomalous data are concerned.</p>
</section>
<section id="checking-the-normality-of-the-residuals">
<span id="regressionnormality"></span><h3><span class="section-number">16.9.3. </span>Checking the normality of the residuals<a class="headerlink" href="#checking-the-normality-of-the-residuals" title="Permalink to this heading">#</a></h3>
<p>Like many of the statistical tools we’ve discussed in this book, regression models rely on a normality assumption. In this case, we assume that the residuals are normally distributed. The tools for testing this aren’t fundamentally different to those that we discussed <a class="reference internal" href="05.02-ttest.html#shapiro"><span class="std std-ref">earlier</span></a>. First, I firmly believe that it never hurts to draw a good old-fashioned histogram:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">pingouin</span> <span class="k">as</span> <span class="nn">pg</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">predictors</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> <span class="s1">&#39;baby_sleep&#39;</span><span class="p">]]</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">]</span>

<span class="n">mod2</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">predictors</span><span class="p">,</span> <span class="n">outcome</span><span class="p">)</span>

<span class="n">res</span> <span class="o">=</span> <span class="n">mod2</span><span class="o">.</span><span class="n">residuals_</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Residuals&#39;</span><span class="p">])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">res</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;Residuals&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;res-hist-fig&quot;</span><span class="p">,</span> <span class="n">fig</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<figure class="align-default" id="fig-res-hist" style="width: 600px">
<img alt="_images/8b1fde83725cf6783d07ae986d2721ad104bbea591d13e2100427a2b951215f2.png" src="_images/8b1fde83725cf6783d07ae986d2721ad104bbea591d13e2100427a2b951215f2.png" />
<figcaption>
<p><span class="caption-number">Fig. 16.8 </span><span class="caption-text">A histogram of the (ordinary) residuals in the <code class="docutils literal notranslate"><span class="pre">mod2</span></code> model. These residuals look very close to being normally distributed, much moreso than is typically seen with real data. This shouldn’t surprise you… they aren’t real data, and they aren’t real residuals!</span><a class="headerlink" href="#fig-res-hist" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The resulting plot is shown in <a class="reference internal" href="#fig-res-hist"><span class="std std-numref">Fig. 16.8</span></a>, and as you can see the plot looks pretty damn close to normal, almost unnaturally so. I could also run a Shapiro-Wilk test to check, using the <code class="docutils literal notranslate"><span class="pre">normality</span></code> function from <code class="docutils literal notranslate"><span class="pre">pingouin</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sw</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">normality</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">method</span> <span class="o">=</span> <span class="s1">&#39;shapiro&#39;</span><span class="p">)</span>
<span class="n">sw</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>W</th>
      <th>pval</th>
      <th>normal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Residuals</th>
      <td>0.99</td>
      <td>0.84</td>
      <td>True</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The W value of .99, at this sample size, is non-significant (<span class="math notranslate nohighlight">\(p\)</span> = .82), again suggesting that the normality assumption isn’t in any danger here. As a third measure, we might also want to draw a QQ-plot. This can be most easily done using the <code class="docutils literal notranslate"><span class="pre">qqplot</span></code> function from <code class="docutils literal notranslate"><span class="pre">statsmodels.api</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>


<span class="n">sm</span><span class="o">.</span><span class="n">qqplot</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s1">&#39;Residuals&#39;</span><span class="p">],</span> <span class="n">line</span> <span class="o">=</span> <span class="s1">&#39;s&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/937db0b7e053a6a967fa9e3b9d3a3142b66ce50db2c67751189eaf3d9444370b.png" src="_images/937db0b7e053a6a967fa9e3b9d3a3142b66ce50db2c67751189eaf3d9444370b.png" />
</div>
</div>
<p>If our residuals were <em>perfectly</em> normally distributed, then they would lie right on the red line. These residuals veer off a little at the ends, but again, pretty damn close, so the Q-Q plot further confirms our conviction based on the histogram and the Shapiro-Wilk test, that it is appropriate to model these data with a linear regression.</p>
<p>A little note: Q-Q plots are often created by plotting the quantiles of the <em>standardized</em> residuals against the theoretical quantiles. Since using Q-Q plots for assessing normality is basically a matter of squinting at the plot at getting a <em>feeling</em>, based on your great experience of squinting at plots, for whether the data <em>seem</em> normal enough, it probably doesn’t matter so much which way you do it. Below you can see a side-by-side comparison of a Q-Q plot of the ordinary (left) and standardized (right) residuals. Honestly, they look about the same to me, and you can get the ordinary residuals straight from <code class="docutils literal notranslate"><span class="pre">pingouin</span></code>, without needing to invoke <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a figure with two panels</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">res_standard</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">res_standard</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Standardized_Residuals&#39;</span><span class="p">])</span>

<span class="n">sm</span><span class="o">.</span><span class="n">qqplot</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s1">&#39;Residuals&#39;</span><span class="p">],</span> <span class="n">line</span> <span class="o">=</span> <span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">sm</span><span class="o">.</span><span class="n">qqplot</span><span class="p">(</span><span class="n">res_standard</span><span class="p">[</span><span class="s1">&#39;Standardized_Residuals&#39;</span><span class="p">],</span> <span class="n">line</span> <span class="o">=</span> <span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Ordinary residuals&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Standardized residuals&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">:</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="_images/94521a9d1cf2e98eaab53037a37c3a9bd785ac79713076fbbd1426a2eaf18e44.png" src="_images/94521a9d1cf2e98eaab53037a37c3a9bd785ac79713076fbbd1426a2eaf18e44.png" />
</div>
</div>
</section>
<section id="checking-the-linearity-of-the-relationship">
<span id="regressionlinearity"></span><h3><span class="section-number">16.9.4. </span>Checking the linearity of the relationship<a class="headerlink" href="#checking-the-linearity-of-the-relationship" title="Permalink to this heading">#</a></h3>
<p>The third thing we might want to test is the linearity of the relationships between the predictors and
the outcomes. There’s a few different things that you might want to do in order to check this. Firstly, it
never hurts to just plot the relationship between the fitted values <span class="math notranslate nohighlight">\(\hat{Y}_i\)</span> and the observed values <span class="math notranslate nohighlight">\(Y_i\)</span> for the
outcome variable, as illustrated in <a class="reference internal" href="#fig-fitted-v-observed"><span class="std std-numref">Fig. 16.9</span></a>.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictors</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> <span class="s1">&#39;baby_sleep&#39;</span><span class="p">]]</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">]</span>

<span class="n">mod2</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">predictors</span><span class="p">,</span> <span class="n">outcome</span><span class="p">,</span> <span class="n">as_dataframe</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>

<span class="n">df_res_pred</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span><span class="s1">&#39;observed&#39;</span><span class="p">:</span> <span class="n">outcome</span><span class="p">,</span>
     <span class="s1">&#39;fitted&#39;</span><span class="p">:</span> <span class="n">mod2</span><span class="p">[</span><span class="s1">&#39;pred&#39;</span><span class="p">]</span>
    <span class="p">})</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>

<span class="n">ax</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">df_res_pred</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;fitted&#39;</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;observed&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;fitted_v_observed-fig&quot;</span><span class="p">,</span> <span class="n">fig</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<figure class="align-default" id="fig-fitted-v-observed" style="width: 600px">
<img alt="_images/50957c1def83f45abed63f5d9f1fb067fd7ba1ca14cd6bd755fe4e62b96792a4.png" src="_images/50957c1def83f45abed63f5d9f1fb067fd7ba1ca14cd6bd755fe4e62b96792a4.png" />
<figcaption>
<p><span class="caption-number">Fig. 16.9 </span><span class="caption-text">Plot of the fitted values against the observed values of the outcome variable. A straight line is what we’re hoping to see here. This looks pretty good, suggesting that there’s nothing grossly wrong, but there could be hidden subtle issues.</span><a class="headerlink" href="#fig-fitted-v-observed" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>One of the reasons I like to draw these plots is that they give you a kind of “big picture view”. If this plot looks approximately linear, then we’re probably not doing too badly (though that’s not to say that there aren’t problems). However, if you can see big departures from linearity here, then it strongly suggests that you need to make some changes.</p>
<p>In any case, in order to get a more detailed picture it’s often more informative to look at the relationship between the fitted values and the residuals themselves. Let’s take a look. Here, we use the <code class="docutils literal notranslate"><span class="pre">residplot</span></code> function from <code class="docutils literal notranslate"><span class="pre">seaborn</span></code>, and in addition to drawing a straight line through all the points, we can ask Python to compute a “locally-weighed linear linear” regression, by setting the argument <code class="docutils literal notranslate"><span class="pre">lowess</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code>. This gives us a sort of moving window, as Python fits a series of little regressions as it moves through the data. With a little work, we can also use <code class="docutils literal notranslate"><span class="pre">regplot</span></code> to produce the same figure for the multiple linear regression model with both <code class="docutils literal notranslate"><span class="pre">dan_sleep</span></code> and <code class="docutils literal notranslate"><span class="pre">baby_sleep</span></code> as predictors (click the show button to see how). If the relationship in our data is truly linear, then we should see a straight, perfectly horizontal line. There’s some hint of curvature here, but it’s not clear whether or not we be concerned.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a figure with two panels</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Fit linear regressions for both predictors, and plot residuals</span>

<span class="n">p1</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">residplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">,</span> <span class="n">lowess</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">line_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">p2</span><span class="o">=</span><span class="n">p3</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">residplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;baby_sleep&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">,</span> <span class="n">lowess</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">line_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>


<span class="c1"># Add panel for multiple linear regression</span>

<span class="n">df_res_pred</span><span class="p">[</span><span class="s1">&#39;residuals&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mod2</span><span class="p">[</span><span class="s1">&#39;residuals&#39;</span><span class="p">]</span>


<span class="n">p3</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">df_res_pred</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;fitted&#39;</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;residuals&#39;</span><span class="p">,</span> <span class="n">lowess</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">line_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dotted&#39;</span><span class="p">)</span>

<span class="c1"># Make figures look nicer</span>
<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="n">string</span><span class="o">.</span><span class="n">ascii_uppercase</span><span class="p">[</span><span class="n">n</span><span class="p">],</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> 
            <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>

<span class="n">subplots</span><span class="o">=</span><span class="p">[</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">,</span><span class="n">p3</span><span class="p">]</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">subplots</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelbottom</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="c1"># Plot figure in book, with caption</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;resid1-fig&quot;</span><span class="p">,</span> <span class="n">fig</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<figure class="align-default" id="fig-resid1" style="width: 600px">
<img alt="_images/701b4ed9d3dfdd9bb80fe655560c5720cea415ad2103023dce12fb422d8d4bca.png" src="_images/701b4ed9d3dfdd9bb80fe655560c5720cea415ad2103023dce12fb422d8d4bca.png" />
<figcaption>
<p><span class="caption-number">Fig. 16.10 </span><span class="caption-text">Scatterplots of the residuals when a linear regression is fit separately for either <code class="docutils literal notranslate"><span class="pre">dan_sleep</span></code> as a predictor (panel A) or <code class="docutils literal notranslate"><span class="pre">baby_sleep</span></code> as a predictor (panel B). Panel C plots the residuals for the overall fit of the model including both predictors. The dotted horizontal line represents the fitted regression model. If a linear model is appropriate for our data, then the points should lie evenly above and below the dotted line, the whole away along the x-axis. The red line shows a locally-weighted smoothing of the regressionn line. Too much curvature in this line would suggest that maybe a straight line is not the best model for our data. But how much curvature is too much?</span><a class="headerlink" href="#fig-resid1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>One method for testing the degree of curvature is known as Tukey’s test for nonadditivity {cite:<code class="docutils literal notranslate"><span class="pre">tukey1949one,</span> <span class="pre">fox2018r</span></code>}. The test is quite simple: you just take the fitted (predicted) values from your original model, square them, and then include these squared fitted values as a predictor when you re-run the model. If the <span class="math notranslate nohighlight">\(t\)</span>-test for the new predictor comes up significant, this implies that there is some nonlinear relationship between the variable and the residuals. Let’s give this a try:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># square the fitted values for our model, and add them to our dataframe</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Tukey&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mod2</span><span class="p">[</span><span class="s1">&#39;pred&#39;</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>

<span class="c1">#re-run the model, with squared fitted values added as predictor (here I have called them &quot;Tukey&quot;)</span>
<span class="n">mod_curve_check</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> <span class="s1">&#39;baby_sleep&#39;</span><span class="p">,</span> <span class="s1">&#39;Tukey&#39;</span><span class="p">]],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">])</span>
<span class="n">mod_curve_check</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>coef</th>
      <th>se</th>
      <th>T</th>
      <th>pval</th>
      <th>r2</th>
      <th>adj_r2</th>
      <th>CI[2.5%]</th>
      <th>CI[97.5%]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Intercept</td>
      <td>1.540</td>
      <td>57.642</td>
      <td>0.027</td>
      <td>0.979</td>
      <td>0.825</td>
      <td>0.819</td>
      <td>-112.878</td>
      <td>115.958</td>
    </tr>
    <tr>
      <th>1</th>
      <td>dan_sleep</td>
      <td>2.867</td>
      <td>5.494</td>
      <td>0.522</td>
      <td>0.603</td>
      <td>0.825</td>
      <td>0.819</td>
      <td>-8.039</td>
      <td>13.773</td>
    </tr>
    <tr>
      <th>2</th>
      <td>baby_sleep</td>
      <td>0.011</td>
      <td>0.266</td>
      <td>0.043</td>
      <td>0.966</td>
      <td>0.825</td>
      <td>0.819</td>
      <td>-0.517</td>
      <td>0.540</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Tukey</td>
      <td>0.010</td>
      <td>0.005</td>
      <td>2.162</td>
      <td>0.033</td>
      <td>0.825</td>
      <td>0.819</td>
      <td>0.001</td>
      <td>0.020</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Since the <span class="math notranslate nohighlight">\(t\)</span>-test on the coefficients for new <code class="docutils literal notranslate"><span class="pre">Tukey</span></code> predictor is significant (<span class="math notranslate nohighlight">\(p\)</span> &lt; 0.05), this suggests that the curvature we see in <a class="reference internal" href="#fig-resid1"><span class="std std-numref">Fig. 16.10</span></a> is genuine, although it still bears remembering that the pattern in <a class="reference internal" href="#fig-fitted-v-observed"><span class="std std-numref">Fig. 16.9</span></a> is pretty damn straight: in other words the deviations from linearity are fairly small, and probably not worth worrying about.</p>
<p>In a lot of cases, the solution to this problem (and many others) is to transform one or more of the variables. We have already discussed the <a class="reference internal" href="03.03-pragmatic_matters.html#transform"><span class="std std-ref">basics of variable transformation</span></a>, but I do want to make special note of one additional possibility that I didn’t mention earlier: the Box-Cox transform. The Box-Cox function is a fairly simple one, but it’s very widely used</p>
<div class="math notranslate nohighlight">
\[
f(x,\lambda) = \frac{x^\lambda - 1}{\lambda}
\]</div>
<p>for all values of <span class="math notranslate nohighlight">\(\lambda\)</span> except <span class="math notranslate nohighlight">\(\lambda = 0\)</span>. When <span class="math notranslate nohighlight">\(\lambda = 0\)</span> we just take the natural logarithm (i.e., <span class="math notranslate nohighlight">\(\ln(x)\)</span>). You can calculate it using the <code class="docutils literal notranslate"><span class="pre">boxcox</span></code> function from <code class="docutils literal notranslate"><span class="pre">scipy.stats</span></code>, which will automatically calculate the optimal value of <span class="math notranslate nohighlight">\(\lambda\)</span>. In the case of our <code class="docutils literal notranslate"><span class="pre">dan_sleep</span></code> data though, the data were already pretty normally distributed (panel A), so applying the transformation (panel B) really doesn’t do much. Now, if only the <em>quality</em> of my sleep could be transformed, and not just the <em>distribution</em>, ah, then we would really be on to something!</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">boxcox</span>

<span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_sleep_transformed&#39;</span><span class="p">],</span> <span class="n">best_lambda</span> <span class="o">=</span> <span class="n">boxcox</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">])</span>

<span class="c1"># Define a figure with two panels</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Fit linear regressions for both predictors, and plot residuals</span>

<span class="n">p1</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">p2</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_sleep_transformed&#39;</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># Make figures look nicer</span>
<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="n">string</span><span class="o">.</span><span class="n">ascii_uppercase</span><span class="p">[</span><span class="n">n</span><span class="p">],</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> 
            <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>

<span class="n">subplots</span><span class="o">=</span><span class="p">[</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">,</span><span class="n">p3</span><span class="p">]</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">subplots</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelbottom</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Optimal value for lambda:&#39;</span><span class="p">,</span> <span class="n">best_lambda</span><span class="p">)</span>


<span class="c1"># Plot figure in book, with caption</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;boxcox-fig&quot;</span><span class="p">,</span> <span class="n">fig</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimal value for lambda: 1.7912772737277842
</pre></div>
</div>
</div>
</div>
<figure class="align-default" id="fig-boxcox" style="width: 600px">
<img alt="_images/249c2abbda236849b4b8d1ef54d5880ac813adafe4accdb06f0e11cda521ceab.png" src="_images/249c2abbda236849b4b8d1ef54d5880ac813adafe4accdb06f0e11cda521ceab.png" />
<figcaption>
<p><span class="caption-number">Fig. 16.11 </span><span class="caption-text">Distribution for the raw <code class="docutils literal notranslate"><span class="pre">dan_sleep</span></code> data (panel A) and the distribution same data after a Box-Cox transformation (panel B). In this case, the transformation has not had much of an effect, as the data were reasonably normally-distributed to begin with.</span><a class="headerlink" href="#fig-boxcox" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="checking-the-homogeneity-of-variance">
<span id="regressionhomogeneity"></span><h3><span class="section-number">16.9.5. </span>Checking the homogeneity of variance<a class="headerlink" href="#checking-the-homogeneity-of-variance" title="Permalink to this heading">#</a></h3>
<p>The regression models that we’ve talked about all make a homogeneity of variance assumption: the variance of the residuals is assumed to be constant. One way to inspect this visually is to use a “Scale-Location Plot”, which is just a variation on the plots we made in <a class="reference internal" href="#fig-resid1"><span class="std std-numref">Fig. 16.10</span></a>. In a Scale-Location Plot, instead of plotting the fitted values against the raw residuals, we plot them against the square root of the absolute value of the standardized residuals, which can make it easier to visually assess the homogeneity of the variance.</p>
<p>As far as I know (and admittedly, that’s not so far), there is no ready-made package in Python that will produce a Scale-Location Plot, but if you have made it this far in the book, you already have the knowledge to build one of these yourself. It could go something like this:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictors</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> <span class="s1">&#39;baby_sleep&#39;</span><span class="p">]]</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">]</span>

<span class="n">mod2</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">predictors</span><span class="p">,</span> <span class="n">outcome</span><span class="p">,</span> <span class="n">as_dataframe</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>


<span class="c1"># Thanks to Ivan Savov (https://github.com/ivanistheone) for contributing this solution for getting the standardized residuals</span>
<span class="n">SS_resid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">mod2</span><span class="p">[</span><span class="s2">&quot;residuals&quot;</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">mod2</span><span class="p">[</span><span class="s1">&#39;residuals&#39;</span><span class="p">])</span>
<span class="n">p</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">predictors</span><span class="p">))</span>
<span class="n">sigmahat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span> <span class="n">SS_resid</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="n">p</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">))</span>
<span class="n">stand_res</span> <span class="o">=</span> <span class="n">mod2</span><span class="p">[</span><span class="s1">&#39;residuals&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">sigmahat</span>
<span class="n">df_slplot</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span><span class="s1">&#39;fitted&#39;</span><span class="p">:</span> <span class="n">mod2</span><span class="p">[</span><span class="s1">&#39;pred&#39;</span><span class="p">],</span>
     <span class="s1">&#39;sqrt_abs_stand_res&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">stand_res</span><span class="p">))</span>
    <span class="p">})</span>


<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>

<span class="n">ax</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">df_slplot</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;fitted&#39;</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;sqrt_abs_stand_res&#39;</span><span class="p">,</span> <span class="n">lowess</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">line_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">))</span>

<span class="c1"># Make figures look nicer</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\sqrt{|standardized residuals|}$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Fitted Values&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelbottom</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>



<span class="c1"># Plot figure in book, with caption</span>

<span class="c1">#glue(&quot;sl-plot-fig&quot;, fig, display=False)</span>
<span class="c1">#plt.close(fig)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="_images/e75a129aca481a4438eb01b826064a6e616131da31107f00143325d380a62677.png" src="_images/e75a129aca481a4438eb01b826064a6e616131da31107f00143325d380a62677.png" />
</div>
</div>
<p>A slightly more formal approach is to run a hypothesis test such as the Breusch–Pagan test <span id="id13">[<a class="reference internal" href="bibliography.html#id85" title="Trevor S Breusch and Adrian R Pagan. A simple test for heteroscedasticity and random coefficient variation. Econometrica: Journal of the econometric society, pages 1287–1294, 1979.">Breusch and Pagan, 1979</a>]</span>. Unfortunately, to run the test, we’ll have to leave the cozy world of <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> and use <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> instead, but luckily the code is not complicated. Basically, we just run our regression in <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> instead of <code class="docutils literal notranslate"><span class="pre">pinguoin</span></code>, and then run the Breusch-Pagan test on the output (see below). I won’t go into how it works, other than to say that after we fit our regression model, we then fit <em>another</em> regression model in which we use our predicted values to predict our residuals. We then use something called the “Lagrange multiplier statistic” (similar to a <span class="math notranslate nohighlight">\(x^2\)</span> test) to check the significance of our new model. If the new model is not significant, this can be used to support the assumption that we are indeed dealing with a relationship that can be modelled linearly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>
<span class="kn">import</span> <span class="nn">statsmodels.stats.api</span> <span class="k">as</span> <span class="nn">sms</span>

<span class="c1"># fit our regression model using statsmodels</span>
<span class="n">fit</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;dan_grump ~ dan_sleep+baby_sleep&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="n">bptest</span><span class="o">=</span><span class="n">sms</span><span class="o">.</span><span class="n">diagnostic</span><span class="o">.</span><span class="n">het_breuschpagan</span><span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">resid</span><span class="p">,</span> <span class="n">fit</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">exog</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Lagrange multiplier statistic:&#39;</span><span class="p">,</span> <span class="n">bptest</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;p:&#39;</span><span class="p">,</span> <span class="n">bptest</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Lagrange multiplier statistic: 0.4886831614554388
p: 0.7832200556890354
</pre></div>
</div>
</div>
</div>
<p>We see that our original impression was right: there’s no violations of homogeneity of variance in this data.</p>
<p>It’s a bit beyond the scope of this chapter to talk too much about how to deal with violations of homogeneity of variance, but I’ll give you a quick sense of what you need to consider. The main thing to worry about, if homogeneity of variance is violated, is that the standard error estimates associated with the regression coefficients are no longer entirely reliable, and so your <span class="math notranslate nohighlight">\(t\)</span>-tests for the coefficients aren’t quite right either. A simple fix to the problem is to make use of a “heteroscedasticity corrected covariance matrix” when estimating the standard errors.</p>
<p>Again, this goes beyond what we can do with <code class="docutils literal notranslate"><span class="pre">pingouin</span></code>, but <code class="docutils literal notranslate"><span class="pre">statmodels</span></code> can get us there. Before I show you how, let’s just re-assure ourselves that both <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> and <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> are doing the same basic calculations when we fit our regression models. We’ll start by re-running our mulitiple regression with <code class="docutils literal notranslate"><span class="pre">pingouin</span></code>, but I’ll bump up the number of decimal places that we round our answer to, so that it is easier to compare with the <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> output:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictors</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> <span class="s1">&#39;baby_sleep&#39;</span><span class="p">]]</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">]</span>

<span class="n">mod2</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">predictors</span><span class="p">,</span> <span class="n">outcome</span><span class="p">)</span>
<span class="n">mod2</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>coef</th>
      <th>se</th>
      <th>T</th>
      <th>pval</th>
      <th>r2</th>
      <th>adj_r2</th>
      <th>CI[2.5%]</th>
      <th>CI[97.5%]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Intercept</td>
      <td>125.9656</td>
      <td>3.0409</td>
      <td>41.4231</td>
      <td>0.0000</td>
      <td>0.8161</td>
      <td>0.8123</td>
      <td>119.9301</td>
      <td>132.0010</td>
    </tr>
    <tr>
      <th>1</th>
      <td>dan_sleep</td>
      <td>-8.9502</td>
      <td>0.5535</td>
      <td>-16.1715</td>
      <td>0.0000</td>
      <td>0.8161</td>
      <td>0.8123</td>
      <td>-10.0487</td>
      <td>-7.8518</td>
    </tr>
    <tr>
      <th>2</th>
      <td>baby_sleep</td>
      <td>0.0105</td>
      <td>0.2711</td>
      <td>0.0388</td>
      <td>0.9691</td>
      <td>0.8161</td>
      <td>0.8123</td>
      <td>-0.5275</td>
      <td>0.5485</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Next, we’ll fit the same model, but using <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> instead:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;dan_grump ~ dan_sleep+baby_sleep&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>        <td>dan_grump</td>    <th>  R-squared:         </th> <td>   0.816</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.812</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   215.2</td>
</tr>
<tr>
  <th>Date:</th>             <td>Mon, 03 Feb 2025</td> <th>  Prob (F-statistic):</th> <td>2.15e-36</td>
</tr>
<tr>
  <th>Time:</th>                 <td>14:59:42</td>     <th>  Log-Likelihood:    </th> <td> -287.48</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   581.0</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    97</td>      <th>  BIC:               </th> <td>   588.8</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>  <td>  125.9656</td> <td>    3.041</td> <td>   41.423</td> <td> 0.000</td> <td>  119.930</td> <td>  132.001</td>
</tr>
<tr>
  <th>dan_sleep</th>  <td>   -8.9502</td> <td>    0.553</td> <td>  -16.172</td> <td> 0.000</td> <td>  -10.049</td> <td>   -7.852</td>
</tr>
<tr>
  <th>baby_sleep</th> <td>    0.0105</td> <td>    0.271</td> <td>    0.039</td> <td> 0.969</td> <td>   -0.527</td> <td>    0.549</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 0.593</td> <th>  Durbin-Watson:     </th> <td>   2.120</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.743</td> <th>  Jarque-Bera (JB):  </th> <td>   0.218</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.053</td> <th>  Prob(JB):          </th> <td>   0.897</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 3.203</td> <th>  Cond. No.          </th> <td>    76.9</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</div></div>
</div>
<p>Of course, <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> gives us way more information than <code class="docutils literal notranslate"><span class="pre">pingouin</span></code>, but if you hunt around in the output for the key things we are used to talking about (the coefficients for <code class="docutils literal notranslate"><span class="pre">dan_sleep</span></code> and <code class="docutils literal notranslate"><span class="pre">baby_sleep</span></code>, the t-values, the <span class="math notranslate nohighlight">\(R^2\)</span> and adjusted <span class="math notranslate nohighlight">\(R^2\)</span>, etc.) it’s all pretty much the same, right?</p>
<p>Good. Now, we’ll re-fit our regression model, but ask <code class="docutils literal notranslate"><span class="pre">statmodels</span></code> to take the potential non-homogeneity (aka the heteroskedasticity) of variance in our model into account by specifying a <code class="docutils literal notranslate"><span class="pre">cov_type</span></code>, that is, the type of “robust sandwich estimator” we want to use. Yes. It really is called a “robust sandwich estimator”. I’m not even kidding.<a class="footnote-reference brackets" href="#sandwich" id="id14" role="doc-noteref"><span class="fn-bracket">[</span>13<span class="fn-bracket">]</span></a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;dan_grump ~ dan_sleep+baby_sleep&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">cov_type</span><span class="o">=</span><span class="s1">&#39;HC1&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>        <td>dan_grump</td>    <th>  R-squared:         </th> <td>   0.816</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.812</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   201.7</td>
</tr>
<tr>
  <th>Date:</th>             <td>Mon, 03 Feb 2025</td> <th>  Prob (F-statistic):</th> <td>2.78e-35</td>
</tr>
<tr>
  <th>Time:</th>                 <td>14:59:42</td>     <th>  Log-Likelihood:    </th> <td> -287.48</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   581.0</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    97</td>      <th>  BIC:               </th> <td>   588.8</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>         <td>HC1</td>       <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
       <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>  <td>  125.9656</td> <td>    3.149</td> <td>   39.996</td> <td> 0.000</td> <td>  119.793</td> <td>  132.138</td>
</tr>
<tr>
  <th>dan_sleep</th>  <td>   -8.9502</td> <td>    0.597</td> <td>  -14.987</td> <td> 0.000</td> <td>  -10.121</td> <td>   -7.780</td>
</tr>
<tr>
  <th>baby_sleep</th> <td>    0.0105</td> <td>    0.282</td> <td>    0.037</td> <td> 0.970</td> <td>   -0.541</td> <td>    0.563</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 0.593</td> <th>  Durbin-Watson:     </th> <td>   2.120</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.743</td> <th>  Jarque-Bera (JB):  </th> <td>   0.218</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.053</td> <th>  Prob(JB):          </th> <td>   0.897</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 3.203</td> <th>  Cond. No.          </th> <td>    76.9</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors are heteroscedasticity robust (HC1)</div></div>
</div>
<p>Not surprisingly, the <span class="math notranslate nohighlight">\(z\)</span>-values are pretty similar to the <span class="math notranslate nohighlight">\(t\)</span>-values that we saw when we fit our basic, non-robust regression, because the homogeneity of variance assumption wasn’t violated in our data anyway. But if it had been, we might have seen some more substantial differences.</p>
</section>
<section id="checking-for-collinearity">
<span id="regressioncollinearity"></span><h3><span class="section-number">16.9.6. </span>Checking for collinearity<a class="headerlink" href="#checking-for-collinearity" title="Permalink to this heading">#</a></h3>
<p>The last kind of regression diagnostic that I’m going to discuss in this chapter is the use of <strong><em>variance inflation factors</em></strong> (VIFs), which are useful for determining whether or not the predictors in your regression model are too highly correlated with each other. There is a variance inflation factor associated with each predictor <span class="math notranslate nohighlight">\(X_k\)</span> in the model, and the formula for the <span class="math notranslate nohighlight">\(k\)</span>-th VIF is:</p>
<div class="math notranslate nohighlight">
\[
\text{VIF}_k = \frac{1}{1-{R^2_{(-k)}}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(R^2_{(-k)}\)</span> refers to <span class="math notranslate nohighlight">\(R\)</span>-squared value you would get if you ran a regression using <span class="math notranslate nohighlight">\(X_k\)</span> as the outcome variable, and all the other <span class="math notranslate nohighlight">\(X\)</span> variables as the predictors. The idea here is that <span class="math notranslate nohighlight">\(R^2_{(-k)}\)</span> is a very good measure of the extent to which <span class="math notranslate nohighlight">\(X_k\)</span> is correlated with all the other variables in the model. Better yet, the square root of the VIF is pretty interpretable: it tells you how much wider the confidence interval for the corresponding coefficient <span class="math notranslate nohighlight">\(b_k\)</span> is, relative to what you would have expected if the predictors are all nice and uncorrelated with one another. If you’ve only got two predictors, the VIF values are always going to be the same, as we can see if we use the <code class="docutils literal notranslate"><span class="pre">variance_inflation_factor</span></code> function from <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>.</p>
<p>First, we need a dataframe with our predictor variables as columns, plus a column with a constant value representing the model intercept:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">matrix</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Intercept&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">]),</span>
                       <span class="s1">&#39;dan_sleep&#39;</span><span class="p">:</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">],</span> 
                       <span class="s1">&#39;baby_sleep&#39;</span><span class="p">:</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;baby_sleep&#39;</span><span class="p">]})</span>
<span class="n">matrix</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Intercept</th>
      <th>dan_sleep</th>
      <th>baby_sleep</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>7.59</td>
      <td>10.18</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>7.91</td>
      <td>11.66</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>5.14</td>
      <td>7.92</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>7.71</td>
      <td>9.61</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>6.68</td>
      <td>9.75</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Then, we can calculate the VIF for each predictor and put the results in a dataframe:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">statsmodels.stats.outliers_influence</span> <span class="kn">import</span> <span class="n">variance_inflation_factor</span>

<span class="n">vif</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">vif</span><span class="p">[</span><span class="s1">&#39;VIF&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">variance_inflation_factor</span><span class="p">(</span><span class="n">matrix</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
<span class="n">vif</span><span class="p">[</span><span class="s1">&#39;variable&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">matrix</span><span class="o">.</span><span class="n">columns</span>
<span class="n">vif</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>VIF</th>
      <th>variable</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>48.784569</td>
      <td>Intercept</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.651038</td>
      <td>dan_sleep</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.651038</td>
      <td>baby_sleep</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>And since the square root of 1.65 is 1.28, which isn’t really a huge change in the confidence intervals for our predictors, we see that the correlation between our two predictors isn’t causing much of a problem.</p>
<p>To give a sense of how we could end up with a model that has bigger collinearity problems, suppose I were to run a much less interesting regression model, in which I tried to predict the day on which the data were collected, as a function of all the other variables in the data set. To see why this would be a bit of a problem, let’s have a look at the correlation matrix for all four variables:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>dan_sleep</th>
      <th>baby_sleep</th>
      <th>dan_grump</th>
      <th>day</th>
      <th>Tukey</th>
      <th>dan_sleep_transformed</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>dan_sleep</th>
      <td>1.000000</td>
      <td>0.627949</td>
      <td>-0.903384</td>
      <td>-0.098408</td>
      <td>-0.997024</td>
      <td>0.997866</td>
    </tr>
    <tr>
      <th>baby_sleep</th>
      <td>0.627949</td>
      <td>1.000000</td>
      <td>-0.565964</td>
      <td>-0.010434</td>
      <td>-0.626177</td>
      <td>0.625397</td>
    </tr>
    <tr>
      <th>dan_grump</th>
      <td>-0.903384</td>
      <td>-0.565964</td>
      <td>1.000000</td>
      <td>0.076479</td>
      <td>0.907817</td>
      <td>-0.895394</td>
    </tr>
    <tr>
      <th>day</th>
      <td>-0.098408</td>
      <td>-0.010434</td>
      <td>0.076479</td>
      <td>1.000000</td>
      <td>0.103952</td>
      <td>-0.093393</td>
    </tr>
    <tr>
      <th>Tukey</th>
      <td>-0.997024</td>
      <td>-0.626177</td>
      <td>0.907817</td>
      <td>0.103952</td>
      <td>1.000000</td>
      <td>-0.989864</td>
    </tr>
    <tr>
      <th>dan_sleep_transformed</th>
      <td>0.997866</td>
      <td>0.625397</td>
      <td>-0.895394</td>
      <td>-0.093393</td>
      <td>-0.989864</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We have some fairly large correlations between some of our predictor variables! When we run the regression model and look at the VIF values, we see that the collinearity is causing a lot of uncertainty about the coefficients. First, we’ll add the <code class="docutils literal notranslate"><span class="pre">day</span></code> column and the <code class="docutils literal notranslate"><span class="pre">dan_grump</span></code> column to our <code class="docutils literal notranslate"><span class="pre">matrix</span></code> dataframe…</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">matrix</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">]</span>
<span class="n">matrix</span><span class="p">[</span><span class="s1">&#39;day&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;day&#39;</span><span class="p">]</span>


<span class="n">matrix</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Intercept</th>
      <th>dan_sleep</th>
      <th>baby_sleep</th>
      <th>dan_grump</th>
      <th>day</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>7.59</td>
      <td>10.18</td>
      <td>56</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>7.91</td>
      <td>11.66</td>
      <td>60</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>5.14</td>
      <td>7.92</td>
      <td>82</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>7.71</td>
      <td>9.61</td>
      <td>55</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>6.68</td>
      <td>9.75</td>
      <td>67</td>
      <td>5</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>and then, we’ll look at the VIFs…</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vif</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">vif</span><span class="p">[</span><span class="s1">&#39;VIF&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">variance_inflation_factor</span><span class="p">(</span><span class="n">matrix</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
<span class="n">vif</span><span class="p">[</span><span class="s1">&#39;variable&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">matrix</span><span class="o">.</span><span class="n">columns</span>
<span class="n">vif</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>VIF</th>
      <th>variable</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>922.960321</td>
      <td>Intercept</td>
    </tr>
    <tr>
      <th>1</th>
      <td>6.148528</td>
      <td>dan_sleep</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.658389</td>
      <td>baby_sleep</td>
    </tr>
    <tr>
      <th>3</th>
      <td>5.442617</td>
      <td>dan_grump</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.015119</td>
      <td>day</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Yep, that’s some mighty fine collinearity you’ve got there. Of course, if you suspect that your model might have some degree of collinearity between predictors, you are still left with some questions to which there are no concrete answers (sorry!). How much collinearity is ok? What should I do about the collinearity in my model? There are some suggestions and guidelines out there (e.g., in the <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> documentation, a vif of 5 is suggested to be the limit, but <a class="reference external" href="https://en.wikipedia.org/wiki/Variance_inflation_factor">elsewhere</a> you can find other suggested values as cutoff lines), but like so many things in statistics (and in life), I’m afraid you will need to exercise your own judgment.</p>
</section>
</section>
<section id="model-selection">
<span id="modelselreg"></span><h2><span class="section-number">16.10. </span>Model Selection<a class="headerlink" href="#model-selection" title="Permalink to this heading">#</a></h2>
<p>One fairly major problem that remains is the problem of “model selection”. That is, if we have a data set that contains several variables, which ones should we include as predictors, and which ones should we not include? In other words, we have a problem of <strong><em>variable selection</em></strong>. In general, model selection is a complex business, but it’s made somewhat simpler if we restrict ourselves to the problem of choosing a subset of the variables that ought to be included in the model. Nevertheless, I’m not going to try covering even this reduced topic in a lot of detail. Instead, I’ll talk about two broad principles that you need to think about; and then discuss one concrete tool to help you select a subset of variables to include in your model. Firstly, the two principles:</p>
<ul class="simple">
<li><p>It’s nice to have an actual substantive basis for your choices. That is, in a lot of situations you the researcher have good reasons to pick out a smallish number of possible regression models that are of theoretical interest; these models will have a sensible interpretation in the context of your field. Never discount the importance of this. Statistics serves the scientific process, not the other way around.</p></li>
<li><p>To the extent that your choices rely on statistical inference, there is a trade off between simplicity and goodness of fit. As you add more predictors to the model, you make it more complex; each predictor adds a new free parameter (i.e., a new regression coefficient), and each new parameter increases the model’s capacity to “absorb” random variations. So the goodness of fit (e.g., <span class="math notranslate nohighlight">\(R^2\)</span>) continues to rise as you add more predictors no matter what. If you want your model to be able to generalise well to new observations, you need to avoid throwing in too many variables.</p></li>
</ul>
<p>This latter principle is often referred to as <strong><em>Ockham’s razor</em></strong>, and is often summarised in terms of the following pithy saying: <em>do not multiply entities beyond necessity</em>. In this context, it means: don’t chuck in a bunch of largely irrelevant predictors just to boost your <span class="math notranslate nohighlight">\(R^2\)</span>. Hm. Yeah, the original was better.</p>
<p>In any case, what we need is an actual mathematical criterion that will implement the qualitative principle behind Ockham’s razor in the context of selecting a regression model. As it turns out there are several possibilities. The one that I’ll talk about is the <strong><em>Akaike information criterion</em></strong> <span id="id15">[<a class="reference internal" href="bibliography.html#id43" title="H. Akaike. A new look at the statistical model identification. IEEE Transactions on Automatic Control, 19:716–723, 1974.">Akaike, 1974</a>]</span> simply because it’s a common one, and because <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> will calculate it for you automatically when you run a linear regression There are many others, including BIC (Bayesian Information Criterion), which <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> also gives you for free. In the context of a linear regression model (and ignoring terms that don’t depend on the model in any way!), the AIC for a model that has <span class="math notranslate nohighlight">\(K\)</span> predictor variables plus an intercept is:</p>
<div class="math notranslate nohighlight">
\[
\text{AIC} = \displaystyle\frac{\text{SS}_{res}}{\hat{\sigma}}^2+ 2K
\]</div>
<p>The smaller the AIC value, the better the model performance is.<a class="footnote-reference brackets" href="#aic-calc" id="id16" role="doc-noteref"><span class="fn-bracket">[</span>14<span class="fn-bracket">]</span></a> If we ignore the low level details, it’s fairly obvious what the AIC does: on the left we have a term that increases as the model predictions get worse; on the right we have a term that increases as the model complexity increases. The best model is the one that fits the data well (low residuals; left hand side) using as few predictors as possible (low <span class="math notranslate nohighlight">\(K\)</span>; right hand side). In short, this is a simple implementation of Ockham’s razor.</p>
<section id="backward-elimination">
<h3><span class="section-number">16.10.1. </span>Backward elimination<a class="headerlink" href="#backward-elimination" title="Permalink to this heading">#</a></h3>
<p>Okay, let’s have a look at how this works in practice. In this example I’ll keep it simple and use only the basic backward elimination approach. That is, start with the complete regression model, including all possible predictors. Then, at each “step” we try all possible ways of removing one of the variables, and whichever of these is best (in terms of lowest AIC value) is accepted. This becomes our new regression model; and we then try all possible deletions from the new model, again choosing the option with lowest AIC. This process continues until we end up with a model that has a lower AIC value than any of the other possible models that you could produce by deleting one of its predictors. Let’s see this in action. First, I need to define the model from which the process starts.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mod_full</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;dan_grump ~ dan_sleep + baby_sleep + day&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can get <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> to fit the model and return a summay, which will include the AIC value.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mod_full</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>        <td>dan_grump</td>    <th>  R-squared:         </th> <td>   0.816</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.811</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   142.2</td>
</tr>
<tr>
  <th>Date:</th>             <td>Mon, 03 Feb 2025</td> <th>  Prob (F-statistic):</th> <td>3.42e-35</td>
</tr>
<tr>
  <th>Time:</th>                 <td>14:59:42</td>     <th>  Log-Likelihood:    </th> <td> -287.43</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   582.9</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    96</td>      <th>  BIC:               </th> <td>   593.3</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>  <td>  126.2787</td> <td>    3.242</td> <td>   38.945</td> <td> 0.000</td> <td>  119.842</td> <td>  132.715</td>
</tr>
<tr>
  <th>dan_sleep</th>  <td>   -8.9693</td> <td>    0.560</td> <td>  -16.016</td> <td> 0.000</td> <td>  -10.081</td> <td>   -7.858</td>
</tr>
<tr>
  <th>baby_sleep</th> <td>    0.0157</td> <td>    0.273</td> <td>    0.058</td> <td> 0.954</td> <td>   -0.526</td> <td>    0.558</td>
</tr>
<tr>
  <th>day</th>        <td>   -0.0044</td> <td>    0.015</td> <td>   -0.288</td> <td> 0.774</td> <td>   -0.035</td> <td>    0.026</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 0.599</td> <th>  Durbin-Watson:     </th> <td>   2.120</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.741</td> <th>  Jarque-Bera (JB):  </th> <td>   0.215</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.037</td> <th>  Prob(JB):          </th> <td>   0.898</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 3.215</td> <th>  Cond. No.          </th> <td>    441.</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</div></div>
</div>
<p>This is great, but we will be doing this several times, and this is quite a lot of information to wade through just to get AIC value. Luckily, we can also ask for <em>only</em> the AIC value, like so:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mod_full</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">aic</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>582.864617028592
</pre></div>
</div>
</div>
</div>
<p>Now that we know how to extract only the AIC value for each model, it’s not hard to loop through all the possible models, each one removing one predictor from the full model, and compare the AIC values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mod_full</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;dan_grump ~ dan_sleep + baby_sleep + day&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span>
<span class="n">mod_no_day</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;dan_grump ~ dan_sleep + baby_sleep&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span>
<span class="n">mod_no_baby</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;dan_grump ~ dan_sleep + day&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span>
<span class="n">mod_no_dan</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;dan_grump ~ baby_sleep + day&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span>

<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">mod_full</span><span class="p">,</span> <span class="n">mod_no_day</span><span class="p">,</span> <span class="n">mod_no_baby</span><span class="p">,</span> <span class="n">mod_no_dan</span><span class="p">]</span>
<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Full Model&#39;</span><span class="p">,</span> <span class="s1">&#39;No Day&#39;</span><span class="p">,</span> <span class="s1">&#39;No Baby&#39;</span><span class="p">,</span> <span class="s1">&#39;No Dan&#39;</span><span class="p">]</span>

<span class="n">aics</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">models</span><span class="p">):</span>
    <span class="n">aics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">models</span><span class="p">[</span><span class="n">n</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">aic</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="n">df_aics</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Model&#39;</span><span class="p">:</span> <span class="n">names</span><span class="p">,</span> <span class="s1">&#39;AIC&#39;</span><span class="p">:</span> <span class="n">aics</span><span class="p">})</span>
<span class="n">df_aics</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Model</th>
      <th>AIC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Full Model</td>
      <td>582.86</td>
    </tr>
    <tr>
      <th>1</th>
      <td>No Day</td>
      <td>580.95</td>
    </tr>
    <tr>
      <th>2</th>
      <td>No Baby</td>
      <td>580.87</td>
    </tr>
    <tr>
      <th>3</th>
      <td>No Dan</td>
      <td>710.94</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Of these models, the one in which we removed <code class="docutils literal notranslate"><span class="pre">baby_sleep</span></code> as a predictor has the lowest AIC, so it seems like removing this predictor is probably a good idea. This shouldn’t be surprising, really, given all the other checks we have already done on these data. But let’s proceed. Now, we can do the same thing again, but comparing only models without <code class="docutils literal notranslate"><span class="pre">baby_sleep</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mod_full</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;dan_grump ~ dan_sleep + day&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span>
<span class="n">mod_no_day</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;dan_grump ~ dan_sleep&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span>
<span class="n">mod_no_dan</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;dan_grump ~ day&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span>

<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">mod_full</span><span class="p">,</span> <span class="n">mod_no_day</span><span class="p">,</span> <span class="n">mod_no_dan</span><span class="p">]</span>
<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Full Model&#39;</span><span class="p">,</span> <span class="s1">&#39;No Day&#39;</span><span class="p">,</span> <span class="s1">&#39;No Dan&#39;</span><span class="p">]</span>

<span class="n">aics</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">models</span><span class="p">):</span>
    <span class="n">aics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">get_aic</span><span class="p">(</span><span class="n">names</span><span class="p">[</span><span class="n">n</span><span class="p">],</span> <span class="n">model</span><span class="p">))</span>

<span class="n">aics</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">63</span><span class="p">],</span> <span class="n">line</span> <span class="mi">10</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span> <span class="n">aics</span> <span class="o">=</span> <span class="p">[]</span>
<span class="g g-Whitespace">      </span><span class="mi">9</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">models</span><span class="p">):</span>
<span class="ne">---&gt; </span><span class="mi">10</span>     <span class="n">aics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">get_aic</span><span class="p">(</span><span class="n">names</span><span class="p">[</span><span class="n">n</span><span class="p">],</span> <span class="n">model</span><span class="p">))</span>
<span class="g g-Whitespace">     </span><span class="mi">12</span> <span class="n">aics</span>

<span class="ne">NameError</span>: name &#39;get_aic&#39; is not defined
</pre></div>
</div>
</div>
</div>
<p>Now, the model in which we removed <code class="docutils literal notranslate"><span class="pre">day</span></code> as a predictor performs best (has the lowest AIC), so the next step would be to remove <code class="docutils literal notranslate"><span class="pre">day</span></code> from our model. In this case, that only leaves us with one predictor: <code class="docutils literal notranslate"><span class="pre">dan_sleep</span></code>. Which is (perhaps not all that surprisingly) the mod1 that we started with at the <a class="reference internal" href="#pingouinregression"><span class="std std-ref">beginning of the chapter</span></a>:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>mod1 = pg.linear_regression(df[&#39;dan_sleep&#39;], df[&#39;dan_grump&#39;])
</pre></div>
</div>
<p>This process, as I have described it, is fairly straightforward, especially once you have a function to just grab the AIC from the <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> output. But if you find yourself doing this sort of thing often, you might want to automate it even further. It wouldn’t take <em>that</em> much work to expand on the code I have given you, and build a function that takes as input your full model, and then <em>automatically</em> considers all possible variations, and continues eliminating predictors until it finds the optimal model. But this I leave as a programming exercise for you, if you feel so inclined.</p>
</section>
<section id="a-caveat">
<h3><span class="section-number">16.10.2. </span>A caveat<a class="headerlink" href="#a-caveat" title="Permalink to this heading">#</a></h3>
<p>Automated variable selection methods are seductive things. They provide an element of objectivity to your model selection, and that’s kind of nice. Unfortunately, they’re sometimes used as an excuse for thoughtlessness. No longer do you have to think carefully about which predictors to add to the model and what the theoretical basis for their inclusion might be… everything is solved by the magic of AIC. And if we start throwing around phrases like Ockham’s razor, well, it sounds like everything is wrapped up in a nice neat little package that no-one can argue with.</p>
<p>Or, perhaps not. First of all, there’s very little agreement on what counts as an appropriate model selection criterion. When I was taught backward elimination as an undergraduate, we used F-tests to do it, because that was the default method used by the software. Here we are using AIC, and since this is an introductory text that’s the only method I’ve described, but the AIC is hardly the Word of the Gods of Statistics. It’s an approximation, derived under certain assumptions, and it’s guaranteed to work only for large samples when those assumptions are met. Alter those assumptions and you get a different criterion, like the BIC for instance. Take a different approach again and you get the NML criterion. Decide that you’re a Bayesian and you get model selection based on posterior odds ratios. Then there are a bunch of regression-specific tools that I haven’t mentioned. And so on. All of these different methods have strengths and weaknesses, and some are easier to calculate than others (AIC is probably the easiest of the lot, which might account for its popularity). Almost all of them produce the same answers when the answer is “obvious”, but there’s a fair amount of disagreement when the model selection problem becomes hard.</p>
<p>What does this mean in practice? Well, you could go and spend several years teaching yourself the theory of model selection, learning all the ins and outs of it; so that you could finally decide on what you personally think the right thing to do is. Speaking as someone who actually did that, I wouldn’t recommend it: you’ll probably come out the other side even more confused than when you started. A better strategy is to show a bit of common sense… if you’re staring at the results of a stepwise AIC model comparison procedure, and the model that makes sense is close to having the smallest AIC, but is narrowly defeated by a model that doesn’t make any sense… trust your instincts. Statistical model selection is an inexact tool, and as I said at the beginning, interpretability matters.</p>
</section>
<section id="comparing-two-regression-models">
<h3><span class="section-number">16.10.3. </span>Comparing two regression models<a class="headerlink" href="#comparing-two-regression-models" title="Permalink to this heading">#</a></h3>
<p>An alternative to using automated model selection procedures is for the researcher to explicitly select two or more regression models to compare to each other. You can do this in a few different ways, depending on what research question you’re trying to answer. Suppose we want to know whether or not the amount of sleep that my son got has any relationship to my grumpiness, over and above what we might expect from the amount of sleep that I got. We also want to make sure that the day on which we took the measurement has no influence on the relationship. That is, we’re interested in the relationship between <code class="docutils literal notranslate"><span class="pre">baby_sleep</span></code> and <code class="docutils literal notranslate"><span class="pre">dan_grump</span></code>, and from that perspective <code class="docutils literal notranslate"><span class="pre">dan_sleep</span></code> and <code class="docutils literal notranslate"><span class="pre">day</span></code> are nuisance variables or <strong><em>covariates</em></strong> that we want to control for. In this situation, what we would like to know is whether <code class="docutils literal notranslate"><span class="pre">dan_grump</span> <span class="pre">~</span> <span class="pre">dan_sleep</span> <span class="pre">+</span> <span class="pre">day</span> <span class="pre">+</span> <span class="pre">baby_sleep</span></code> (which I’ll call Model 1, or <code class="docutils literal notranslate"><span class="pre">M1</span></code>) is a better regression model for these data than <code class="docutils literal notranslate"><span class="pre">dan_grump</span> <span class="pre">~</span> <span class="pre">dan_sleep</span> <span class="pre">+</span> <span class="pre">day</span></code> (which I’ll call Model 0, or <code class="docutils literal notranslate"><span class="pre">M0</span></code>). There are two different ways we can compare these two models, one based on a model selection criterion like AIC, and the other based on an explicit hypothesis test. I’ll show you the AIC-based approach first because it’s simpler, and follows naturally from the method we used in the last section. The first thing I need to do is actually run the regressions. Since we want to calculate AIC, it will be easier to use <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> than <code class="docutils literal notranslate"><span class="pre">pigouin</span></code>. First we’ll define the two models, then use our handy-dandy AIC function to get the AIC for each of them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">M0</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;dan_grump ~ dan_sleep + day&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span>
<span class="n">M1</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;dan_grump ~ dan_sleep + day + baby_sleep&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span>

<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">M0</span><span class="p">,</span> <span class="n">M1</span><span class="p">]</span>
<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;M0&#39;</span><span class="p">,</span> <span class="s1">&#39;M1&#39;</span><span class="p">]</span>

<span class="n">aics</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">models</span><span class="p">):</span>
    <span class="n">aics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">get_aic</span><span class="p">(</span><span class="n">names</span><span class="p">[</span><span class="n">n</span><span class="p">],</span> <span class="n">model</span><span class="p">))</span>

<span class="n">aics</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;M0 AIC: 580.9&#39;, &#39;M1 AIC: 582.9&#39;]
</pre></div>
</div>
</div>
</div>
<p>Since Model 0 has the smaller AIC value, it is judged to be the better model for these data.</p>
<p>By the way, I mentioned before that <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> gives us both AIC and BIC automatically. In fact, while I’m not particularly impressed with either AIC or BIC as model selection methods, if you do find yourself using one of these two, the empirical evidence suggests that BIC is the better criterion of the two. In most simulation studies that I’ve seen, BIC does a much better job of selecting the correct model. And indeed, it wouldn’t be hard to modify our function <code class="docutils literal notranslate"><span class="pre">get_aic</span></code> from above to get both the AIC and BIC values from the <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> output, so why not just do that, like so <a class="footnote-reference brackets" href="#exercise" id="id17" role="doc-noteref"><span class="fn-bracket">[</span>15<span class="fn-bracket">]</span></a>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_aic_bic</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
    <span class="n">aic</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">as_text</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)[</span><span class="mi">7</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;AIC:&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
    <span class="n">bic</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">as_text</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)[</span><span class="mi">8</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;BIC:&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model_name</span> <span class="o">+</span> <span class="s1">&#39; AIC: &#39;</span> <span class="o">+</span> <span class="n">aic</span> <span class="o">+</span> <span class="s1">&#39; BIC: &#39;</span> <span class="o">+</span> <span class="n">bic</span>
    <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">M0</span><span class="p">,</span> <span class="n">M1</span><span class="p">]</span>
<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;M0&#39;</span><span class="p">,</span> <span class="s1">&#39;M1&#39;</span><span class="p">]</span>

<span class="n">ics</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">models</span><span class="p">):</span>
    <span class="n">ics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">get_aic_bic</span><span class="p">(</span><span class="n">names</span><span class="p">[</span><span class="n">n</span><span class="p">],</span> <span class="n">model</span><span class="p">))</span>

<span class="n">ics</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;M0 AIC: 580.9 BIC: 588.7&#39;, &#39;M1 AIC: 582.9 BIC: 593.3&#39;]
</pre></div>
</div>
</div>
</div>
<p>BIC is also smaller for <code class="docutils literal notranslate"><span class="pre">MO</span></code> than for <code class="docutils literal notranslate"><span class="pre">M1</span></code>, so based on both AIC and BIC, it looks like Model 0 is the better choice.</p>
<p>A somewhat different approach to the problem comes out of the hypothesis testing framework. Suppose you have two regression models, where one of them (Model 0) contains a <em>subset</em> of the predictors from the other one (Model 1). That is, Model 1 contains all of the predictors included in Model 0, plus one or more additional predictors. When this happens we say that Model 0 is <strong><em>nested</em></strong> within Model 1, or possibly that Model 0 is a <strong><em>submodel</em></strong> of Model 1. Regardless of the terminology, what this means is that we can think of Model 0 as a null hypothesis and Model 1 as an alternative hypothesis. And in fact we can construct an <span class="math notranslate nohighlight">\(F\)</span> test for this in a fairly straightforward fashion. We can fit both models to the data and obtain a residual sum of squares for both models. I’ll denote these as SS<span class="math notranslate nohighlight">\(_{res}^{(0)}\)</span> and SS<span class="math notranslate nohighlight">\(_{res}^{(1)}\)</span> respectively. The superscripting here just indicates which model we’re talking about.  Then our <span class="math notranslate nohighlight">\(F\)</span> statistic is</p>
<div class="math notranslate nohighlight">
\[
F = \frac{(\text{SS}_{res}^{(0)} - \text{SS}_{res}^{(1)})/k}{(\text{SS}_{res}^{(1)})/(N-p-1)}
\]</div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is the number of observations, <span class="math notranslate nohighlight">\(p\)</span> is the number of predictors in the full model (not including the intercept), and <span class="math notranslate nohighlight">\(k\)</span> is the difference in the number of parameters between the two models.^[It’s worth noting in passing that this same <span class="math notranslate nohighlight">\(F\)</span> statistic can be used to test a much broader range of hypotheses than those that I’m mentioning here. Very briefly: notice that the nested model M0 corresponds to the full model M1 when we constrain some of the regression coefficients to zero. It is sometimes useful to construct submodels by placing other kinds of constraints on the regression coefficients. For instance, maybe two different coefficients might have to sum to zero, or something like that. You can construct hypothesis tests for those kind of constraints too, but it is somewhat more complicated and the sampling distribution for <span class="math notranslate nohighlight">\(F\)</span> can end up being something known as the non-central <span class="math notranslate nohighlight">\(F\)</span> distribution, which is waaaaay beyond the scope of this book! All I want to do is alert you to this possibility.] The degrees of freedom here are <span class="math notranslate nohighlight">\(k\)</span> and <span class="math notranslate nohighlight">\(N-p-1\)</span>. Note that it’s often more convenient to think about the difference between those two SS values as a sum of squares in its own right. That is:</p>
<div class="math notranslate nohighlight">
\[
\text{SS}_\Delta = \text{SS}_{res}^{(0)} - \text{SS}_{res}^{(1)}
\]</div>
<p>The reason why this his helpful is that we can express <span class="math notranslate nohighlight">\(\text{SS}_\Delta\)</span> a measure of the extent to which the two models make different predictions about the the outcome variable. Specifically:
$<span class="math notranslate nohighlight">\(
\text{SS}_\Delta  = \sum_{i} \left( \hat{y}_i^{(1)} - \hat{y}_i^{(0)} \right)^2
\)</span><span class="math notranslate nohighlight">\(
where \)</span>\hat{y}_i^{(0)}<span class="math notranslate nohighlight">\( is the fitted value for \)</span>y_i<span class="math notranslate nohighlight">\( according to model \)</span>M_0<span class="math notranslate nohighlight">\( and  \)</span>\hat{y}_i^{(1)}<span class="math notranslate nohighlight">\( is the is the fitted value for \)</span>y_i<span class="math notranslate nohighlight">\( according to model \)</span>M_1$.</p>
<p>Okay, so that’s the hypothesis test that we use to compare two regression models to one another. Now, how do we do it in Python? As it turns out, <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> has a function called <code class="docutils literal notranslate"><span class="pre">anova_lm</span></code> that will do just this. All we have to do is fit the two models that we want to compare, and then plug them in the function (null model first):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">statsmodels.stats.anova</span> <span class="kn">import</span> <span class="n">anova_lm</span>

<span class="n">M0</span> <span class="o">=</span> <span class="n">M0</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">M1</span> <span class="o">=</span> <span class="n">M1</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="n">anova_lm</span><span class="p">(</span><span class="n">M0</span><span class="p">,</span><span class="n">M1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>df_resid</th>
      <th>ssr</th>
      <th>df_diff</th>
      <th>ss_diff</th>
      <th>F</th>
      <th>Pr(&gt;F)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>97.0</td>
      <td>1837.155916</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>96.0</td>
      <td>1837.092228</td>
      <td>1.0</td>
      <td>0.063688</td>
      <td>0.003328</td>
      <td>0.954116</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Since we have <span class="math notranslate nohighlight">\(p&gt;.05\)</span> we retain the null hypothesis (<code class="docutils literal notranslate"><span class="pre">M0</span></code>).  This approach to regression, in which we add all of our covariates into a null model, and then <em>add</em> the variables of interest into an alternative model, and then compare the two models in hypothesis testing framework, is often referred to as <strong><em>hierarchical regression</em></strong>.</p>
</section>
</section>
<section id="summary">
<h2><span class="section-number">16.11. </span>Summary<a class="headerlink" href="#summary" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Basic ideas in <a class="reference internal" href="#regression"><span class="std std-ref">linear regression</span></a> and how regression models are <a class="reference internal" href="#regressionestimation"><span class="std std-ref">estimated</span></a>.</p></li>
<li><p><a class="reference internal" href="#multipleregression"><span class="std std-ref">Multiple linear regression</span></a>.</p></li>
<li><p>Measuring the <a class="reference internal" href="#r2"><span class="std std-ref">overall performance</span></a> of a regression model using <span class="math notranslate nohighlight">\(R^2\)</span></p></li>
<li><p><a class="reference internal" href="#regressiontests"><span class="std std-ref">Hypothesis tests for regression models</span></a></p></li>
<li><p><a class="reference internal" href="#regressioncoefs"><span class="std std-ref">Calculating confidence intervals </span></a> for regression coefficients, and standardised coefficients</p></li>
<li><p>The <a class="reference internal" href="#regressionassumptions"><span class="std std-ref">assumptions of regression</span></a> and <a class="reference internal" href="#regressiondiagnostics"><span class="std std-ref">how to check them</span></a></p></li>
<li><p><a class="reference internal" href="#modelselreg"><span class="std std-ref">Selecting a regression model</span></a></p></li>
</ul>
<hr class="footnotes docutils" />
<aside class="footnote brackets" id="americanhighschool" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Translator’s note: and when <em>I</em> went to high school in the United States ca. <a class="reference external" href="https://en.wikipedia.org/wiki/1990">1990</a> <a class="reference external" href="https://en.wikipedia.org/wiki/Common_Era">CE</a>  we learned this as <span class="math notranslate nohighlight">\(y=ax+b\)</span>. Why? Who decides whether it is <span class="math notranslate nohighlight">\(a\)</span> or <span class="math notranslate nohighlight">\(m\)</span>? <span class="math notranslate nohighlight">\(c\)</span> or <span class="math notranslate nohighlight">\(b\)</span>? I truly have no idea, but either way, it represents the same idea.</p>
</aside>
<aside class="footnote brackets" id="noteepsilon" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>The <span class="math notranslate nohighlight">\(\epsilon\)</span> symbol is the Greek letter epsilon. It’s traditional to use <span class="math notranslate nohighlight">\(\epsilon_i\)</span> or <span class="math notranslate nohighlight">\(e_i\)</span> to denote a residual.</p>
</aside>
<aside class="footnote brackets" id="notekungfu" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">3</a><span class="fn-bracket">]</span></span>
<p>Or at least, I’m assuming that it doesn’t help most people. But on the off chance that someone reading this is a proper kung fu master of linear algebra (and to be fair, I always have a few of these people in my intro stats class), it <em>will</em> help <em>you</em> to know that the solution to the estimation problem turns out to be <span class="math notranslate nohighlight">\(\hat{b} = (X^TX)^{-1} X^T y\)</span>, where <span class="math notranslate nohighlight">\(\hat{b}\)</span> is a vector containing the estimated regression coefficients,  <span class="math notranslate nohighlight">\(X\)</span> is the “design matrix” that contains the predictor variables (plus an additional column containing all ones; strictly <span class="math notranslate nohighlight">\(X\)</span> is a matrix of the regressors, but I haven’t discussed the distinction yet), and <span class="math notranslate nohighlight">\(y\)</span> is a vector containing the outcome variable. For everyone else, this isn’t exactly helpful, and can be downright scary. However, since quite a few things in linear regression can be written in linear algebra terms, you’ll see a bunch of footnotes like this one in this chapter. If you can follow the maths in them, great. If not, ignore it.</p>
</aside>
<aside class="footnote brackets" id="drink" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">4</a><span class="fn-bracket">]</span></span>
<p>You saw that coming, didn’t you? If not, then I bet you haven’t read the sections on <a class="reference internal" href="05.02-ttest.html#ttest"><span class="std std-ref">t-tests</span></a> or <a class="reference internal" href="05.03-anova.html#anova"><span class="std std-ref">ANOVA</span></a> yet, have you? I’m such a shill for <code class="docutils literal notranslate"><span class="pre">pingouin</span></code>.</p>
</aside>
<aside class="footnote brackets" id="notenolan" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">5</a><span class="fn-bracket">]</span></span>
<p>Christopher Nolan, have your people call my people if you’re interested, we’ll do lunch!</p>
</aside>
<aside class="footnote brackets" id="noteformula" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">6</a><span class="fn-bracket">]</span></span>
<p>This is extra confusing if you happen to have come from the world of R, where this sort of model is usually defined with a formula, in which the outcome measure comes first, followed by the predictor(s), or even if you have used <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>, which also preserves the R-style formula notation.</p>
</aside>
<aside class="footnote brackets" id="notenever" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">7</a><span class="fn-bracket">]</span></span>
<p>And by “sometimes” I mean “almost never”. In practice everyone just calls it “<span class="math notranslate nohighlight">\(R\)</span>-squared”.</p>
</aside>
<aside class="footnote brackets" id="notecorrection" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">8</a><span class="fn-bracket">]</span></span>
<p>Note that, although <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> has done multiple tests here, it hasn’t done a Bonferroni correction or anything. These are standard one-sample <span class="math notranslate nohighlight">\(t\)</span>-tests with a two-sided alternative. If you want to make corrections for multiple tests, you need to do that yourself.</p>
</aside>
<aside class="footnote brackets" id="noteregressors" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">9</a><span class="fn-bracket">]</span></span>
<p>Strictly, you standardise all the <em>regressors</em>: that is, every “thing” that has a regression coefficient associated with it in the model. For the regression models that I’ve talked about so far, each predictor variable maps onto exactly one regressor, and vice versa. However, that’s not actually true in general: we’ll see some examples of this when we learn about <a class="reference internal" href="05.05-anova2.html#anova2"><span class="std std-ref">factorial ANOVA</span></a>. But for now, we don’t need to care too much about this distinction.</p>
</aside>
<aside class="footnote brackets" id="notehope" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">10</a><span class="fn-bracket">]</span></span>
<p>Or have no hope, as the case may be.</p>
</aside>
<aside class="footnote brackets" id="notehatmatrix" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">11</a><span class="fn-bracket">]</span></span>
<p>Again, for the linear algebra fanatics: the “hat matrix” is defined to be that matrix <span class="math notranslate nohighlight">\(H\)</span> that converts the vector of observed values <span class="math notranslate nohighlight">\(y\)</span> into a vector of fitted values <span class="math notranslate nohighlight">\(\hat{y}\)</span>, such that <span class="math notranslate nohighlight">\(\hat{y} = H y\)</span>. The name comes from the fact that this is the matrix that “puts a hat on <span class="math notranslate nohighlight">\(y\)</span>”. The  hat <em>value</em> of the <span class="math notranslate nohighlight">\(i\)</span>-th observation is the <span class="math notranslate nohighlight">\(i\)</span>-th diagonal element of this matrix (so technically I should be writing it as <span class="math notranslate nohighlight">\(h_{ii}\)</span> rather than <span class="math notranslate nohighlight">\(h_{i}\)</span>). Oh, and in case you care, here’s how it’s calculated: <span class="math notranslate nohighlight">\(H = X(X^TX)^{-1} X^T\)</span>. Pretty, isn’t it?</p>
</aside>
<aside class="footnote brackets" id="noterobust" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">12</a><span class="fn-bracket">]</span></span>
<p>An alternative is to run a “robust regression”; I might discuss robust regression in a later version of this book, but I don’t promise.</p>
</aside>
<aside class="footnote brackets" id="sandwich" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">13</a><span class="fn-bracket">]</span></span>
<p>Again, a footnote that should be read only by the two readers of this book that love linear algebra (mmmm… I love the smell of matrix computations in the morning; smells like… nerd). In these estimators, the covariance matrix for <span class="math notranslate nohighlight">\(b\)</span> is given by <span class="math notranslate nohighlight">\((X^T X)^{-1}\  X^T \Sigma X \ (X^T X)^{-1}\)</span>. See, it’s a “sandwich”? Assuming you think that  <span class="math notranslate nohighlight">\((X^T X)^{-1} = \text{&quot;bread&quot;}\)</span> and  <span class="math notranslate nohighlight">\(X^T \Sigma X = \text{&quot;filling&quot;}\)</span>, that is. Which of course everyone does, right? In any case, the usual estimator is what you get when you set <span class="math notranslate nohighlight">\(\Sigma = \hat\sigma^2 I\)</span>. But there are others. For instance, the corrected version that I learned originally uses <span class="math notranslate nohighlight">\(\Sigma = \text{diag}(\epsilon_i^2)\)</span> <span id="id18">[<a class="reference internal" href="bibliography.html#id48" title="H. White. A heteroskedasticity-consistent covariance matrix estimator and a direct test for heteroskedasticity. Econometrika, 48:817-838, 1980.">White, 1980</a>]</span>. <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> gives you a choice of four different sandwich estimators you can use, but not (at least that I can find today on June 21 2023) a lot of information on what they actually are, or references for where they come from, so if you <em>really</em> need this information, you’ll have to do your own digging. Sorry.</p>
</aside>
<aside class="footnote brackets" id="aic-calc" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">14</a><span class="fn-bracket">]</span></span>
<p>Note, depending on exactly how AIC is calculated, the actual AIC value may be different. That is, if you calculate AIC using the formula above, a Python function, or an algorithm from some other statistics software. However, if you use the same method to calculate AIC for two different models, and take the differnce between them, then this should end up being the same, no matter what method was used. In practice, this is all you care about: the actual value of an AIC statistic isn’t very informative, but the differences between two AIC values <em>are</em> useful, since these provide a measure of the extent to which one model outperforms another.</p>
</aside>
<aside class="footnote brackets" id="exercise" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id17">15</a><span class="fn-bracket">]</span></span>
<p>I leave it as a “fun” programming exercise to you to adapt this function even further, so that it present the results in a nice table format, e.g. in a dataframe. Not necessary, but it would make it easier to compare models if the results were nicely lined up in columns, don’t you think?</p>
</aside>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="05.03-anova.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">15. </span>Comparing several means (one-way ANOVA)</p>
      </div>
    </a>
    <a class="right-next"
       href="05.05-anova2.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">17. </span>Factorial ANOVA</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-a-linear-regression-model">16.1. Estimating a linear regression model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-with-python">16.2. Linear Regression with Python</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warning">16.2.1. Warning!!!</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-estimated-model">16.2.2. Interpreting the estimated model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-linear-regression">16.3. Multiple linear regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-linear-regression-in-python">16.4. Multiple Linear Regression in Python</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#formula-for-the-general-case">16.4.1. Formula for the general case</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantifying-the-fit-of-the-regression-model">16.5. Quantifying the fit of the regression model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-r-2-value">16.5.1. The <span class="math notranslate nohighlight">\(R^2\)</span> value</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-relationship-between-regression-and-correlation">16.5.2. The relationship between regression and correlation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-adjusted-r-2-value">16.5.3. The adjusted <span class="math notranslate nohighlight">\(R^2\)</span> value</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hypothesis-tests-for-regression-models">16.6. Hypothesis tests for regression models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-the-model-as-a-whole">16.6.1. Testing the model as a whole</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#an-f-test-function">16.6.2. An F-test function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tests-for-individual-coefficients">16.6.3. Tests for individual coefficients</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-the-significance-of-a-correlation">16.7. Testing the significance of a correlation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hypothesis-tests-for-a-single-correlation">16.7.1. Hypothesis tests for a single correlation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hypothesis-tests-for-all-pairwise-correlations">16.7.2. Hypothesis tests for all pairwise correlations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-standardised-regression-coefficients">16.7.3. Calculating standardised regression coefficients</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumptions-of-regression">16.8. Assumptions of regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-checking">16.9. Model checking</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#three-kinds-of-residuals">16.9.1. Three kinds of residuals</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#three-kinds-of-anomalous-data">16.9.2. Three kinds of anomalous data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checking-the-normality-of-the-residuals">16.9.3. Checking the normality of the residuals</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checking-the-linearity-of-the-relationship">16.9.4. Checking the linearity of the relationship</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checking-the-homogeneity-of-variance">16.9.5. Checking the homogeneity of variance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checking-for-collinearity">16.9.6. Checking for collinearity</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-selection">16.10. Model Selection</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-elimination">16.10.1. Backward elimination</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-caveat">16.10.2. A caveat</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-two-regression-models">16.10.3. Comparing two regression models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">16.11. Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Danielle Navarro and Ethan Weed
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2021.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>